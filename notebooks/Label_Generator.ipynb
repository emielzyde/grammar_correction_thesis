{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code from Marek Rei's 'sequence-labeler' repository (https://github.com/marekrei/sequence-labeler) and is used to label data using various self-training methods. It was designed specifically to be run in the Colaboratory environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "_4evZSKA1IaM",
    "outputId": "d3523fd4-2eb4-4aed-82b3-c0e6a2be7844"
   },
   "outputs": [],
   "source": [
    "#I will attach my Drive so that I can easily load files.\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\", force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBid4x1E1ZWU"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import numpy\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "\n",
    "class SequenceLabeler(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        self.UNK = \"<unk>\"\n",
    "        self.CUNK = \"<cunk>\"\n",
    "\n",
    "        self.word2id = None\n",
    "        self.char2id = None\n",
    "        self.label2id = None\n",
    "        self.singletons = None\n",
    "\n",
    "\n",
    "    def build_vocabs(self, data_train, data_dev, data_test, embedding_path=None):\n",
    "        data_source = list(data_train)\n",
    "        if self.config[\"vocab_include_devtest\"]:\n",
    "            if data_dev != None:\n",
    "                data_source += data_dev\n",
    "            if data_test != None:\n",
    "                data_source += data_test\n",
    "\n",
    "        char_counter = collections.Counter()\n",
    "        for sentence in data_source:\n",
    "            for word in sentence:\n",
    "                char_counter.update(word[0])\n",
    "        self.char2id = collections.OrderedDict([(self.CUNK, 0)])\n",
    "        for char, count in char_counter.most_common():\n",
    "            if char not in self.char2id:\n",
    "                self.char2id[char] = len(self.char2id)\n",
    "\n",
    "        word_counter = collections.Counter()\n",
    "        for sentence in data_source:\n",
    "            for word in sentence:\n",
    "                w = word[0]\n",
    "                if self.config[\"lowercase\"] == True:\n",
    "                    w = w.lower()\n",
    "                if self.config[\"replace_digits\"] == True:\n",
    "                    w = re.sub(r'\\d', '0', w)\n",
    "                word_counter[w] += 1\n",
    "        self.word2id = collections.OrderedDict([(self.UNK, 0)])\n",
    "        for word, count in word_counter.most_common():\n",
    "            if self.config[\"min_word_freq\"] <= 0 or count >= self.config[\"min_word_freq\"]:\n",
    "                if word not in self.word2id:\n",
    "                    self.word2id[word] = len(self.word2id)\n",
    "\n",
    "        self.singletons = set([word for word in word_counter if word_counter[word] == 1])\n",
    "\n",
    "        label_counter = collections.Counter()\n",
    "        for sentence in data_train: #this one only based on training data\n",
    "            for word in sentence:\n",
    "                label_counter[word[-1]] += 1\n",
    "        self.label2id = collections.OrderedDict()\n",
    "        for label, count in label_counter.most_common():\n",
    "            if label not in self.label2id:\n",
    "                self.label2id[label] = len(self.label2id)\n",
    "\n",
    "        if embedding_path != None and self.config[\"vocab_only_embedded\"] == True:\n",
    "            self.embedding_vocab = set([self.UNK])\n",
    "            with open(embedding_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    line_parts = line.strip().split()\n",
    "                    if len(line_parts) <= 2:\n",
    "                        continue\n",
    "                    w = line_parts[0]\n",
    "                    if self.config[\"lowercase\"] == True:\n",
    "                        w = w.lower()\n",
    "                    if self.config[\"replace_digits\"] == True:\n",
    "                        w = re.sub(r'\\d', '0', w)\n",
    "                    self.embedding_vocab.add(w)\n",
    "            word2id_revised = collections.OrderedDict()\n",
    "            for word in self.word2id:\n",
    "                if word in embedding_vocab and word not in word2id_revised:\n",
    "                    word2id_revised[word] = len(word2id_revised)\n",
    "            self.word2id = word2id_revised\n",
    "\n",
    "        print(\"n_words: \" + str(len(self.word2id)))\n",
    "        print(\"n_chars: \" + str(len(self.char2id)))\n",
    "        print(\"n_labels: \" + str(len(self.label2id)))\n",
    "        print(\"n_singletons: \" + str(len(self.singletons)))\n",
    "\n",
    "\n",
    "    def construct_network(self):\n",
    "        self.word_ids = tf.placeholder(tf.int32, [None, None], name=\"word_ids\")\n",
    "        self.char_ids = tf.placeholder(tf.int32, [None, None, None], name=\"char_ids\")\n",
    "        self.sentence_lengths = tf.placeholder(tf.int32, [None], name=\"sentence_lengths\")\n",
    "        self.word_lengths = tf.placeholder(tf.int32, [None, None], name=\"word_lengths\")\n",
    "        self.label_ids = tf.placeholder(tf.int32, [None, None], name=\"label_ids\")\n",
    "        self.learningrate = tf.placeholder(tf.float32, name=\"learningrate\")\n",
    "        self.is_training = tf.placeholder(tf.int32, name=\"is_training\")\n",
    "\n",
    "        self.loss = 0.0\n",
    "        input_tensor = None\n",
    "        input_vector_size = 0\n",
    "\n",
    "        self.initializer = None\n",
    "        if self.config[\"initializer\"] == \"normal\":\n",
    "            self.initializer = tf.random_normal_initializer(mean=0.0, stddev=0.1)\n",
    "        elif self.config[\"initializer\"] == \"glorot\":\n",
    "            self.initializer = tf.glorot_uniform_initializer()\n",
    "        elif self.config[\"initializer\"] == \"xavier\":\n",
    "            self.initializer = tf.glorot_normal_initializer()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown initializer\")\n",
    "\n",
    "        self.word_embeddings = tf.get_variable(\"word_embeddings\", \n",
    "            shape=[len(self.word2id), self.config[\"word_embedding_size\"]], \n",
    "            initializer=(tf.zeros_initializer() if self.config[\"emb_initial_zero\"] == True else self.initializer), \n",
    "            trainable=(True if self.config[\"train_embeddings\"] == True else False))\n",
    "        input_tensor = tf.nn.embedding_lookup(self.word_embeddings, self.word_ids)\n",
    "        input_vector_size = self.config[\"word_embedding_size\"]\n",
    "\n",
    "        if self.config[\"char_embedding_size\"] > 0 and self.config[\"char_recurrent_size\"] > 0:\n",
    "            with tf.variable_scope(\"chars\"), tf.control_dependencies([tf.assert_equal(tf.shape(self.char_ids)[2], tf.reduce_max(self.word_lengths), message=\"Char dimensions don't match\")]):\n",
    "                self.char_embeddings = tf.get_variable(\"char_embeddings\", \n",
    "                    shape=[len(self.char2id), self.config[\"char_embedding_size\"]], \n",
    "                    initializer=self.initializer, \n",
    "                    trainable=True)\n",
    "                char_input_tensor = tf.nn.embedding_lookup(self.char_embeddings, self.char_ids)\n",
    "\n",
    "                s = tf.shape(char_input_tensor)\n",
    "                char_input_tensor = tf.reshape(char_input_tensor, shape=[s[0]*s[1], s[2], self.config[\"char_embedding_size\"]])\n",
    "                _word_lengths = tf.reshape(self.word_lengths, shape=[s[0]*s[1]])\n",
    "\n",
    "                char_lstm_cell_fw = tf.nn.rnn_cell.LSTMCell(self.config[\"char_recurrent_size\"], \n",
    "                    use_peepholes=self.config[\"lstm_use_peepholes\"], \n",
    "                    state_is_tuple=True, \n",
    "                    initializer=self.initializer,\n",
    "                    reuse=False)\n",
    "                char_lstm_cell_bw = tf.nn.rnn_cell.LSTMCell(self.config[\"char_recurrent_size\"], \n",
    "                    use_peepholes=self.config[\"lstm_use_peepholes\"], \n",
    "                    state_is_tuple=True, \n",
    "                    initializer=self.initializer,\n",
    "                    reuse=False)\n",
    "\n",
    "                char_lstm_outputs = tf.nn.bidirectional_dynamic_rnn(char_lstm_cell_fw, char_lstm_cell_bw, char_input_tensor, sequence_length=_word_lengths, dtype=tf.float32, time_major=False)\n",
    "                _, ((_, char_output_fw), (_, char_output_bw)) = char_lstm_outputs\n",
    "                char_output_tensor = tf.concat([char_output_fw, char_output_bw], axis=-1)\n",
    "                char_output_tensor = tf.reshape(char_output_tensor, shape=[s[0], s[1], 2 * self.config[\"char_recurrent_size\"]])\n",
    "                char_output_vector_size = 2 * self.config[\"char_recurrent_size\"]\n",
    "\n",
    "                if self.config[\"lmcost_char_gamma\"] > 0.0:\n",
    "                    self.loss += self.config[\"lmcost_char_gamma\"] * self.construct_lmcost(char_output_tensor, char_output_tensor, self.sentence_lengths, self.word_ids, \"separate\", \"lmcost_char_separate\")\n",
    "                if self.config[\"lmcost_joint_char_gamma\"] > 0.0:\n",
    "                    self.loss += self.config[\"lmcost_joint_char_gamma\"] * self.construct_lmcost(char_output_tensor, char_output_tensor, self.sentence_lengths, self.word_ids, \"joint\", \"lmcost_char_joint\")\n",
    "\n",
    "                if self.config[\"char_hidden_layer_size\"] > 0:\n",
    "                    char_hidden_layer_size = self.config[\"word_embedding_size\"] if self.config[\"char_integration_method\"] == \"attention\" else self.config[\"char_hidden_layer_size\"]\n",
    "                    char_output_tensor = tf.layers.dense(char_output_tensor, char_hidden_layer_size, activation=tf.tanh, kernel_initializer=self.initializer)\n",
    "                    char_output_vector_size = char_hidden_layer_size\n",
    "\n",
    "                if self.config[\"char_integration_method\"] == \"concat\":\n",
    "                    input_tensor = tf.concat([input_tensor, char_output_tensor], axis=-1)\n",
    "                    input_vector_size += char_output_vector_size\n",
    "                elif self.config[\"char_integration_method\"] == \"attention\":\n",
    "                    assert(char_output_vector_size == self.config[\"word_embedding_size\"]), \"This method requires the char representation to have the same size as word embeddings\"\n",
    "                    static_input_tensor = tf.stop_gradient(input_tensor)\n",
    "                    is_unk = tf.equal(self.word_ids, self.word2id[self.UNK])\n",
    "                    char_output_tensor_normalised = tf.nn.l2_normalize(char_output_tensor, 2)\n",
    "                    static_input_tensor_normalised = tf.nn.l2_normalize(static_input_tensor, 2)\n",
    "                    cosine_cost = 1.0 - tf.reduce_sum(tf.multiply(char_output_tensor_normalised, static_input_tensor_normalised), axis=2)\n",
    "                    is_padding = tf.logical_not(tf.sequence_mask(self.sentence_lengths, maxlen=tf.shape(self.word_ids)[1]))\n",
    "                    cosine_cost_unk = tf.where(tf.logical_or(is_unk, is_padding), x=tf.zeros_like(cosine_cost), y=cosine_cost)\n",
    "                    self.loss += self.config[\"char_attention_cosine_cost\"] * tf.reduce_sum(cosine_cost_unk)\n",
    "                    attention_evidence_tensor = tf.concat([input_tensor, char_output_tensor], axis=2)\n",
    "                    attention_output = tf.layers.dense(attention_evidence_tensor, self.config[\"word_embedding_size\"], activation=tf.tanh, kernel_initializer=self.initializer)\n",
    "                    attention_output = tf.layers.dense(attention_output, self.config[\"word_embedding_size\"], activation=tf.sigmoid, kernel_initializer=self.initializer)\n",
    "                    input_tensor = tf.multiply(input_tensor, attention_output) + tf.multiply(char_output_tensor, (1.0 - attention_output))\n",
    "                elif self.config[\"char_integration_method\"] == \"none\":\n",
    "                    input_tensor = input_tensor\n",
    "                else:\n",
    "                    raise ValueError(\"Unknown char integration method\")\n",
    "\n",
    "        dropout_input = self.config[\"dropout_input\"] * tf.cast(self.is_training, tf.float32) + (1.0 - tf.cast(self.is_training, tf.float32))\n",
    "        input_tensor =  tf.nn.dropout(input_tensor, dropout_input, name=\"dropout_word\")\n",
    "\n",
    "        word_lstm_cell_fw = tf.nn.rnn_cell.LSTMCell(self.config[\"word_recurrent_size\"], \n",
    "            use_peepholes=self.config[\"lstm_use_peepholes\"], \n",
    "            state_is_tuple=True, \n",
    "            initializer=self.initializer,\n",
    "            reuse=False)\n",
    "        word_lstm_cell_bw = tf.nn.rnn_cell.LSTMCell(self.config[\"word_recurrent_size\"], \n",
    "            use_peepholes=self.config[\"lstm_use_peepholes\"], \n",
    "            state_is_tuple=True, \n",
    "            initializer=self.initializer,\n",
    "            reuse=False)\n",
    "\n",
    "        with tf.control_dependencies([tf.assert_equal(tf.shape(self.word_ids)[1], tf.reduce_max(self.sentence_lengths), message=\"Sentence dimensions don't match\")]):\n",
    "            (lstm_outputs_fw, lstm_outputs_bw), _ = tf.nn.bidirectional_dynamic_rnn(word_lstm_cell_fw, word_lstm_cell_bw, input_tensor, sequence_length=self.sentence_lengths, dtype=tf.float32, time_major=False)\n",
    "\n",
    "        dropout_word_lstm = self.config[\"dropout_word_lstm\"] * tf.cast(self.is_training, tf.float32) + (1.0 - tf.cast(self.is_training, tf.float32))\n",
    "        lstm_outputs_fw =  tf.nn.dropout(lstm_outputs_fw, dropout_word_lstm)\n",
    "        lstm_outputs_bw =  tf.nn.dropout(lstm_outputs_bw, dropout_word_lstm)\n",
    "\n",
    "        if self.config[\"lmcost_lstm_gamma\"] > 0.0:\n",
    "            self.loss += self.config[\"lmcost_lstm_gamma\"] * self.construct_lmcost(lstm_outputs_fw, lstm_outputs_bw, self.sentence_lengths, self.word_ids, \"separate\", \"lmcost_lstm_separate\")\n",
    "        if self.config[\"lmcost_joint_lstm_gamma\"] > 0.0:\n",
    "            self.loss += self.config[\"lmcost_joint_lstm_gamma\"] * self.construct_lmcost(lstm_outputs_fw, lstm_outputs_bw, self.sentence_lengths, self.word_ids, \"joint\", \"lmcost_lstm_joint\")\n",
    "\n",
    "        processed_tensor = tf.concat([lstm_outputs_fw, lstm_outputs_bw], 2)\n",
    "        processed_tensor_size = self.config[\"word_recurrent_size\"] * 2\n",
    "\n",
    "        if self.config[\"hidden_layer_size\"] > 0:\n",
    "            processed_tensor = tf.layers.dense(processed_tensor, self.config[\"hidden_layer_size\"], activation=tf.tanh, kernel_initializer=self.initializer)\n",
    "            processed_tensor_size = self.config[\"hidden_layer_size\"]\n",
    "\n",
    "        self.scores = tf.layers.dense(processed_tensor, len(self.label2id), activation=None, kernel_initializer=self.initializer, name=\"output_ff\")\n",
    "\n",
    "        if self.config[\"crf_on_top\"] == True:\n",
    "            crf_num_tags = self.scores.get_shape()[2].value\n",
    "            self.crf_transition_params = tf.get_variable(\"output_crf_transitions\", [crf_num_tags, crf_num_tags], initializer=self.initializer)\n",
    "            log_likelihood, self.crf_transition_params = tf.contrib.crf.crf_log_likelihood(self.scores, self.label_ids, self.sentence_lengths, transition_params=self.crf_transition_params)\n",
    "            self.loss += self.config[\"main_cost\"] * tf.reduce_sum(-log_likelihood) \n",
    "        else:\n",
    "            self.probabilities = tf.nn.softmax(self.scores)\n",
    "            self.predictions = tf.argmax(self.probabilities, 2)\n",
    "            loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.scores, labels=self.label_ids)\n",
    "            mask = tf.sequence_mask(self.sentence_lengths, maxlen=tf.shape(self.word_ids)[1])\n",
    "            loss_ = tf.boolean_mask(loss_, mask)\n",
    "            self.loss += self.config[\"main_cost\"] * tf.reduce_sum(loss_) \n",
    "\n",
    "        self.train_op = self.construct_optimizer(self.config[\"opt_strategy\"], self.loss, self.learningrate, self.config[\"clip\"])\n",
    "\n",
    "\n",
    "    def construct_lmcost(self, input_tensor_fw, input_tensor_bw, sentence_lengths, target_ids, lmcost_type, name):\n",
    "        with tf.variable_scope(name):\n",
    "            lmcost_max_vocab_size = min(len(self.word2id), self.config[\"lmcost_max_vocab_size\"])\n",
    "            target_ids = tf.where(tf.greater_equal(target_ids, lmcost_max_vocab_size-1), x=(lmcost_max_vocab_size-1)+tf.zeros_like(target_ids), y=target_ids)\n",
    "            cost = 0.0\n",
    "            if lmcost_type == \"separate\":\n",
    "                lmcost_fw_mask = tf.sequence_mask(sentence_lengths, maxlen=tf.shape(target_ids)[1])[:,1:]\n",
    "                lmcost_bw_mask = tf.sequence_mask(sentence_lengths, maxlen=tf.shape(target_ids)[1])[:,:-1]\n",
    "                lmcost_fw = self._construct_lmcost(input_tensor_fw[:,:-1,:], lmcost_max_vocab_size, lmcost_fw_mask, target_ids[:,1:], name=name+\"_fw\")\n",
    "                lmcost_bw = self._construct_lmcost(input_tensor_bw[:,1:,:], lmcost_max_vocab_size, lmcost_bw_mask, target_ids[:,:-1], name=name+\"_bw\")\n",
    "                cost += lmcost_fw + lmcost_bw\n",
    "            elif lmcost_type == \"joint\":\n",
    "                joint_input_tensor = tf.concat([input_tensor_fw[:,:-2,:], input_tensor_bw[:,2:,:]], axis=-1)\n",
    "                lmcost_mask = tf.sequence_mask(sentence_lengths, maxlen=tf.shape(target_ids)[1])[:,1:-1]\n",
    "                cost += self._construct_lmcost(joint_input_tensor, lmcost_max_vocab_size, lmcost_mask, target_ids[:,1:-1], name=name+\"_joint\")\n",
    "            else:\n",
    "                raise ValueError(\"Unknown lmcost_type: \" + str(lmcost_type))\n",
    "            return cost\n",
    "\n",
    "\n",
    "    def _construct_lmcost(self, input_tensor, lmcost_max_vocab_size, lmcost_mask, target_ids, name):\n",
    "        with tf.variable_scope(name):\n",
    "            lmcost_hidden_layer = tf.layers.dense(input_tensor, self.config[\"lmcost_hidden_layer_size\"], activation=tf.tanh, kernel_initializer=self.initializer)\n",
    "            lmcost_output = tf.layers.dense(lmcost_hidden_layer, lmcost_max_vocab_size, activation=None, kernel_initializer=self.initializer)\n",
    "            lmcost_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=lmcost_output, labels=target_ids)\n",
    "            lmcost_loss = tf.where(lmcost_mask, lmcost_loss, tf.zeros_like(lmcost_loss))\n",
    "            return tf.reduce_sum(lmcost_loss)\n",
    "\n",
    "\n",
    "    def construct_optimizer(self, opt_strategy, loss, learningrate, clip):\n",
    "        optimizer = None\n",
    "        if opt_strategy == \"adadelta\":\n",
    "            optimizer = tf.train.AdadeltaOptimizer(learning_rate=learningrate)\n",
    "        elif opt_strategy == \"adam\":\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learningrate)\n",
    "        elif opt_strategy == \"sgd\":\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=learningrate)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown optimisation strategy: \" + str(opt_strategy))\n",
    "\n",
    "        if clip > 0.0:\n",
    "            grads, vs     = zip(*optimizer.compute_gradients(loss))\n",
    "            grads, gnorm  = tf.clip_by_global_norm(grads, clip)\n",
    "            train_op = optimizer.apply_gradients(zip(grads, vs))\n",
    "        else:\n",
    "            train_op = optimizer.minimize(loss)\n",
    "        return train_op\n",
    "\n",
    "\n",
    "    def preload_word_embeddings(self, embedding_path):\n",
    "        loaded_embeddings = set()\n",
    "        embedding_matrix = self.session.run(self.word_embeddings)\n",
    "        with open(embedding_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line_parts = line.strip().split()\n",
    "                if len(line_parts) <= 2:\n",
    "                    continue\n",
    "                w = line_parts[0]\n",
    "                if self.config[\"lowercase\"] == True:\n",
    "                    w = w.lower()\n",
    "                if self.config[\"replace_digits\"] == True:\n",
    "                    w = re.sub(r'\\d', '0', w)\n",
    "                if w in self.word2id and w not in loaded_embeddings:\n",
    "                    word_id = self.word2id[w]\n",
    "                    embedding = numpy.array(line_parts[1:])\n",
    "                    embedding_matrix[word_id] = embedding\n",
    "                    loaded_embeddings.add(w)\n",
    "        self.session.run(self.word_embeddings.assign(embedding_matrix))\n",
    "        print(\"n_preloaded_embeddings: \" + str(len(loaded_embeddings)))\n",
    "\n",
    "\n",
    "    def translate2id(self, token, token2id, unk_token, lowercase=False, replace_digits=False, singletons=None, singletons_prob=0.0):\n",
    "        if lowercase == True:\n",
    "            token = token.lower()\n",
    "        if replace_digits == True:\n",
    "            token = re.sub(r'\\d', '0', token)\n",
    "\n",
    "        token_id = None\n",
    "        if singletons != None and token in singletons and token in token2id and unk_token != None and numpy.random.uniform() < singletons_prob:\n",
    "            token_id = token2id[unk_token]\n",
    "        elif token in token2id:\n",
    "            token_id = token2id[token]\n",
    "        elif unk_token != None:\n",
    "            token_id = token2id[unk_token]\n",
    "        else:\n",
    "            raise ValueError(\"Unable to handle value, no UNK token: \" + str(token))\n",
    "        return token_id\n",
    "\n",
    "\n",
    "    def create_input_dictionary_for_batch(self, batch, is_training, learningrate):\n",
    "        sentence_lengths = numpy.array([len(sentence) for sentence in batch])\n",
    "        max_sentence_length = sentence_lengths.max()\n",
    "        max_word_length = numpy.array([numpy.array([len(word[0]) for word in sentence]).max() for sentence in batch]).max()\n",
    "        if self.config[\"allowed_word_length\"] > 0 and self.config[\"allowed_word_length\"] < max_word_length:\n",
    "            max_word_length = min(max_word_length, self.config[\"allowed_word_length\"])\n",
    "\n",
    "        word_ids = numpy.zeros((len(batch), max_sentence_length), dtype=numpy.int32)\n",
    "        char_ids = numpy.zeros((len(batch), max_sentence_length, max_word_length), dtype=numpy.int32)\n",
    "        word_lengths = numpy.zeros((len(batch), max_sentence_length), dtype=numpy.int32)\n",
    "        label_ids = numpy.zeros((len(batch), max_sentence_length), dtype=numpy.int32)\n",
    "\n",
    "        singletons = self.singletons if is_training == True else None\n",
    "        singletons_prob = self.config[\"singletons_prob\"] if is_training == True else 0.0\n",
    "        for i in range(len(batch)):\n",
    "            for j in range(len(batch[i])):\n",
    "                word_ids[i][j] = self.translate2id(batch[i][j][0], self.word2id, self.UNK, lowercase=self.config[\"lowercase\"], replace_digits=self.config[\"replace_digits\"], singletons=singletons, singletons_prob=singletons_prob)\n",
    "                label_ids[i][j] = self.translate2id(batch[i][j][-1], self.label2id, None)\n",
    "                word_lengths[i][j] = min(len(batch[i][j][0]), max_word_length)\n",
    "                for k in range(min(len(batch[i][j][0]), max_word_length)):\n",
    "                    char_ids[i][j][k] = self.translate2id(batch[i][j][0][k], self.char2id, self.CUNK)\n",
    "\n",
    "        input_dictionary = {self.word_ids: word_ids, self.char_ids: char_ids, self.sentence_lengths: sentence_lengths, self.word_lengths: word_lengths, self.label_ids: label_ids, self.learningrate: learningrate, self.is_training: is_training}\n",
    "        return input_dictionary\n",
    "\n",
    "\n",
    "    def viterbi_decode(self, score, transition_params):\n",
    "        trellis = numpy.zeros_like(score)\n",
    "        backpointers = numpy.zeros_like(score, dtype=numpy.int32)\n",
    "        trellis[0] = score[0]\n",
    "\n",
    "        for t in range(1, score.shape[0]):\n",
    "            v = numpy.expand_dims(trellis[t - 1], 1) + transition_params\n",
    "            trellis[t] = score[t] + numpy.max(v, 0)\n",
    "            backpointers[t] = numpy.argmax(v, 0)\n",
    "\n",
    "        viterbi = [numpy.argmax(trellis[-1])]\n",
    "        for bp in reversed(backpointers[1:]):\n",
    "            viterbi.append(bp[viterbi[-1]])\n",
    "        viterbi.reverse()\n",
    "\n",
    "        viterbi_score = numpy.max(trellis[-1])\n",
    "        return viterbi, viterbi_score, trellis\n",
    "\n",
    "\n",
    "    def process_batch(self, batch, is_training, learningrate):\n",
    "        feed_dict = self.create_input_dictionary_for_batch(batch, is_training, learningrate)\n",
    "\n",
    "        if self.config[\"crf_on_top\"] == True:\n",
    "            cost, scores = self.session.run([self.loss, self.scores] + ([self.train_op] if is_training == True else []), feed_dict=feed_dict)[:2]\n",
    "            predicted_labels = []\n",
    "            predicted_probs = []\n",
    "            for i in range(len(batch)):\n",
    "                sentence_length = len(batch[i])\n",
    "                viterbi_seq, viterbi_score, viterbi_trellis = self.viterbi_decode(scores[i], self.session.run(self.crf_transition_params))\n",
    "                predicted_labels.append(viterbi_seq[:sentence_length])\n",
    "                predicted_probs.append(viterbi_trellis[:sentence_length])\n",
    "        else:\n",
    "            cost, predicted_labels_, predicted_probs_ = self.session.run([self.loss, self.predictions, self.probabilities] + ([self.train_op] if is_training == True else []), feed_dict=feed_dict)[:3]\n",
    "            predicted_labels = []\n",
    "            predicted_probs = []\n",
    "            for i in range(len(batch)):\n",
    "                sentence_length = len(batch[i])\n",
    "                predicted_labels.append(predicted_labels_[i][:sentence_length])\n",
    "                predicted_probs.append(predicted_probs_[i][:sentence_length])\n",
    "\n",
    "        return cost, predicted_labels, predicted_probs\n",
    "\n",
    "    def process_batch_labelling(self, batch, is_training, learningrate, threshold_val = 50):\n",
    "        feed_dict = self.create_input_dictionary_for_batch(batch, is_training, learningrate)\n",
    "\n",
    "        if self.config[\"crf_on_top\"] == True:\n",
    "            cost, scores = self.session.run([self.loss, self.scores] + ([self.train_op] if is_training == True else []), feed_dict=feed_dict)[:2]\n",
    "            predicted_labels = []\n",
    "            predicted_probs = []\n",
    "            for i in range(len(batch)):\n",
    "                sentence_length = len(batch[i])\n",
    "                viterbi_seq, viterbi_score, viterbi_trellis = self.viterbi_decode(scores[i], self.session.run(self.crf_transition_params))\n",
    "                predicted_labels.append(viterbi_seq[:sentence_length])\n",
    "                predicted_probs.append(viterbi_trellis[:sentence_length])\n",
    "        else:\n",
    "          \n",
    "            ### ORIGINAL METHOD ### \n",
    "            \n",
    "            cost, predicted_labels_, predicted_probs_ = self.session.run([self.loss, self.predictions, self.probabilities] + ([self.train_op] if is_training == True else []), feed_dict=feed_dict)[:3]\n",
    "            predicted_labels = []\n",
    "            predicted_probs = []\n",
    "            remove_list = []\n",
    "            for i in range(len(batch)):\n",
    "                sentence_length = len(batch[i])\n",
    "                predicted_labels.append(predicted_labels_[i][:sentence_length])\n",
    "                predicted_probs.append(predicted_probs_[i][:sentence_length])\n",
    "            \n",
    "            ### METHOD 1 ###\n",
    "            \n",
    "            #threshold_val_2 = 60\n",
    "            #remove_list = []\n",
    "            #cost, predicted_labels_, predicted_probs_ = self.session.run([self.loss, self.predictions, self.probabilities] + ([self.train_op] if is_training == True else []), feed_dict=feed_dict)[:3]\n",
    "            #predicted_labels = []\n",
    "            #predicted_probs = []\n",
    "            #for item in predicted_probs_:\n",
    "            #  for it in item:\n",
    "            #      it[1] = 1 if it[1] > threshold_val_2/100 else 0 \n",
    "            #predicted_labels_ = np.argmax(np.array(predicted_probs_), 2)  \n",
    "\n",
    "            #for i in range(len(batch)):\n",
    "            #    sentence_length = len(batch[i])\n",
    "            #    predicted_labels.append(predicted_labels_[i][:sentence_length])\n",
    "            #    predicted_probs.append(predicted_probs_[i][:sentence_length])\n",
    "            \n",
    "            ###  METHOD 2 ###\n",
    "            \n",
    "            #threshold_val = 70\n",
    "            #cost, predicted_labels_, predicted_probs_ = self.session.run([self.loss, self.predictions, self.probabilities] + ([self.train_op] if is_training == True else []), feed_dict=feed_dict)[:3]\n",
    "            #remove_list = []\n",
    "            #appender = True \n",
    "            #predicted_labels = []\n",
    "            #predicted_probs = []\n",
    "            #for i in range(len(batch)):\n",
    "            #      sentence_length = len(batch[i])\n",
    "            #      predicted_probber= predicted_probs_[i][:sentence_length]\n",
    "            #      for j in range(len(predicted_probber)):\n",
    "            #        vals = predicted_probber[j]\n",
    "            #        if vals[0] < threshold_val/100 and vals[1] < threshold_val/100:\n",
    "            #          appender = False\n",
    "            #      if appender:\n",
    "            #        predicted_labels.append(predicted_labels_[i][:sentence_length])\n",
    "            #        predicted_probs.append(predicted_probs_[i][:sentence_length])\n",
    "            #      else:\n",
    "            #        predicted_labels.append(predicted_labels_[i][:sentence_length])\n",
    "            #        predicted_probs.append(predicted_probs_[i][:sentence_length])\n",
    "            #        remove_list.append(i)\n",
    "            #      appender = True \n",
    "\n",
    "        return cost, predicted_labels, predicted_probs, remove_list\n",
    "\n",
    "    def initialize_session(self):\n",
    "        tf.set_random_seed(self.config[\"random_seed\"])\n",
    "        session_config = tf.ConfigProto()\n",
    "        session_config.gpu_options.allow_growth = self.config[\"tf_allow_growth\"]\n",
    "        session_config.gpu_options.per_process_gpu_memory_fraction = self.config[\"tf_per_process_gpu_memory_fraction\"]\n",
    "        self.session = tf.Session(config=session_config)\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        self.saver = tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "\n",
    "    def get_parameter_count(self):\n",
    "        total_parameters = 0\n",
    "        for variable in tf.trainable_variables():\n",
    "            shape = variable.get_shape()\n",
    "            variable_parameters = 1\n",
    "            for dim in shape:\n",
    "                variable_parameters *= dim.value\n",
    "            total_parameters += variable_parameters\n",
    "        return total_parameters\n",
    "\n",
    "\n",
    "    def get_parameter_count_without_word_embeddings(self):\n",
    "        shape = self.word_embeddings.get_shape()\n",
    "        variable_parameters = 1\n",
    "        for dim in shape:\n",
    "            variable_parameters *= dim.value\n",
    "        return self.get_parameter_count() - variable_parameters\n",
    "\n",
    "\n",
    "    def save(self, filename):\n",
    "        dump = {}\n",
    "        dump[\"config\"] = self.config\n",
    "        dump[\"UNK\"] = self.UNK\n",
    "        dump[\"CUNK\"] = self.CUNK\n",
    "        dump[\"word2id\"] = self.word2id\n",
    "        dump[\"char2id\"] = self.char2id\n",
    "        dump[\"label2id\"] = self.label2id\n",
    "        dump[\"singletons\"] = self.singletons\n",
    "\n",
    "        dump[\"params\"] = {}\n",
    "        for variable in tf.global_variables():\n",
    "            assert(variable.name not in dump[\"params\"]), \"Error: variable with this name already exists\" + str(variable.name)\n",
    "            dump[\"params\"][variable.name] = self.session.run(variable)\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(dump, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            dump = pickle.load(f)\n",
    "\n",
    "            # for safety, so we don't overwrite old models\n",
    "            dump[\"config\"][\"save\"] = None\n",
    "\n",
    "            labeler = SequenceLabeler(dump[\"config\"])\n",
    "            labeler.UNK = dump[\"UNK\"]\n",
    "            labeler.CUNK = dump[\"CUNK\"]\n",
    "            labeler.word2id = dump[\"word2id\"]\n",
    "            labeler.char2id = dump[\"char2id\"]\n",
    "            labeler.label2id = dump[\"label2id\"]\n",
    "            labeler.singletons = dump[\"singletons\"]\n",
    "\n",
    "            labeler.construct_network()\n",
    "            labeler.initialize_session()\n",
    "            labeler.load_params(filename)\n",
    "\n",
    "            return labeler\n",
    "\n",
    "\n",
    "    def load_params(self, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            dump = pickle.load(f)\n",
    "\n",
    "            for variable in tf.global_variables():\n",
    "                assert(variable.name in dump[\"params\"]), \"Variable not in dump: \" + str(variable.name)\n",
    "                assert(variable.shape == dump[\"params\"][variable.name].shape), \"Variable shape not as expected: \" + str(variable.name) + \" \" + str(variable.shape) + \" \" + str(dump[\"params\"][variable.name].shape)\n",
    "                value = numpy.asarray(dump[\"params\"][variable.name])\n",
    "                self.session.run(variable.assign(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v0VwuBuV1k5Y"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "\n",
    "from collections import defaultdict, namedtuple\n",
    "\n",
    "ANY_SPACE = '<SPACE>'\n",
    "\n",
    "class FormatError(Exception):\n",
    "    pass\n",
    "\n",
    "Metrics = namedtuple('Metrics', 'tp fp fn prec rec fscore')\n",
    "\n",
    "class EvalCounts(object):\n",
    "    def __init__(self):\n",
    "        self.correct_chunk = 0    # number of correctly identified chunks\n",
    "        self.correct_tags = 0     # number of correct chunk tags\n",
    "        self.found_correct = 0    # number of chunks in corpus\n",
    "        self.found_guessed = 0    # number of identified chunks\n",
    "        self.token_counter = 0    # token counter (ignores sentence breaks)\n",
    "\n",
    "        # counts by type\n",
    "        self.t_correct_chunk = defaultdict(int)\n",
    "        self.t_found_correct = defaultdict(int)\n",
    "        self.t_found_guessed = defaultdict(int)\n",
    "\n",
    "def parse_args(argv):\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='evaluate tagging results using CoNLL criteria',\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
    "    )\n",
    "    arg = parser.add_argument\n",
    "    arg('-b', '--boundary', metavar='STR', default='-X-',\n",
    "        help='sentence boundary')\n",
    "    arg('-d', '--delimiter', metavar='CHAR', default=ANY_SPACE,\n",
    "        help='character delimiting items in input')\n",
    "    arg('-o', '--otag', metavar='CHAR', default='O',\n",
    "        help='alternative outside tag')\n",
    "    arg('file', nargs='?', default=None)\n",
    "    return parser.parse_args(argv)\n",
    "\n",
    "def parse_tag(t):\n",
    "    m = re.match(r'^([^-]*)-(.*)$', t)\n",
    "    return m.groups() if m else (t, '')\n",
    "\n",
    "def evaluate(iterable, options=None):\n",
    "    if options is None:\n",
    "        options = parse_args([])    # use defaults\n",
    "\n",
    "    counts = EvalCounts()\n",
    "    num_features = None       # number of features per line\n",
    "    in_correct = False        # currently processed chunks is correct until now\n",
    "    last_correct = 'O'        # previous chunk tag in corpus\n",
    "    last_correct_type = ''    # type of previously identified chunk tag\n",
    "    last_guessed = 'O'        # previously identified chunk tag\n",
    "    last_guessed_type = ''    # type of previous chunk tag in corpus\n",
    "\n",
    "    for line in iterable:\n",
    "        line = line.rstrip('\\r\\n')\n",
    "\n",
    "        if options.delimiter == ANY_SPACE:\n",
    "            features = line.split()\n",
    "        else:\n",
    "            features = line.split(options.delimiter)\n",
    "\n",
    "        if num_features is None:\n",
    "            num_features = len(features)\n",
    "        elif num_features != len(features) and len(features) != 0:\n",
    "            raise FormatError('unexpected number of features: %d (%d)' %\n",
    "                              (len(features), num_features))\n",
    "\n",
    "        if len(features) == 0 or features[0] == options.boundary:\n",
    "            features = [options.boundary, 'O', 'O']\n",
    "        if len(features) < 3:\n",
    "            raise FormatError('unexpected number of features in line %s' % line)\n",
    "\n",
    "        guessed, guessed_type = parse_tag(features.pop())\n",
    "        correct, correct_type = parse_tag(features.pop())\n",
    "        first_item = features.pop(0)\n",
    "\n",
    "        if first_item == options.boundary:\n",
    "            guessed = 'O'\n",
    "\n",
    "        end_correct = end_of_chunk(last_correct, correct,\n",
    "                                   last_correct_type, correct_type)\n",
    "        end_guessed = end_of_chunk(last_guessed, guessed,\n",
    "                                   last_guessed_type, guessed_type)\n",
    "        start_correct = start_of_chunk(last_correct, correct,\n",
    "                                       last_correct_type, correct_type)\n",
    "        start_guessed = start_of_chunk(last_guessed, guessed,\n",
    "                                       last_guessed_type, guessed_type)\n",
    "\n",
    "        if in_correct:\n",
    "            if (end_correct and end_guessed and\n",
    "                last_guessed_type == last_correct_type):\n",
    "                in_correct = False\n",
    "                counts.correct_chunk += 1\n",
    "                counts.t_correct_chunk[last_correct_type] += 1\n",
    "            elif (end_correct != end_guessed or guessed_type != correct_type):\n",
    "                in_correct = False\n",
    "\n",
    "        if start_correct and start_guessed and guessed_type == correct_type:\n",
    "            in_correct = True\n",
    "\n",
    "        if start_correct:\n",
    "            counts.found_correct += 1\n",
    "            counts.t_found_correct[correct_type] += 1\n",
    "        if start_guessed:\n",
    "            counts.found_guessed += 1\n",
    "            counts.t_found_guessed[guessed_type] += 1\n",
    "        if first_item != options.boundary:\n",
    "            if correct == guessed and guessed_type == correct_type:\n",
    "                counts.correct_tags += 1\n",
    "            counts.token_counter += 1\n",
    "\n",
    "        last_guessed = guessed\n",
    "        last_correct = correct\n",
    "        last_guessed_type = guessed_type\n",
    "        last_correct_type = correct_type\n",
    "\n",
    "    if in_correct:\n",
    "        counts.correct_chunk += 1\n",
    "        counts.t_correct_chunk[last_correct_type] += 1\n",
    "\n",
    "    return counts\n",
    "\n",
    "def uniq(iterable):\n",
    "  seen = set()\n",
    "  return [i for i in iterable if not (i in seen or seen.add(i))]\n",
    "\n",
    "def calculate_metrics(correct, guessed, total):\n",
    "    tp, fp, fn = correct, guessed-correct, total-correct\n",
    "    p = 0 if tp + fp == 0 else 1.*tp / (tp + fp)\n",
    "    r = 0 if tp + fn == 0 else 1.*tp / (tp + fn)\n",
    "    f = 0 if p + r == 0 else 2 * p * r / (p + r)\n",
    "    return Metrics(tp, fp, fn, p, r, f)\n",
    "\n",
    "def metrics(counts):\n",
    "    c = counts\n",
    "    overall = calculate_metrics(\n",
    "        c.correct_chunk, c.found_guessed, c.found_correct\n",
    "    )\n",
    "    by_type = {}\n",
    "    for t in uniq(list(c.t_found_correct.keys()) + list(c.t_found_guessed.keys())):\n",
    "        by_type[t] = calculate_metrics(\n",
    "            c.t_correct_chunk[t], c.t_found_guessed[t], c.t_found_correct[t]\n",
    "        )\n",
    "    return overall, by_type\n",
    "\n",
    "def report(counts, out=None):\n",
    "    if out is None:\n",
    "        out = sys.stdout\n",
    "\n",
    "    overall, by_type = metrics(counts)\n",
    "\n",
    "    c = counts\n",
    "    out.write('processed %d tokens with %d phrases; ' %\n",
    "              (c.token_counter, c.found_correct))\n",
    "    out.write('found: %d phrases; correct: %d.\\n' %\n",
    "              (c.found_guessed, c.correct_chunk))\n",
    "\n",
    "    if c.token_counter > 0:\n",
    "        out.write('accuracy: %6.2f%%; ' %\n",
    "                  (100.*c.correct_tags/c.token_counter))\n",
    "        out.write('precision: %6.2f%%; ' % (100.*overall.prec))\n",
    "        out.write('recall: %6.2f%%; ' % (100.*overall.rec))\n",
    "        out.write('FB1: %6.2f\\n' % (100.*overall.fscore))\n",
    "\n",
    "    for i, m in sorted(by_type.items()):\n",
    "        out.write('%17s: ' % i)\n",
    "        out.write('precision: %6.2f%%; ' % (100.*m.prec))\n",
    "        out.write('recall: %6.2f%%; ' % (100.*m.rec))\n",
    "        out.write('FB1: %6.2f  %d\\n' % (100.*m.fscore, c.t_found_guessed[i]))\n",
    "\n",
    "def end_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "    # check if a chunk ended between the previous and current word\n",
    "    # arguments: previous and current chunk tags, previous and current types\n",
    "    chunk_end = False\n",
    "\n",
    "    if prev_tag == 'E': chunk_end = True\n",
    "    if prev_tag == 'S': chunk_end = True\n",
    "\n",
    "    if prev_tag == 'B' and tag == 'B': chunk_end = True\n",
    "    if prev_tag == 'B' and tag == 'S': chunk_end = True\n",
    "    if prev_tag == 'B' and tag == 'O': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'B': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'S': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'O': chunk_end = True\n",
    "\n",
    "    if prev_tag != 'O' and prev_tag != '.' and prev_type != type_:\n",
    "        chunk_end = True\n",
    "\n",
    "    # these chunks are assumed to have length 1\n",
    "    if prev_tag == ']': chunk_end = True\n",
    "    if prev_tag == '[': chunk_end = True\n",
    "\n",
    "    return chunk_end\n",
    "\n",
    "def start_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "    # check if a chunk started between the previous and current word\n",
    "    # arguments: previous and current chunk tags, previous and current types\n",
    "    chunk_start = False\n",
    "\n",
    "    if tag == 'B': chunk_start = True\n",
    "    if tag == 'S': chunk_start = True\n",
    "\n",
    "    if prev_tag == 'E' and tag == 'E': chunk_start = True\n",
    "    if prev_tag == 'E' and tag == 'I': chunk_start = True\n",
    "    if prev_tag == 'S' and tag == 'E': chunk_start = True\n",
    "    if prev_tag == 'S' and tag == 'I': chunk_start = True\n",
    "    if prev_tag == 'O' and tag == 'E': chunk_start = True\n",
    "    if prev_tag == 'O' and tag == 'I': chunk_start = True\n",
    "\n",
    "    if tag != 'O' and tag != '.' and prev_type != type_:\n",
    "        chunk_start = True\n",
    "\n",
    "    # these chunks are assumed to have length 1\n",
    "    if tag == '[': chunk_start = True\n",
    "    if tag == ']': chunk_start = True\n",
    "\n",
    "    return chunk_start\n",
    "\n",
    "def main(argv):\n",
    "    args = parse_args(argv[1:])\n",
    "\n",
    "    if args.file is None:\n",
    "        counts = evaluate(sys.stdin, args)\n",
    "    else:\n",
    "        with open(args.file) as f:\n",
    "            counts = evaluate(f, args)\n",
    "    report(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wK87lDzX1pyT"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import collections\n",
    "import numpy\n",
    "\n",
    "class SequenceLabelingEvaluator(object):\n",
    "    def __init__(self, main_label, label2id, conll_eval=False):\n",
    "        self.main_label = main_label\n",
    "        self.label2id = label2id\n",
    "        self.conll_eval = conll_eval\n",
    "        self.main_label_id = self.label2id[self.main_label]\n",
    "\n",
    "        self.cost_sum = 0.0\n",
    "        self.correct_sum = 0.0\n",
    "        self.main_predicted_count = 0\n",
    "        self.main_total_count = 0\n",
    "        self.main_correct_count = 0\n",
    "        self.token_count = 0\n",
    "        self.start_time = time.time()\n",
    "\n",
    "        self.id2label = collections.OrderedDict()\n",
    "        for label in self.label2id:\n",
    "            self.id2label[self.label2id[label]] = label\n",
    "\n",
    "        self.conll_format = []\n",
    "        self.return_data = []\n",
    "        self.incorrect_counter = 0\n",
    "\n",
    "    def append_data(self, cost, batch, predicted_labels):\n",
    "        self.cost_sum += cost\n",
    "        for i in range(len(batch)):\n",
    "            for j in range(len(batch[i])):\n",
    "                token = batch[i][j][0]\n",
    "                gold_label = batch[i][j][-1]\n",
    "                predicted_label = self.id2label[predicted_labels[i][j]]\n",
    "\n",
    "                self.token_count += 1\n",
    "                if gold_label == predicted_label:\n",
    "                    self.correct_sum += 1\n",
    "                if predicted_label == self.main_label:\n",
    "                    self.main_predicted_count += 1\n",
    "                if gold_label == self.main_label:\n",
    "                    self.main_total_count += 1\n",
    "                if predicted_label == gold_label and gold_label == self.main_label:\n",
    "                    self.main_correct_count += 1\n",
    "\n",
    "                self.conll_format.append(token + \"\\t\" + gold_label + \"\\t\" + predicted_label)\n",
    "            self.conll_format.append(\"\")\n",
    "\n",
    "    def append_data_labelling(self, cost, batch, predicted_labels, remove_list):\n",
    "        sentence_counter = 0\n",
    "        self.cost_sum += cost\n",
    "        for i in range(len(batch)):\n",
    "            if i in remove_list:\n",
    "              continue\n",
    "            sentence_counter += 1 \n",
    "            for j in range(len(batch[i])):\n",
    "                token = batch[i][j][0]\n",
    "                gold_label = batch[i][j][-1]\n",
    "                predicted_label = self.id2label[predicted_labels[i][j]]\n",
    "\n",
    "                self.token_count += 1\n",
    "                if gold_label == predicted_label:\n",
    "                    self.correct_sum += 1\n",
    "                if predicted_label == self.main_label:\n",
    "                    self.main_predicted_count += 1\n",
    "                if gold_label == self.main_label:\n",
    "                    self.main_total_count += 1\n",
    "                if predicted_label == gold_label and gold_label == self.main_label:\n",
    "                    self.main_correct_count += 1\n",
    "                if predicted_label == 'i':\n",
    "                    self.incorrect_counter += 1 \n",
    "\n",
    "                self.return_data.append(token + \"\\t\" + predicted_label)\n",
    "          \n",
    "            self.return_data.append(\"\")\n",
    "            \n",
    "        return self.return_data, self.incorrect_counter, sentence_counter\n",
    "      \n",
    "    def get_results(self, name):\n",
    "        p = (float(self.main_correct_count) / float(self.main_predicted_count)) if (self.main_predicted_count > 0) else 0.0\n",
    "        r = (float(self.main_correct_count) / float(self.main_total_count)) if (self.main_total_count > 0) else 0.0\n",
    "        f = (2.0 * p * r / (p + r)) if (p+r > 0.0) else 0.0\n",
    "        f05 = ((1.0 + 0.5*0.5) * p * r / ((0.5*0.5 * p) + r)) if (p+r > 0.0) else 0.0\n",
    "\n",
    "        results = collections.OrderedDict()\n",
    "        #results[name + \"_cost_avg\"] = self.cost_sum / float(self.token_count)\n",
    "        #results[name + \"_cost_sum\"] = self.cost_sum\n",
    "        #results[name + \"_main_predicted_count\"] = self.main_predicted_count\n",
    "        #results[name + \"_main_total_count\"] = self.main_total_count\n",
    "        #results[name + \"_main_correct_count\"] = self.main_correct_count\n",
    "        #results[name + \"_p\"] = p\n",
    "        #results[name + \"_r\"] = r\n",
    "        #results[name + \"_f\"] = f\n",
    "        results[name + \"_f05\"] = f05\n",
    "        #results[name + \"_accuracy\"] = self.correct_sum / float(self.token_count)\n",
    "        #results[name + \"_token_count\"] = self.token_count\n",
    "        results[name + \"_time\"] = float(time.time()) - float(self.start_time)\n",
    "\n",
    "        if self.label2id is not None and self.conll_eval == True:\n",
    "            conll_counts = conlleval.evaluate(self.conll_format)\n",
    "            conll_metrics_overall, conll_metrics_by_type = conlleval.metrics(conll_counts)\n",
    "            results[name + \"_conll_accuracy\"] = float(conll_counts.correct_tags) / float(conll_counts.token_counter)\n",
    "            results[name + \"_conll_p\"] = conll_metrics_overall.prec\n",
    "            results[name + \"_conll_r\"] = conll_metrics_overall.rec\n",
    "            results[name + \"_conll_f\"] = conll_metrics_overall.fscore\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "colab_type": "code",
    "id": "kUPehDAi14hK",
    "outputId": "1890559d-488c-4b46-d0e7-cd3dc6be3f9f"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import collections\n",
    "import numpy\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import gc\n",
    "\n",
    "try:\n",
    "    import ConfigParser as configparser\n",
    "except:\n",
    "    import configparser\n",
    "\n",
    "def read_input_files(file_paths, max_sentence_length=-1, return_splits=False):\n",
    "    \"\"\"\n",
    "    Reads input files in whitespace-separated format.\n",
    "    Will split file_paths on comma, reading from multiple files.\n",
    "    The format assumes the first column is the word, the last column is the label.\n",
    "    If return_splits=True, also returns a vector of 'split_points' by signifying the sentence_id of the last sentence of each file.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    line_length = None\n",
    "    split_points = []\n",
    "    counter = -1    \n",
    "    for file_path in file_paths.strip().split(\",\"):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            sentence = []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if len(line) > 0:\n",
    "                    line_parts = line.split()\n",
    "                    assert(len(line_parts) >= 2)\n",
    "                    assert(len(line_parts) == line_length or line_length == None)\n",
    "                    line_length = len(line_parts)\n",
    "                    sentence.append(line_parts)\n",
    "                elif len(line) == 0 and len(sentence) > 0:\n",
    "                    if max_sentence_length <= 0 or len(sentence) <= max_sentence_length:\n",
    "                        sentences.append(sentence)\n",
    "                        counter += 1\n",
    "                    sentence = []\n",
    "            if len(sentence) > 0:\n",
    "                if max_sentence_length <= 0 or len(sentence) <= max_sentence_length:\n",
    "                    sentences.append(sentence)\n",
    "                    counter += 1\n",
    "        split_points += [counter]\n",
    "\n",
    "    if return_splits:\n",
    "        return sentences, split_points\n",
    "    else:    \n",
    "        return sentences\n",
    "\n",
    "\n",
    "\n",
    "def parse_config(config_section, config_path):\n",
    "    \"\"\"\n",
    "    Reads configuration from the file and returns a dictionary.\n",
    "    Tries to guess the correct datatype for each of the config values.\n",
    "    \"\"\"\n",
    "    config_parser = configparser.SafeConfigParser(allow_no_value=True)\n",
    "    config_parser.read(config_path)\n",
    "    config = collections.OrderedDict()\n",
    "    for key, value in config_parser.items(config_section):\n",
    "        if value is None or len(value.strip()) == 0:\n",
    "            config[key] = None\n",
    "        elif value.lower() in [\"true\", \"false\"]:\n",
    "            config[key] = config_parser.getboolean(config_section, key)\n",
    "        elif value.isdigit():\n",
    "            config[key] = config_parser.getint(config_section, key)\n",
    "        elif is_float(value):\n",
    "            config[key] = config_parser.getfloat(config_section, key)\n",
    "        else:\n",
    "            config[key] = config_parser.get(config_section, key)\n",
    "    return config\n",
    "\n",
    "\n",
    "def is_float(value):\n",
    "    \"\"\"\n",
    "    Check in value is of type float()\n",
    "    \"\"\"\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def create_batches_of_sentence_ids(sentences, batch_equal_size, max_batch_size):\n",
    "    \"\"\"\n",
    "    Groups together sentences into batches\n",
    "    If batch_equal_size is True, make all sentences in a batch be equal length.\n",
    "    If max_batch_size is positive, this value determines the maximum number of sentences in each batch.\n",
    "    If max_batch_size has a negative value, the function dynamically creates the batches such that each batch contains abs(max_batch_size) words.\n",
    "    Returns a list of lists with sentences ids.\n",
    "    \"\"\"\n",
    "    batches_of_sentence_ids = []\n",
    "    if batch_equal_size == True:\n",
    "        sentence_ids_by_length = collections.OrderedDict()\n",
    "        sentence_length_sum = 0.0\n",
    "        for i in range(len(sentences)):\n",
    "            length = len(sentences[i])\n",
    "            if length not in sentence_ids_by_length:\n",
    "                sentence_ids_by_length[length] = []\n",
    "            sentence_ids_by_length[length].append(i)\n",
    "\n",
    "        for sentence_length in sentence_ids_by_length:\n",
    "            if max_batch_size > 0:\n",
    "                batch_size = max_batch_size\n",
    "            else:\n",
    "                batch_size = int((-1.0 * max_batch_size) / sentence_length)\n",
    "\n",
    "            for i in range(0, len(sentence_ids_by_length[sentence_length]), batch_size):\n",
    "                batches_of_sentence_ids.append(sentence_ids_by_length[sentence_length][i:i + batch_size])\n",
    "    else:\n",
    "        current_batch = []\n",
    "        max_sentence_length = 0\n",
    "        for i in range(len(sentences)):\n",
    "            current_batch.append(i)\n",
    "            if len(sentences[i]) > max_sentence_length:\n",
    "                max_sentence_length = len(sentences[i])\n",
    "            if (max_batch_size > 0 and len(current_batch) >= max_batch_size) \\\n",
    "              or (max_batch_size <= 0 and len(current_batch)*max_sentence_length >= (-1 * max_batch_size)):\n",
    "                batches_of_sentence_ids.append(current_batch)\n",
    "                current_batch = []\n",
    "                max_sentence_length = 0\n",
    "        if len(current_batch) > 0:\n",
    "            batches_of_sentence_ids.append(current_batch)\n",
    "    return batches_of_sentence_ids\n",
    "\n",
    "\n",
    "\n",
    "def process_sentences(data, labeler, is_training, learningrate, config, name):\n",
    "    \"\"\"\n",
    "    Process all the sentences with the labeler, return evaluation metrics.\n",
    "    \"\"\"\n",
    "    evaluator = SequenceLabelingEvaluator(config[\"main_label\"], labeler.label2id, config[\"conll_eval\"])\n",
    "    batches_of_sentence_ids = create_batches_of_sentence_ids(data, config[\"batch_equal_size\"], config[\"max_batch_size\"])\n",
    "    if is_training == True:\n",
    "        random.shuffle(batches_of_sentence_ids)\n",
    "\n",
    "    for sentence_ids_in_batch in batches_of_sentence_ids:\n",
    "        batch = [data[i] for i in sentence_ids_in_batch]\n",
    "        cost, predicted_labels, predicted_probs = labeler.process_batch(batch, is_training, learningrate)\n",
    "\n",
    "        evaluator.append_data(cost, batch, predicted_labels)\n",
    "\n",
    "        word_ids, char_ids, char_mask, label_ids = None, None, None, None\n",
    "        while config[\"garbage_collection\"] == True and gc.collect() > 0:\n",
    "            pass\n",
    "\n",
    "    results = evaluator.get_results(name)\n",
    "    for key in results:\n",
    "        print(key + \": \" + str(results[key]))\n",
    "\n",
    "    return results\n",
    "\n",
    "def process_sentences_labelling(data, labeler, is_training, learningrate, config, name, ReturnData = False):\n",
    "    \"\"\"\n",
    "    Process all the sentences with the labeler, return evaluation metrics.\n",
    "    \"\"\"\n",
    "    overall_sent_count = 0\n",
    "    evaluator = SequenceLabelingEvaluator(config[\"main_label\"], labeler.label2id, config[\"conll_eval\"])\n",
    "    batches_of_sentence_ids = create_batches_of_sentence_ids(data, config[\"batch_equal_size\"], config[\"max_batch_size\"])\n",
    "    if is_training == True:\n",
    "        random.shuffle(batches_of_sentence_ids)\n",
    "\n",
    "    for sentence_ids_in_batch in batches_of_sentence_ids:\n",
    "        batch = [data[i] for i in sentence_ids_in_batch]\n",
    "        cost, predicted_labels, predicted_probs, remove_list = labeler.process_batch_labelling(batch, is_training, learningrate)\n",
    "\n",
    "        processed_data, incorrect_counter, sentence_counter = evaluator.append_data_labelling(cost, batch, predicted_labels, remove_list)\n",
    "        overall_sent_count += sentence_counter \n",
    "        \n",
    "        word_ids, char_ids, char_mask, label_ids = None, None, None, None\n",
    "        while config[\"garbage_collection\"] == True and gc.collect() > 0:\n",
    "            pass\n",
    "\n",
    "    results = evaluator.get_results(name)\n",
    "    for key in results:\n",
    "        print(key + \": \" + str(results[key]))\n",
    "        \n",
    "    if ReturnData:\n",
    "      return results, processed_data, incorrect_counter, overall_sent_count\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_labels(config_path, load_path):\n",
    "    config = parse_config(\"config\", config_path)\n",
    "    temp_model_path = config_path + \".model\"\n",
    "    \n",
    "    #Restore the best model \n",
    "    labeler = SequenceLabeler.load(load_path)\n",
    "\n",
    "    #Label the data in the 'label_path' section \n",
    "    if config[\"path_label\"] is not None:\n",
    "        for path_test in config[\"path_label\"].strip().split(\":\"):\n",
    "            data_test = read_input_files(path_test)\n",
    "            results_test, processed_data, incorrect_counter, sent_count = process_sentences_labelling(data_test, labeler, is_training=False, learningrate=0.0, config=config, name=\"test\"+str(i), ReturnData = True)\n",
    "            evaluator_file = open('/content/drive/My Drive/beamsearch_remove_60', 'w')\n",
    "            for j in range(len(processed_data)):\n",
    "              evaluator_file.write(processed_data[j] + \"\\n\")\n",
    "            evaluator_file.close()\n",
    "            print('Number of incorrect tokens: ', incorrect_counter)\n",
    "            print('Number of sentences: ', sent_count)\n",
    "              \n",
    "def run_experiment(config_path):\n",
    "    config = parse_config(\"config\", config_path)\n",
    "    temp_model_path = config_path + \".model\"\n",
    "    if \"random_seed\" in config:\n",
    "        random.seed(config[\"random_seed\"])\n",
    "        numpy.random.seed(config[\"random_seed\"])\n",
    "\n",
    "    for key, val in config.items():\n",
    "        print(str(key) + \": \" + str(val))\n",
    "\n",
    "    data_train, data_dev, data_test = None, None, None\n",
    "    if config[\"path_train\"] != None and len(config[\"path_train\"]) > 0:\n",
    "        if config['alternating_training']:\n",
    "            # implements dataset-switching, i.e. first trains on the 'main' dataset, then on the augmented dataset in similar sized chunks\n",
    "            data_train, split_points = read_input_files(config[\"path_train\"], config[\"max_train_sent_length\"], return_splits=True)\n",
    "            main_train = data_train[:split_points[0]]\n",
    "            data_train = data_train[split_points[0]:]\n",
    "            random.shuffle(data_train)  # shuffle all augmented data\n",
    "            data_train = main_train + data_train\n",
    "            minibatch_size = split_points[0]\n",
    "            minibatches = []\n",
    "            for batch_start_index in range(0, len(data_train), minibatch_size):\n",
    "                minibatches += [data_train[batch_start_index : batch_start_index+minibatch_size]]\n",
    "            if len(minibatches[-1]) < 0.5 * minibatch_size:\n",
    "                minibatches[-2] = minibatches[-2] + minibatches[-1]\n",
    "                minibatches = minibatches[:-1]    # merge last minibatch with previous, if too small\n",
    "        else:\n",
    "            data_train = read_input_files(config[\"path_train\"], config[\"max_train_sent_length\"])\n",
    "            minibatches = [data_train]\n",
    "        print(\"minibatch sizes: \"+\", \".join([str(len(i)) for i in minibatches]))\n",
    "\n",
    "    if config[\"path_dev\"] != None and len(config[\"path_dev\"]) > 0:\n",
    "        data_dev = read_input_files(config[\"path_dev\"])\n",
    "    if config[\"path_test\"] != None and len(config[\"path_test\"]) > 0:\n",
    "        data_test = []\n",
    "        for path_test in config[\"path_test\"].strip().split(\":\"):\n",
    "            data_test += read_input_files(path_test)\n",
    "\n",
    "    if config[\"load\"] != None and len(config[\"load\"]) > 0:\n",
    "        labeler = SequenceLabeler.load(config[\"load\"])\n",
    "    else:\n",
    "        labeler = SequenceLabeler(config)\n",
    "        labeler.build_vocabs(data_train, data_dev, data_test, config[\"preload_vectors\"])\n",
    "        labeler.construct_network()\n",
    "        labeler.initialize_session()\n",
    "        if config[\"preload_vectors\"] != None:\n",
    "            labeler.preload_word_embeddings(config[\"preload_vectors\"])\n",
    "\n",
    "    print(\"parameter_count: \" + str(labeler.get_parameter_count()))\n",
    "    print(\"parameter_count_without_word_embeddings: \" + str(labeler.get_parameter_count_without_word_embeddings()))\n",
    "\n",
    "    if data_train != None:\n",
    "        model_selector = config[\"model_selector\"].split(\":\")[0]\n",
    "        model_selector_type = config[\"model_selector\"].split(\":\")[1]\n",
    "        no_improvement_for = 0\n",
    "        best_selector_value = 0.0\n",
    "        best_epoch = -1\n",
    "        learningrate = config[\"learningrate\"]\n",
    "        for epoch in range(config[\"epochs\"]):\n",
    "            print(\"EPOCH: \" + str(epoch))\n",
    "            for batchno, minibatch in enumerate(minibatches):\n",
    "                print(\"BATCH: \" + str(batchno))\n",
    "                print(\"current_learningrate: \" + str(learningrate))    \n",
    "                random.shuffle(minibatch)\n",
    "                results_train = process_sentences(minibatch, labeler, is_training=True, learningrate=learningrate, config=config, name=\"train\")\n",
    "\n",
    "                if data_dev != None:\n",
    "                    results_dev = process_sentences(data_dev, labeler, is_training=False, learningrate=0.0, config=config, name=\"dev\")\n",
    "                    no_improvement_for += 1\n",
    "\n",
    "                    #if math.isnan(results_dev[\"dev_cost_sum\"]) or math.isinf(results_dev[\"dev_cost_sum\"]):\n",
    "                    #    sys.stderr.write(\"ERROR: Cost is NaN or Inf. Exiting.\\n\")\n",
    "                    #    break\n",
    "\n",
    "                    if ((epoch == 0 and batchno == 0) or (model_selector_type == \"high\" and results_dev[model_selector] > best_selector_value) \n",
    "                                or (model_selector_type == \"low\" and results_dev[model_selector] < best_selector_value)):\n",
    "                        best_epoch = epoch\n",
    "                        best_batch = batchno\n",
    "                        no_improvement_for = 0\n",
    "                        best_selector_value = results_dev[model_selector]\n",
    "                        labeler.saver.save(labeler.session, temp_model_path, latest_filename=os.path.basename(temp_model_path)+\".checkpoint\")\n",
    "                    print(\"best_epoch and best_batch: \" + str(best_epoch) + \"-\" + str(best_batch))\n",
    "                    print(\"no improvement for: \" + str(no_improvement_for))\n",
    "\n",
    "                if no_improvement_for > config[\"learningrate_delay\"]:\n",
    "                    learningrate *= config[\"learningrate_decay\"]\n",
    "\n",
    "                if config[\"stop_if_no_improvement_for_epochs\"] > 0 and no_improvement_for >= config[\"stop_if_no_improvement_for_epochs\"]:\n",
    "                    break\n",
    "\n",
    "            if config[\"stop_if_no_improvement_for_epochs\"] > 0 and no_improvement_for >= config[\"stop_if_no_improvement_for_epochs\"]:\n",
    "                break\n",
    "\n",
    "            while config[\"garbage_collection\"] == True and gc.collect() > 0:\n",
    "                pass\n",
    "\n",
    "        if data_dev != None and best_epoch >= 0:\n",
    "            # loading the best model so far\n",
    "            labeler.saver.restore(labeler.session, temp_model_path)\n",
    "\n",
    "            os.remove(temp_model_path+\".checkpoint\")\n",
    "            os.remove(temp_model_path+\".data-00000-of-00001\")\n",
    "            os.remove(temp_model_path+\".index\")\n",
    "            os.remove(temp_model_path+\".meta\")\n",
    "\n",
    "    if config[\"save\"] is not None and len(config[\"save\"]) > 0:\n",
    "        labeler.save(config[\"save\"])\n",
    "\n",
    "    if config[\"path_test\"] is not None:\n",
    "        i = 0\n",
    "        for path_test in config[\"path_test\"].strip().split(\":\"):\n",
    "            data_test = read_input_files(path_test)\n",
    "            results_test = process_sentences(data_test, labeler, is_training=False, learningrate=0.0, config=config, name=\"test\"+str(i))\n",
    "            i += 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path1 = '/content/drive/My Drive/save_fce_05'\n",
    "    path2 = '/content/drive/My Drive/beam_search_labelling.conf'\n",
    "    get_labels(path2, path1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Label_Generator.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
