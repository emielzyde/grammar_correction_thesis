{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Label_Runner.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"_4evZSKA1IaM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":118},"outputId":"e3a666e6-0d43-46d1-a764-29f714d10382","executionInfo":{"status":"ok","timestamp":1564987747369,"user_tz":-120,"elapsed":37588,"user":{"displayName":"Emiel Zyde","photoUrl":"","userId":"05469662566681818171"}}},"source":["#I will attach my Drive so that I can easily load files.\n","from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MBid4x1E1ZWU","colab_type":"code","colab":{}},"source":["import collections\n","import tensorflow as tf\n","import re\n","import numpy\n","from tensorflow.python.framework import ops\n","from tensorflow.python.ops import math_ops\n","\n","try:\n","    import cPickle as pickle\n","except:\n","    import pickle\n","\n","class SequenceLabeler(object):\n","    def __init__(self, config):\n","        self.config = config\n","\n","        self.UNK = \"<unk>\"\n","        self.CUNK = \"<cunk>\"\n","\n","        self.word2id = None\n","        self.char2id = None\n","        self.label2id = None\n","        self.singletons = None\n","\n","\n","    def build_vocabs(self, data_train, data_dev, data_test, embedding_path=None):\n","        data_source = list(data_train)\n","        if self.config[\"vocab_include_devtest\"]:\n","            if data_dev != None:\n","                data_source += data_dev\n","            if data_test != None:\n","                data_source += data_test\n","\n","        char_counter = collections.Counter()\n","        for sentence in data_source:\n","            for word in sentence:\n","                char_counter.update(word[0])\n","        self.char2id = collections.OrderedDict([(self.CUNK, 0)])\n","        for char, count in char_counter.most_common():\n","            if char not in self.char2id:\n","                self.char2id[char] = len(self.char2id)\n","\n","        word_counter = collections.Counter()\n","        for sentence in data_source:\n","            for word in sentence:\n","                w = word[0]\n","                if self.config[\"lowercase\"] == True:\n","                    w = w.lower()\n","                if self.config[\"replace_digits\"] == True:\n","                    w = re.sub(r'\\d', '0', w)\n","                word_counter[w] += 1\n","        self.word2id = collections.OrderedDict([(self.UNK, 0)])\n","        for word, count in word_counter.most_common():\n","            if self.config[\"min_word_freq\"] <= 0 or count >= self.config[\"min_word_freq\"]:\n","                if word not in self.word2id:\n","                    self.word2id[word] = len(self.word2id)\n","\n","        self.singletons = set([word for word in word_counter if word_counter[word] == 1])\n","\n","        label_counter = collections.Counter()\n","        for sentence in data_train: #this one only based on training data\n","            for word in sentence:\n","                label_counter[word[-1]] += 1\n","        self.label2id = collections.OrderedDict()\n","        for label, count in label_counter.most_common():\n","            if label not in self.label2id:\n","                self.label2id[label] = len(self.label2id)\n","\n","        if embedding_path != None and self.config[\"vocab_only_embedded\"] == True:\n","            self.embedding_vocab = set([self.UNK])\n","            with open(embedding_path, 'r') as f:\n","                for line in f:\n","                    line_parts = line.strip().split()\n","                    if len(line_parts) <= 2:\n","                        continue\n","                    w = line_parts[0]\n","                    if self.config[\"lowercase\"] == True:\n","                        w = w.lower()\n","                    if self.config[\"replace_digits\"] == True:\n","                        w = re.sub(r'\\d', '0', w)\n","                    self.embedding_vocab.add(w)\n","            word2id_revised = collections.OrderedDict()\n","            for word in self.word2id:\n","                if word in embedding_vocab and word not in word2id_revised:\n","                    word2id_revised[word] = len(word2id_revised)\n","            self.word2id = word2id_revised\n","\n","        print(\"n_words: \" + str(len(self.word2id)))\n","        print(\"n_chars: \" + str(len(self.char2id)))\n","        print(\"n_labels: \" + str(len(self.label2id)))\n","        print(\"n_singletons: \" + str(len(self.singletons)))\n","\n","\n","    def construct_network(self):\n","        self.word_ids = tf.placeholder(tf.int32, [None, None], name=\"word_ids\")\n","        self.char_ids = tf.placeholder(tf.int32, [None, None, None], name=\"char_ids\")\n","        self.sentence_lengths = tf.placeholder(tf.int32, [None], name=\"sentence_lengths\")\n","        self.word_lengths = tf.placeholder(tf.int32, [None, None], name=\"word_lengths\")\n","        self.label_ids = tf.placeholder(tf.int32, [None, None], name=\"label_ids\")\n","        self.learningrate = tf.placeholder(tf.float32, name=\"learningrate\")\n","        self.is_training = tf.placeholder(tf.int32, name=\"is_training\")\n","\n","        self.loss = 0.0\n","        input_tensor = None\n","        input_vector_size = 0\n","\n","        self.initializer = None\n","        if self.config[\"initializer\"] == \"normal\":\n","            self.initializer = tf.random_normal_initializer(mean=0.0, stddev=0.1)\n","        elif self.config[\"initializer\"] == \"glorot\":\n","            self.initializer = tf.glorot_uniform_initializer()\n","        elif self.config[\"initializer\"] == \"xavier\":\n","            self.initializer = tf.glorot_normal_initializer()\n","        else:\n","            raise ValueError(\"Unknown initializer\")\n","\n","        self.word_embeddings = tf.get_variable(\"word_embeddings\", \n","            shape=[len(self.word2id), self.config[\"word_embedding_size\"]], \n","            initializer=(tf.zeros_initializer() if self.config[\"emb_initial_zero\"] == True else self.initializer), \n","            trainable=(True if self.config[\"train_embeddings\"] == True else False))\n","        input_tensor = tf.nn.embedding_lookup(self.word_embeddings, self.word_ids)\n","        input_vector_size = self.config[\"word_embedding_size\"]\n","\n","        if self.config[\"char_embedding_size\"] > 0 and self.config[\"char_recurrent_size\"] > 0:\n","            with tf.variable_scope(\"chars\"), tf.control_dependencies([tf.assert_equal(tf.shape(self.char_ids)[2], tf.reduce_max(self.word_lengths), message=\"Char dimensions don't match\")]):\n","                self.char_embeddings = tf.get_variable(\"char_embeddings\", \n","                    shape=[len(self.char2id), self.config[\"char_embedding_size\"]], \n","                    initializer=self.initializer, \n","                    trainable=True)\n","                char_input_tensor = tf.nn.embedding_lookup(self.char_embeddings, self.char_ids)\n","\n","                s = tf.shape(char_input_tensor)\n","                char_input_tensor = tf.reshape(char_input_tensor, shape=[s[0]*s[1], s[2], self.config[\"char_embedding_size\"]])\n","                _word_lengths = tf.reshape(self.word_lengths, shape=[s[0]*s[1]])\n","\n","                char_lstm_cell_fw = tf.nn.rnn_cell.LSTMCell(self.config[\"char_recurrent_size\"], \n","                    use_peepholes=self.config[\"lstm_use_peepholes\"], \n","                    state_is_tuple=True, \n","                    initializer=self.initializer,\n","                    reuse=False)\n","                char_lstm_cell_bw = tf.nn.rnn_cell.LSTMCell(self.config[\"char_recurrent_size\"], \n","                    use_peepholes=self.config[\"lstm_use_peepholes\"], \n","                    state_is_tuple=True, \n","                    initializer=self.initializer,\n","                    reuse=False)\n","\n","                char_lstm_outputs = tf.nn.bidirectional_dynamic_rnn(char_lstm_cell_fw, char_lstm_cell_bw, char_input_tensor, sequence_length=_word_lengths, dtype=tf.float32, time_major=False)\n","                _, ((_, char_output_fw), (_, char_output_bw)) = char_lstm_outputs\n","                char_output_tensor = tf.concat([char_output_fw, char_output_bw], axis=-1)\n","                char_output_tensor = tf.reshape(char_output_tensor, shape=[s[0], s[1], 2 * self.config[\"char_recurrent_size\"]])\n","                char_output_vector_size = 2 * self.config[\"char_recurrent_size\"]\n","\n","                if self.config[\"lmcost_char_gamma\"] > 0.0:\n","                    self.loss += self.config[\"lmcost_char_gamma\"] * self.construct_lmcost(char_output_tensor, char_output_tensor, self.sentence_lengths, self.word_ids, \"separate\", \"lmcost_char_separate\")\n","                if self.config[\"lmcost_joint_char_gamma\"] > 0.0:\n","                    self.loss += self.config[\"lmcost_joint_char_gamma\"] * self.construct_lmcost(char_output_tensor, char_output_tensor, self.sentence_lengths, self.word_ids, \"joint\", \"lmcost_char_joint\")\n","\n","                if self.config[\"char_hidden_layer_size\"] > 0:\n","                    char_hidden_layer_size = self.config[\"word_embedding_size\"] if self.config[\"char_integration_method\"] == \"attention\" else self.config[\"char_hidden_layer_size\"]\n","                    char_output_tensor = tf.layers.dense(char_output_tensor, char_hidden_layer_size, activation=tf.tanh, kernel_initializer=self.initializer)\n","                    char_output_vector_size = char_hidden_layer_size\n","\n","                if self.config[\"char_integration_method\"] == \"concat\":\n","                    input_tensor = tf.concat([input_tensor, char_output_tensor], axis=-1)\n","                    input_vector_size += char_output_vector_size\n","                elif self.config[\"char_integration_method\"] == \"attention\":\n","                    assert(char_output_vector_size == self.config[\"word_embedding_size\"]), \"This method requires the char representation to have the same size as word embeddings\"\n","                    static_input_tensor = tf.stop_gradient(input_tensor)\n","                    is_unk = tf.equal(self.word_ids, self.word2id[self.UNK])\n","                    char_output_tensor_normalised = tf.nn.l2_normalize(char_output_tensor, 2)\n","                    static_input_tensor_normalised = tf.nn.l2_normalize(static_input_tensor, 2)\n","                    cosine_cost = 1.0 - tf.reduce_sum(tf.multiply(char_output_tensor_normalised, static_input_tensor_normalised), axis=2)\n","                    is_padding = tf.logical_not(tf.sequence_mask(self.sentence_lengths, maxlen=tf.shape(self.word_ids)[1]))\n","                    cosine_cost_unk = tf.where(tf.logical_or(is_unk, is_padding), x=tf.zeros_like(cosine_cost), y=cosine_cost)\n","                    self.loss += self.config[\"char_attention_cosine_cost\"] * tf.reduce_sum(cosine_cost_unk)\n","                    attention_evidence_tensor = tf.concat([input_tensor, char_output_tensor], axis=2)\n","                    attention_output = tf.layers.dense(attention_evidence_tensor, self.config[\"word_embedding_size\"], activation=tf.tanh, kernel_initializer=self.initializer)\n","                    attention_output = tf.layers.dense(attention_output, self.config[\"word_embedding_size\"], activation=tf.sigmoid, kernel_initializer=self.initializer)\n","                    input_tensor = tf.multiply(input_tensor, attention_output) + tf.multiply(char_output_tensor, (1.0 - attention_output))\n","                elif self.config[\"char_integration_method\"] == \"none\":\n","                    input_tensor = input_tensor\n","                else:\n","                    raise ValueError(\"Unknown char integration method\")\n","\n","        dropout_input = self.config[\"dropout_input\"] * tf.cast(self.is_training, tf.float32) + (1.0 - tf.cast(self.is_training, tf.float32))\n","        input_tensor =  tf.nn.dropout(input_tensor, dropout_input, name=\"dropout_word\")\n","\n","        word_lstm_cell_fw = tf.nn.rnn_cell.LSTMCell(self.config[\"word_recurrent_size\"], \n","            use_peepholes=self.config[\"lstm_use_peepholes\"], \n","            state_is_tuple=True, \n","            initializer=self.initializer,\n","            reuse=False)\n","        word_lstm_cell_bw = tf.nn.rnn_cell.LSTMCell(self.config[\"word_recurrent_size\"], \n","            use_peepholes=self.config[\"lstm_use_peepholes\"], \n","            state_is_tuple=True, \n","            initializer=self.initializer,\n","            reuse=False)\n","\n","        with tf.control_dependencies([tf.assert_equal(tf.shape(self.word_ids)[1], tf.reduce_max(self.sentence_lengths), message=\"Sentence dimensions don't match\")]):\n","            (lstm_outputs_fw, lstm_outputs_bw), _ = tf.nn.bidirectional_dynamic_rnn(word_lstm_cell_fw, word_lstm_cell_bw, input_tensor, sequence_length=self.sentence_lengths, dtype=tf.float32, time_major=False)\n","\n","        dropout_word_lstm = self.config[\"dropout_word_lstm\"] * tf.cast(self.is_training, tf.float32) + (1.0 - tf.cast(self.is_training, tf.float32))\n","        lstm_outputs_fw =  tf.nn.dropout(lstm_outputs_fw, dropout_word_lstm)\n","        lstm_outputs_bw =  tf.nn.dropout(lstm_outputs_bw, dropout_word_lstm)\n","\n","        if self.config[\"lmcost_lstm_gamma\"] > 0.0:\n","            self.loss += self.config[\"lmcost_lstm_gamma\"] * self.construct_lmcost(lstm_outputs_fw, lstm_outputs_bw, self.sentence_lengths, self.word_ids, \"separate\", \"lmcost_lstm_separate\")\n","        if self.config[\"lmcost_joint_lstm_gamma\"] > 0.0:\n","            self.loss += self.config[\"lmcost_joint_lstm_gamma\"] * self.construct_lmcost(lstm_outputs_fw, lstm_outputs_bw, self.sentence_lengths, self.word_ids, \"joint\", \"lmcost_lstm_joint\")\n","\n","        processed_tensor = tf.concat([lstm_outputs_fw, lstm_outputs_bw], 2)\n","        processed_tensor_size = self.config[\"word_recurrent_size\"] * 2\n","\n","        if self.config[\"hidden_layer_size\"] > 0:\n","            processed_tensor = tf.layers.dense(processed_tensor, self.config[\"hidden_layer_size\"], activation=tf.tanh, kernel_initializer=self.initializer)\n","            processed_tensor_size = self.config[\"hidden_layer_size\"]\n","\n","        self.scores = tf.layers.dense(processed_tensor, len(self.label2id), activation=None, kernel_initializer=self.initializer, name=\"output_ff\")\n","\n","        if self.config[\"crf_on_top\"] == True:\n","            crf_num_tags = self.scores.get_shape()[2].value\n","            self.crf_transition_params = tf.get_variable(\"output_crf_transitions\", [crf_num_tags, crf_num_tags], initializer=self.initializer)\n","            log_likelihood, self.crf_transition_params = tf.contrib.crf.crf_log_likelihood(self.scores, self.label_ids, self.sentence_lengths, transition_params=self.crf_transition_params)\n","            self.loss += self.config[\"main_cost\"] * tf.reduce_sum(-log_likelihood) \n","        else:\n","            self.probabilities = tf.nn.softmax(self.scores)\n","            self.predictions = tf.argmax(self.probabilities, 2)\n","            loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.scores, labels=self.label_ids)\n","            mask = tf.sequence_mask(self.sentence_lengths, maxlen=tf.shape(self.word_ids)[1])\n","            loss_ = tf.boolean_mask(loss_, mask)\n","            self.loss += self.config[\"main_cost\"] * tf.reduce_sum(loss_) \n","\n","        self.train_op = self.construct_optimizer(self.config[\"opt_strategy\"], self.loss, self.learningrate, self.config[\"clip\"])\n","\n","\n","    def construct_lmcost(self, input_tensor_fw, input_tensor_bw, sentence_lengths, target_ids, lmcost_type, name):\n","        with tf.variable_scope(name):\n","            lmcost_max_vocab_size = min(len(self.word2id), self.config[\"lmcost_max_vocab_size\"])\n","            target_ids = tf.where(tf.greater_equal(target_ids, lmcost_max_vocab_size-1), x=(lmcost_max_vocab_size-1)+tf.zeros_like(target_ids), y=target_ids)\n","            cost = 0.0\n","            if lmcost_type == \"separate\":\n","                lmcost_fw_mask = tf.sequence_mask(sentence_lengths, maxlen=tf.shape(target_ids)[1])[:,1:]\n","                lmcost_bw_mask = tf.sequence_mask(sentence_lengths, maxlen=tf.shape(target_ids)[1])[:,:-1]\n","                lmcost_fw = self._construct_lmcost(input_tensor_fw[:,:-1,:], lmcost_max_vocab_size, lmcost_fw_mask, target_ids[:,1:], name=name+\"_fw\")\n","                lmcost_bw = self._construct_lmcost(input_tensor_bw[:,1:,:], lmcost_max_vocab_size, lmcost_bw_mask, target_ids[:,:-1], name=name+\"_bw\")\n","                cost += lmcost_fw + lmcost_bw\n","            elif lmcost_type == \"joint\":\n","                joint_input_tensor = tf.concat([input_tensor_fw[:,:-2,:], input_tensor_bw[:,2:,:]], axis=-1)\n","                lmcost_mask = tf.sequence_mask(sentence_lengths, maxlen=tf.shape(target_ids)[1])[:,1:-1]\n","                cost += self._construct_lmcost(joint_input_tensor, lmcost_max_vocab_size, lmcost_mask, target_ids[:,1:-1], name=name+\"_joint\")\n","            else:\n","                raise ValueError(\"Unknown lmcost_type: \" + str(lmcost_type))\n","            return cost\n","\n","\n","    def _construct_lmcost(self, input_tensor, lmcost_max_vocab_size, lmcost_mask, target_ids, name):\n","        with tf.variable_scope(name):\n","            lmcost_hidden_layer = tf.layers.dense(input_tensor, self.config[\"lmcost_hidden_layer_size\"], activation=tf.tanh, kernel_initializer=self.initializer)\n","            lmcost_output = tf.layers.dense(lmcost_hidden_layer, lmcost_max_vocab_size, activation=None, kernel_initializer=self.initializer)\n","            lmcost_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=lmcost_output, labels=target_ids)\n","            lmcost_loss = tf.where(lmcost_mask, lmcost_loss, tf.zeros_like(lmcost_loss))\n","            return tf.reduce_sum(lmcost_loss)\n","\n","\n","    def construct_optimizer(self, opt_strategy, loss, learningrate, clip):\n","        optimizer = None\n","        if opt_strategy == \"adadelta\":\n","            optimizer = tf.train.AdadeltaOptimizer(learning_rate=learningrate)\n","        elif opt_strategy == \"adam\":\n","            optimizer = tf.train.AdamOptimizer(learning_rate=learningrate)\n","        elif opt_strategy == \"sgd\":\n","            optimizer = tf.train.GradientDescentOptimizer(learning_rate=learningrate)\n","        else:\n","            raise ValueError(\"Unknown optimisation strategy: \" + str(opt_strategy))\n","\n","        if clip > 0.0:\n","            grads, vs     = zip(*optimizer.compute_gradients(loss))\n","            grads, gnorm  = tf.clip_by_global_norm(grads, clip)\n","            train_op = optimizer.apply_gradients(zip(grads, vs))\n","        else:\n","            train_op = optimizer.minimize(loss)\n","        return train_op\n","\n","\n","    def preload_word_embeddings(self, embedding_path):\n","        loaded_embeddings = set()\n","        embedding_matrix = self.session.run(self.word_embeddings)\n","        with open(embedding_path, 'r') as f:\n","            for line in f:\n","                line_parts = line.strip().split()\n","                if len(line_parts) <= 2:\n","                    continue\n","                w = line_parts[0]\n","                if self.config[\"lowercase\"] == True:\n","                    w = w.lower()\n","                if self.config[\"replace_digits\"] == True:\n","                    w = re.sub(r'\\d', '0', w)\n","                if w in self.word2id and w not in loaded_embeddings:\n","                    word_id = self.word2id[w]\n","                    embedding = numpy.array(line_parts[1:])\n","                    embedding_matrix[word_id] = embedding\n","                    loaded_embeddings.add(w)\n","        self.session.run(self.word_embeddings.assign(embedding_matrix))\n","        print(\"n_preloaded_embeddings: \" + str(len(loaded_embeddings)))\n","\n","\n","    def translate2id(self, token, token2id, unk_token, lowercase=False, replace_digits=False, singletons=None, singletons_prob=0.0):\n","        if lowercase == True:\n","            token = token.lower()\n","        if replace_digits == True:\n","            token = re.sub(r'\\d', '0', token)\n","\n","        token_id = None\n","        if singletons != None and token in singletons and token in token2id and unk_token != None and numpy.random.uniform() < singletons_prob:\n","            token_id = token2id[unk_token]\n","        elif token in token2id:\n","            token_id = token2id[token]\n","        elif unk_token != None:\n","            token_id = token2id[unk_token]\n","        else:\n","            raise ValueError(\"Unable to handle value, no UNK token: \" + str(token))\n","        return token_id\n","\n","\n","    def create_input_dictionary_for_batch(self, batch, is_training, learningrate):\n","        sentence_lengths = numpy.array([len(sentence) for sentence in batch])\n","        max_sentence_length = sentence_lengths.max()\n","        max_word_length = numpy.array([numpy.array([len(word[0]) for word in sentence]).max() for sentence in batch]).max()\n","        if self.config[\"allowed_word_length\"] > 0 and self.config[\"allowed_word_length\"] < max_word_length:\n","            max_word_length = min(max_word_length, self.config[\"allowed_word_length\"])\n","\n","        word_ids = numpy.zeros((len(batch), max_sentence_length), dtype=numpy.int32)\n","        char_ids = numpy.zeros((len(batch), max_sentence_length, max_word_length), dtype=numpy.int32)\n","        word_lengths = numpy.zeros((len(batch), max_sentence_length), dtype=numpy.int32)\n","        label_ids = numpy.zeros((len(batch), max_sentence_length), dtype=numpy.int32)\n","\n","        singletons = self.singletons if is_training == True else None\n","        singletons_prob = self.config[\"singletons_prob\"] if is_training == True else 0.0\n","        for i in range(len(batch)):\n","            for j in range(len(batch[i])):\n","                word_ids[i][j] = self.translate2id(batch[i][j][0], self.word2id, self.UNK, lowercase=self.config[\"lowercase\"], replace_digits=self.config[\"replace_digits\"], singletons=singletons, singletons_prob=singletons_prob)\n","                label_ids[i][j] = self.translate2id(batch[i][j][-1], self.label2id, None)\n","                word_lengths[i][j] = min(len(batch[i][j][0]), max_word_length)\n","                for k in range(min(len(batch[i][j][0]), max_word_length)):\n","                    char_ids[i][j][k] = self.translate2id(batch[i][j][0][k], self.char2id, self.CUNK)\n","\n","        input_dictionary = {self.word_ids: word_ids, self.char_ids: char_ids, self.sentence_lengths: sentence_lengths, self.word_lengths: word_lengths, self.label_ids: label_ids, self.learningrate: learningrate, self.is_training: is_training}\n","        return input_dictionary\n","\n","\n","    def viterbi_decode(self, score, transition_params):\n","        trellis = numpy.zeros_like(score)\n","        backpointers = numpy.zeros_like(score, dtype=numpy.int32)\n","        trellis[0] = score[0]\n","\n","        for t in range(1, score.shape[0]):\n","            v = numpy.expand_dims(trellis[t - 1], 1) + transition_params\n","            trellis[t] = score[t] + numpy.max(v, 0)\n","            backpointers[t] = numpy.argmax(v, 0)\n","\n","        viterbi = [numpy.argmax(trellis[-1])]\n","        for bp in reversed(backpointers[1:]):\n","            viterbi.append(bp[viterbi[-1]])\n","        viterbi.reverse()\n","\n","        viterbi_score = numpy.max(trellis[-1])\n","        return viterbi, viterbi_score, trellis\n","\n","\n","    def process_batch(self, batch, is_training, learningrate):\n","        feed_dict = self.create_input_dictionary_for_batch(batch, is_training, learningrate)\n","\n","        if self.config[\"crf_on_top\"] == True:\n","            cost, scores = self.session.run([self.loss, self.scores] + ([self.train_op] if is_training == True else []), feed_dict=feed_dict)[:2]\n","            predicted_labels = []\n","            predicted_probs = []\n","            for i in range(len(batch)):\n","                sentence_length = len(batch[i])\n","                viterbi_seq, viterbi_score, viterbi_trellis = self.viterbi_decode(scores[i], self.session.run(self.crf_transition_params))\n","                predicted_labels.append(viterbi_seq[:sentence_length])\n","                predicted_probs.append(viterbi_trellis[:sentence_length])\n","        else:\n","            cost, predicted_labels_, predicted_probs_ = self.session.run([self.loss, self.predictions, self.probabilities] + ([self.train_op] if is_training == True else []), feed_dict=feed_dict)[:3]\n","            predicted_labels = []\n","            predicted_probs = []\n","            for i in range(len(batch)):\n","                sentence_length = len(batch[i])\n","                predicted_labels.append(predicted_labels_[i][:sentence_length])\n","                predicted_probs.append(predicted_probs_[i][:sentence_length])\n","\n","        return cost, predicted_labels, predicted_probs\n","\n","\n","    def initialize_session(self):\n","        tf.set_random_seed(self.config[\"random_seed\"])\n","        session_config = tf.ConfigProto()\n","        session_config.gpu_options.allow_growth = self.config[\"tf_allow_growth\"]\n","        session_config.gpu_options.per_process_gpu_memory_fraction = self.config[\"tf_per_process_gpu_memory_fraction\"]\n","        self.session = tf.Session(config=session_config)\n","        self.session.run(tf.global_variables_initializer())\n","        self.saver = tf.train.Saver(max_to_keep=1)\n","\n","\n","    def get_parameter_count(self):\n","        total_parameters = 0\n","        for variable in tf.trainable_variables():\n","            shape = variable.get_shape()\n","            variable_parameters = 1\n","            for dim in shape:\n","                variable_parameters *= dim.value\n","            total_parameters += variable_parameters\n","        return total_parameters\n","\n","\n","    def get_parameter_count_without_word_embeddings(self):\n","        shape = self.word_embeddings.get_shape()\n","        variable_parameters = 1\n","        for dim in shape:\n","            variable_parameters *= dim.value\n","        return self.get_parameter_count() - variable_parameters\n","\n","\n","    def save(self, filename):\n","        dump = {}\n","        dump[\"config\"] = self.config\n","        dump[\"UNK\"] = self.UNK\n","        dump[\"CUNK\"] = self.CUNK\n","        dump[\"word2id\"] = self.word2id\n","        dump[\"char2id\"] = self.char2id\n","        dump[\"label2id\"] = self.label2id\n","        dump[\"singletons\"] = self.singletons\n","\n","        dump[\"params\"] = {}\n","        for variable in tf.global_variables():\n","            assert(variable.name not in dump[\"params\"]), \"Error: variable with this name already exists\" + str(variable.name)\n","            dump[\"params\"][variable.name] = self.session.run(variable)\n","        with open(filename, 'wb') as f:\n","            pickle.dump(dump, f, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","\n","    @staticmethod\n","    def load(filename):\n","        with open(filename, 'rb') as f:\n","            dump = pickle.load(f)\n","\n","            # for safety, so we don't overwrite old models\n","            dump[\"config\"][\"save\"] = None\n","\n","            labeler = SequenceLabeler(dump[\"config\"])\n","            labeler.UNK = dump[\"UNK\"]\n","            labeler.CUNK = dump[\"CUNK\"]\n","            labeler.word2id = dump[\"word2id\"]\n","            labeler.char2id = dump[\"char2id\"]\n","            labeler.label2id = dump[\"label2id\"]\n","            labeler.singletons = dump[\"singletons\"]\n","\n","            labeler.construct_network()\n","            labeler.initialize_session()\n","            labeler.load_params(filename)\n","\n","            return labeler\n","\n","\n","    def load_params(self, filename):\n","        with open(filename, 'rb') as f:\n","            dump = pickle.load(f)\n","\n","            for variable in tf.global_variables():\n","                assert(variable.name in dump[\"params\"]), \"Variable not in dump: \" + str(variable.name)\n","                assert(variable.shape == dump[\"params\"][variable.name].shape), \"Variable shape not as expected: \" + str(variable.name) + \" \" + str(variable.shape) + \" \" + str(dump[\"params\"][variable.name].shape)\n","                value = numpy.asarray(dump[\"params\"][variable.name])\n","                self.session.run(variable.assign(value))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v0VwuBuV1k5Y","colab_type":"code","colab":{}},"source":["import sys\n","import re\n","\n","from collections import defaultdict, namedtuple\n","\n","ANY_SPACE = '<SPACE>'\n","\n","class FormatError(Exception):\n","    pass\n","\n","Metrics = namedtuple('Metrics', 'tp fp fn prec rec fscore')\n","\n","class EvalCounts(object):\n","    def __init__(self):\n","        self.correct_chunk = 0    # number of correctly identified chunks\n","        self.correct_tags = 0     # number of correct chunk tags\n","        self.found_correct = 0    # number of chunks in corpus\n","        self.found_guessed = 0    # number of identified chunks\n","        self.token_counter = 0    # token counter (ignores sentence breaks)\n","\n","        # counts by type\n","        self.t_correct_chunk = defaultdict(int)\n","        self.t_found_correct = defaultdict(int)\n","        self.t_found_guessed = defaultdict(int)\n","\n","def parse_args(argv):\n","    import argparse\n","    parser = argparse.ArgumentParser(\n","        description='evaluate tagging results using CoNLL criteria',\n","        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n","    )\n","    arg = parser.add_argument\n","    arg('-b', '--boundary', metavar='STR', default='-X-',\n","        help='sentence boundary')\n","    arg('-d', '--delimiter', metavar='CHAR', default=ANY_SPACE,\n","        help='character delimiting items in input')\n","    arg('-o', '--otag', metavar='CHAR', default='O',\n","        help='alternative outside tag')\n","    arg('file', nargs='?', default=None)\n","    return parser.parse_args(argv)\n","\n","def parse_tag(t):\n","    m = re.match(r'^([^-]*)-(.*)$', t)\n","    return m.groups() if m else (t, '')\n","\n","def evaluate(iterable, options=None):\n","    if options is None:\n","        options = parse_args([])    # use defaults\n","\n","    counts = EvalCounts()\n","    num_features = None       # number of features per line\n","    in_correct = False        # currently processed chunks is correct until now\n","    last_correct = 'O'        # previous chunk tag in corpus\n","    last_correct_type = ''    # type of previously identified chunk tag\n","    last_guessed = 'O'        # previously identified chunk tag\n","    last_guessed_type = ''    # type of previous chunk tag in corpus\n","\n","    for line in iterable:\n","        line = line.rstrip('\\r\\n')\n","\n","        if options.delimiter == ANY_SPACE:\n","            features = line.split()\n","        else:\n","            features = line.split(options.delimiter)\n","\n","        if num_features is None:\n","            num_features = len(features)\n","        elif num_features != len(features) and len(features) != 0:\n","            raise FormatError('unexpected number of features: %d (%d)' %\n","                              (len(features), num_features))\n","\n","        if len(features) == 0 or features[0] == options.boundary:\n","            features = [options.boundary, 'O', 'O']\n","        if len(features) < 3:\n","            raise FormatError('unexpected number of features in line %s' % line)\n","\n","        guessed, guessed_type = parse_tag(features.pop())\n","        correct, correct_type = parse_tag(features.pop())\n","        first_item = features.pop(0)\n","\n","        if first_item == options.boundary:\n","            guessed = 'O'\n","\n","        end_correct = end_of_chunk(last_correct, correct,\n","                                   last_correct_type, correct_type)\n","        end_guessed = end_of_chunk(last_guessed, guessed,\n","                                   last_guessed_type, guessed_type)\n","        start_correct = start_of_chunk(last_correct, correct,\n","                                       last_correct_type, correct_type)\n","        start_guessed = start_of_chunk(last_guessed, guessed,\n","                                       last_guessed_type, guessed_type)\n","\n","        if in_correct:\n","            if (end_correct and end_guessed and\n","                last_guessed_type == last_correct_type):\n","                in_correct = False\n","                counts.correct_chunk += 1\n","                counts.t_correct_chunk[last_correct_type] += 1\n","            elif (end_correct != end_guessed or guessed_type != correct_type):\n","                in_correct = False\n","\n","        if start_correct and start_guessed and guessed_type == correct_type:\n","            in_correct = True\n","\n","        if start_correct:\n","            counts.found_correct += 1\n","            counts.t_found_correct[correct_type] += 1\n","        if start_guessed:\n","            counts.found_guessed += 1\n","            counts.t_found_guessed[guessed_type] += 1\n","        if first_item != options.boundary:\n","            if correct == guessed and guessed_type == correct_type:\n","                counts.correct_tags += 1\n","            counts.token_counter += 1\n","\n","        last_guessed = guessed\n","        last_correct = correct\n","        last_guessed_type = guessed_type\n","        last_correct_type = correct_type\n","\n","    if in_correct:\n","        counts.correct_chunk += 1\n","        counts.t_correct_chunk[last_correct_type] += 1\n","\n","    return counts\n","\n","def uniq(iterable):\n","  seen = set()\n","  return [i for i in iterable if not (i in seen or seen.add(i))]\n","\n","def calculate_metrics(correct, guessed, total):\n","    tp, fp, fn = correct, guessed-correct, total-correct\n","    p = 0 if tp + fp == 0 else 1.*tp / (tp + fp)\n","    r = 0 if tp + fn == 0 else 1.*tp / (tp + fn)\n","    f = 0 if p + r == 0 else 2 * p * r / (p + r)\n","    return Metrics(tp, fp, fn, p, r, f)\n","\n","def metrics(counts):\n","    c = counts\n","    overall = calculate_metrics(\n","        c.correct_chunk, c.found_guessed, c.found_correct\n","    )\n","    by_type = {}\n","    for t in uniq(list(c.t_found_correct.keys()) + list(c.t_found_guessed.keys())):\n","        by_type[t] = calculate_metrics(\n","            c.t_correct_chunk[t], c.t_found_guessed[t], c.t_found_correct[t]\n","        )\n","    return overall, by_type\n","\n","def report(counts, out=None):\n","    if out is None:\n","        out = sys.stdout\n","\n","    overall, by_type = metrics(counts)\n","\n","    c = counts\n","    out.write('processed %d tokens with %d phrases; ' %\n","              (c.token_counter, c.found_correct))\n","    out.write('found: %d phrases; correct: %d.\\n' %\n","              (c.found_guessed, c.correct_chunk))\n","\n","    if c.token_counter > 0:\n","        out.write('accuracy: %6.2f%%; ' %\n","                  (100.*c.correct_tags/c.token_counter))\n","        out.write('precision: %6.2f%%; ' % (100.*overall.prec))\n","        out.write('recall: %6.2f%%; ' % (100.*overall.rec))\n","        out.write('FB1: %6.2f\\n' % (100.*overall.fscore))\n","\n","    for i, m in sorted(by_type.items()):\n","        out.write('%17s: ' % i)\n","        out.write('precision: %6.2f%%; ' % (100.*m.prec))\n","        out.write('recall: %6.2f%%; ' % (100.*m.rec))\n","        out.write('FB1: %6.2f  %d\\n' % (100.*m.fscore, c.t_found_guessed[i]))\n","\n","def end_of_chunk(prev_tag, tag, prev_type, type_):\n","    # check if a chunk ended between the previous and current word\n","    # arguments: previous and current chunk tags, previous and current types\n","    chunk_end = False\n","\n","    if prev_tag == 'E': chunk_end = True\n","    if prev_tag == 'S': chunk_end = True\n","\n","    if prev_tag == 'B' and tag == 'B': chunk_end = True\n","    if prev_tag == 'B' and tag == 'S': chunk_end = True\n","    if prev_tag == 'B' and tag == 'O': chunk_end = True\n","    if prev_tag == 'I' and tag == 'B': chunk_end = True\n","    if prev_tag == 'I' and tag == 'S': chunk_end = True\n","    if prev_tag == 'I' and tag == 'O': chunk_end = True\n","\n","    if prev_tag != 'O' and prev_tag != '.' and prev_type != type_:\n","        chunk_end = True\n","\n","    # these chunks are assumed to have length 1\n","    if prev_tag == ']': chunk_end = True\n","    if prev_tag == '[': chunk_end = True\n","\n","    return chunk_end\n","\n","def start_of_chunk(prev_tag, tag, prev_type, type_):\n","    # check if a chunk started between the previous and current word\n","    # arguments: previous and current chunk tags, previous and current types\n","    chunk_start = False\n","\n","    if tag == 'B': chunk_start = True\n","    if tag == 'S': chunk_start = True\n","\n","    if prev_tag == 'E' and tag == 'E': chunk_start = True\n","    if prev_tag == 'E' and tag == 'I': chunk_start = True\n","    if prev_tag == 'S' and tag == 'E': chunk_start = True\n","    if prev_tag == 'S' and tag == 'I': chunk_start = True\n","    if prev_tag == 'O' and tag == 'E': chunk_start = True\n","    if prev_tag == 'O' and tag == 'I': chunk_start = True\n","\n","    if tag != 'O' and tag != '.' and prev_type != type_:\n","        chunk_start = True\n","\n","    # these chunks are assumed to have length 1\n","    if tag == '[': chunk_start = True\n","    if tag == ']': chunk_start = True\n","\n","    return chunk_start\n","\n","def main(argv):\n","    args = parse_args(argv[1:])\n","\n","    if args.file is None:\n","        counts = evaluate(sys.stdin, args)\n","    else:\n","        with open(args.file) as f:\n","            counts = evaluate(f, args)\n","    report(counts)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wK87lDzX1pyT","colab_type":"code","colab":{}},"source":["import time\n","import collections\n","import numpy\n","\n","class SequenceLabelingEvaluator(object):\n","    def __init__(self, main_label, label2id, conll_eval=False):\n","        self.main_label = main_label\n","        self.label2id = label2id\n","        self.conll_eval = conll_eval\n","        self.main_label_id = self.label2id[self.main_label]\n","\n","        self.cost_sum = 0.0\n","        self.correct_sum = 0.0\n","        self.main_predicted_count = 0\n","        self.main_total_count = 0\n","        self.main_correct_count = 0\n","        self.token_count = 0\n","        self.start_time = time.time()\n","\n","        self.id2label = collections.OrderedDict()\n","        for label in self.label2id:\n","            self.id2label[self.label2id[label]] = label\n","\n","        self.conll_format = []\n","\n","    def append_data(self, cost, batch, predicted_labels):\n","        self.cost_sum += cost\n","        for i in range(len(batch)):\n","            for j in range(len(batch[i])):\n","                token = batch[i][j][0]\n","                gold_label = batch[i][j][-1]\n","                predicted_label = self.id2label[predicted_labels[i][j]]\n","\n","                self.token_count += 1\n","                if gold_label == predicted_label:\n","                    self.correct_sum += 1\n","                if predicted_label == self.main_label:\n","                    self.main_predicted_count += 1\n","                if gold_label == self.main_label:\n","                    self.main_total_count += 1\n","                if predicted_label == gold_label and gold_label == self.main_label:\n","                    self.main_correct_count += 1\n","\n","                self.conll_format.append(token + \"\\t\" + gold_label + \"\\t\" + predicted_label)\n","            self.conll_format.append(\"\")\n","\n","\n","    def get_results(self, name):\n","        p = (float(self.main_correct_count) / float(self.main_predicted_count)) if (self.main_predicted_count > 0) else 0.0\n","        r = (float(self.main_correct_count) / float(self.main_total_count)) if (self.main_total_count > 0) else 0.0\n","        f = (2.0 * p * r / (p + r)) if (p+r > 0.0) else 0.0\n","        f05 = ((1.0 + 0.5*0.5) * p * r / ((0.5*0.5 * p) + r)) if (p+r > 0.0) else 0.0\n","\n","        results = collections.OrderedDict()\n","        #results[name + \"_cost_avg\"] = self.cost_sum / float(self.token_count)\n","        #results[name + \"_cost_sum\"] = self.cost_sum\n","        #results[name + \"_main_predicted_count\"] = self.main_predicted_count\n","        #results[name + \"_main_total_count\"] = self.main_total_count\n","        #results[name + \"_main_correct_count\"] = self.main_correct_count\n","        #results[name + \"_p\"] = p\n","        #results[name + \"_r\"] = r\n","        #results[name + \"_f\"] = f\n","        results[name + \"_f05\"] = f05\n","        #results[name + \"_accuracy\"] = self.correct_sum / float(self.token_count)\n","        #results[name + \"_token_count\"] = self.token_count\n","        results[name + \"_time\"] = float(time.time()) - float(self.start_time)\n","\n","        if self.label2id is not None and self.conll_eval == True:\n","            conll_counts = conlleval.evaluate(self.conll_format)\n","            conll_metrics_overall, conll_metrics_by_type = conlleval.metrics(conll_counts)\n","            results[name + \"_conll_accuracy\"] = float(conll_counts.correct_tags) / float(conll_counts.token_counter)\n","            results[name + \"_conll_p\"] = conll_metrics_overall.prec\n","            results[name + \"_conll_r\"] = conll_metrics_overall.rec\n","            results[name + \"_conll_f\"] = conll_metrics_overall.fscore\n","\n","        return results"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kUPehDAi14hK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"1596ef17-3534-4b12-cd2d-ab5f66c6123c"},"source":["import sys\n","import collections\n","import numpy\n","import random\n","import math\n","import os\n","import gc\n","\n","try:\n","    import ConfigParser as configparser\n","except:\n","    import configparser\n","\n","def read_input_files(file_paths, max_sentence_length=-1, return_splits=False):\n","    \"\"\"\n","    Reads input files in whitespace-separated format.\n","    Will split file_paths on comma, reading from multiple files.\n","    The format assumes the first column is the word, the last column is the label.\n","    If return_splits=True, also returns a vector of 'split_points' by signifying the sentence_id of the last sentence of each file.\n","    \"\"\"\n","    sentences = []\n","    line_length = None\n","    split_points = []\n","    counter = -1    \n","    for file_path in file_paths.strip().split(\",\"):\n","        with open(file_path, \"r\") as f:\n","            sentence = []\n","            for line in f:\n","                line = line.strip()\n","                if len(line) > 0:\n","                    line_parts = line.split()\n","                    assert(len(line_parts) >= 2)\n","                    assert(len(line_parts) == line_length or line_length == None)\n","                    line_length = len(line_parts)\n","                    sentence.append(line_parts)\n","                elif len(line) == 0 and len(sentence) > 0:\n","                    if max_sentence_length <= 0 or len(sentence) <= max_sentence_length:\n","                        sentences.append(sentence)\n","                        counter += 1\n","                    sentence = []\n","            if len(sentence) > 0:\n","                if max_sentence_length <= 0 or len(sentence) <= max_sentence_length:\n","                    sentences.append(sentence)\n","                    counter += 1\n","        split_points += [counter]\n","\n","    if return_splits:\n","        return sentences, split_points\n","    else:    \n","        return sentences\n","\n","\n","\n","def parse_config(config_section, config_path):\n","    \"\"\"\n","    Reads configuration from the file and returns a dictionary.\n","    Tries to guess the correct datatype for each of the config values.\n","    \"\"\"\n","    config_parser = configparser.SafeConfigParser(allow_no_value=True)\n","    config_parser.read(config_path)\n","    config = collections.OrderedDict()\n","    for key, value in config_parser.items(config_section):\n","        if value is None or len(value.strip()) == 0:\n","            config[key] = None\n","        elif value.lower() in [\"true\", \"false\"]:\n","            config[key] = config_parser.getboolean(config_section, key)\n","        elif value.isdigit():\n","            config[key] = config_parser.getint(config_section, key)\n","        elif is_float(value):\n","            config[key] = config_parser.getfloat(config_section, key)\n","        else:\n","            config[key] = config_parser.get(config_section, key)\n","    return config\n","\n","\n","def is_float(value):\n","    \"\"\"\n","    Check in value is of type float()\n","    \"\"\"\n","    try:\n","        float(value)\n","        return True\n","    except ValueError:\n","        return False\n","\n","\n","def create_batches_of_sentence_ids(sentences, batch_equal_size, max_batch_size):\n","    \"\"\"\n","    Groups together sentences into batches\n","    If batch_equal_size is True, make all sentences in a batch be equal length.\n","    If max_batch_size is positive, this value determines the maximum number of sentences in each batch.\n","    If max_batch_size has a negative value, the function dynamically creates the batches such that each batch contains abs(max_batch_size) words.\n","    Returns a list of lists with sentences ids.\n","    \"\"\"\n","    batches_of_sentence_ids = []\n","    if batch_equal_size == True:\n","        sentence_ids_by_length = collections.OrderedDict()\n","        sentence_length_sum = 0.0\n","        for i in range(len(sentences)):\n","            length = len(sentences[i])\n","            if length not in sentence_ids_by_length:\n","                sentence_ids_by_length[length] = []\n","            sentence_ids_by_length[length].append(i)\n","\n","        for sentence_length in sentence_ids_by_length:\n","            if max_batch_size > 0:\n","                batch_size = max_batch_size\n","            else:\n","                batch_size = int((-1.0 * max_batch_size) / sentence_length)\n","\n","            for i in range(0, len(sentence_ids_by_length[sentence_length]), batch_size):\n","                batches_of_sentence_ids.append(sentence_ids_by_length[sentence_length][i:i + batch_size])\n","    else:\n","        current_batch = []\n","        max_sentence_length = 0\n","        for i in range(len(sentences)):\n","            current_batch.append(i)\n","            if len(sentences[i]) > max_sentence_length:\n","                max_sentence_length = len(sentences[i])\n","            if (max_batch_size > 0 and len(current_batch) >= max_batch_size) \\\n","              or (max_batch_size <= 0 and len(current_batch)*max_sentence_length >= (-1 * max_batch_size)):\n","                batches_of_sentence_ids.append(current_batch)\n","                current_batch = []\n","                max_sentence_length = 0\n","        if len(current_batch) > 0:\n","            batches_of_sentence_ids.append(current_batch)\n","    return batches_of_sentence_ids\n","\n","\n","\n","def process_sentences(data, labeler, is_training, learningrate, config, name):\n","    \"\"\"\n","    Process all the sentences with the labeler, return evaluation metrics.\n","    \"\"\"\n","    evaluator = SequenceLabelingEvaluator(config[\"main_label\"], labeler.label2id, config[\"conll_eval\"])\n","    batches_of_sentence_ids = create_batches_of_sentence_ids(data, config[\"batch_equal_size\"], config[\"max_batch_size\"])\n","    if is_training == True:\n","        random.shuffle(batches_of_sentence_ids)\n","\n","    for sentence_ids_in_batch in batches_of_sentence_ids:\n","        batch = [data[i] for i in sentence_ids_in_batch]\n","        cost, predicted_labels, predicted_probs = labeler.process_batch(batch, is_training, learningrate)\n","\n","        evaluator.append_data(cost, batch, predicted_labels)\n","\n","        word_ids, char_ids, char_mask, label_ids = None, None, None, None\n","        while config[\"garbage_collection\"] == True and gc.collect() > 0:\n","            pass\n","\n","    results = evaluator.get_results(name)\n","    for key in results:\n","        print(key + \": \" + str(results[key]))\n","\n","    return results\n","\n","\n","\n","def run_experiment(config_path):\n","    config = parse_config(\"config\", config_path)\n","    temp_model_path = config_path + \".model\"\n","    if \"random_seed\" in config:\n","        random.seed(config[\"random_seed\"])\n","        numpy.random.seed(config[\"random_seed\"])\n","\n","    for key, val in config.items():\n","        print(str(key) + \": \" + str(val))\n","\n","    data_train, data_dev, data_test = None, None, None\n","    if config[\"path_train\"] != None and len(config[\"path_train\"]) > 0:\n","        if config['alternating_training']:\n","            # implements dataset-switching, i.e. first trains on the 'main' dataset, then on the augmented dataset in similar sized chunks\n","            data_train, split_points = read_input_files(config[\"path_train\"], config[\"max_train_sent_length\"], return_splits=True)\n","            main_train = data_train[:split_points[0]]\n","            data_train = data_train[split_points[0]:]\n","            random.shuffle(data_train)  # shuffle all augmented data\n","            data_train = main_train + data_train\n","            minibatch_size = split_points[0]\n","            minibatches = []\n","            for batch_start_index in range(0, len(data_train), minibatch_size):\n","                minibatches += [data_train[batch_start_index : batch_start_index+minibatch_size]]\n","            if len(minibatches[-1]) < 0.5 * minibatch_size:\n","                minibatches[-2] = minibatches[-2] + minibatches[-1]\n","                minibatches = minibatches[:-1]    # merge last minibatch with previous, if too small\n","        else:\n","            data_train = read_input_files(config[\"path_train\"], config[\"max_train_sent_length\"])\n","            minibatches = [data_train]\n","        print(\"minibatch sizes: \"+\", \".join([str(len(i)) for i in minibatches]))\n","\n","    if config[\"path_dev\"] != None and len(config[\"path_dev\"]) > 0:\n","        data_dev = read_input_files(config[\"path_dev\"])\n","    if config[\"path_test\"] != None and len(config[\"path_test\"]) > 0:\n","        data_test = []\n","        for path_test in config[\"path_test\"].strip().split(\":\"):\n","            data_test += read_input_files(path_test)\n","\n","    if config[\"load\"] != None and len(config[\"load\"]) > 0:\n","        labeler = SequenceLabeler.load(config[\"load\"])\n","    else:\n","        labeler = SequenceLabeler(config)\n","        labeler.build_vocabs(data_train, data_dev, data_test, config[\"preload_vectors\"])\n","        labeler.construct_network()\n","        labeler.initialize_session()\n","        if config[\"preload_vectors\"] != None:\n","            labeler.preload_word_embeddings(config[\"preload_vectors\"])\n","\n","    print(\"parameter_count: \" + str(labeler.get_parameter_count()))\n","    print(\"parameter_count_without_word_embeddings: \" + str(labeler.get_parameter_count_without_word_embeddings()))\n","\n","    if data_train != None:\n","        model_selector = config[\"model_selector\"].split(\":\")[0]\n","        model_selector_type = config[\"model_selector\"].split(\":\")[1]\n","        no_improvement_for = 0\n","        best_selector_value = 0.0\n","        best_epoch = -1\n","        learningrate = config[\"learningrate\"]\n","        for epoch in range(config[\"epochs\"]):\n","            print(\"EPOCH: \" + str(epoch))\n","            for batchno, minibatch in enumerate(minibatches):\n","                print(\"BATCH: \" + str(batchno))\n","                print(\"current_learningrate: \" + str(learningrate))    \n","                random.shuffle(minibatch)\n","                results_train = process_sentences(minibatch, labeler, is_training=True, learningrate=learningrate, config=config, name=\"train\")\n","\n","                if data_dev != None:\n","                    results_dev = process_sentences(data_dev, labeler, is_training=False, learningrate=0.0, config=config, name=\"dev\")\n","                    no_improvement_for += 1\n","\n","                    #if math.isnan(results_dev[\"dev_cost_sum\"]) or math.isinf(results_dev[\"dev_cost_sum\"]):\n","                    #    sys.stderr.write(\"ERROR: Cost is NaN or Inf. Exiting.\\n\")\n","                    #    break\n","\n","                    if ((epoch == 0 and batchno == 0) or (model_selector_type == \"high\" and results_dev[model_selector] > best_selector_value) \n","                                or (model_selector_type == \"low\" and results_dev[model_selector] < best_selector_value)):\n","                        best_epoch = epoch\n","                        best_batch = batchno\n","                        no_improvement_for = 0\n","                        best_selector_value = results_dev[model_selector]\n","                        labeler.saver.save(labeler.session, temp_model_path, latest_filename=os.path.basename(temp_model_path)+\".checkpoint\")\n","                    print(\"best_epoch and best_batch: \" + str(best_epoch) + \"-\" + str(best_batch))\n","                    print(\"no improvement for: \" + str(no_improvement_for))\n","\n","                if no_improvement_for > config[\"learningrate_delay\"]:\n","                    learningrate *= config[\"learningrate_decay\"]\n","\n","                if config[\"stop_if_no_improvement_for_epochs\"] > 0 and no_improvement_for >= config[\"stop_if_no_improvement_for_epochs\"]:\n","                    break\n","\n","            if config[\"stop_if_no_improvement_for_epochs\"] > 0 and no_improvement_for >= config[\"stop_if_no_improvement_for_epochs\"]:\n","                break\n","\n","            while config[\"garbage_collection\"] == True and gc.collect() > 0:\n","                pass\n","\n","        if data_dev != None and best_epoch >= 0:\n","            # loading the best model so far\n","            labeler.saver.restore(labeler.session, temp_model_path)\n","\n","            os.remove(temp_model_path+\".checkpoint\")\n","            os.remove(temp_model_path+\".data-00000-of-00001\")\n","            os.remove(temp_model_path+\".index\")\n","            os.remove(temp_model_path+\".meta\")\n","\n","    if config[\"save\"] is not None and len(config[\"save\"]) > 0:\n","        labeler.save(config[\"save\"])\n","\n","    if config[\"path_test\"] is not None:\n","        i = 0\n","        for path_test in config[\"path_test\"].strip().split(\":\"):\n","            data_test = read_input_files(path_test)\n","            results_test = process_sentences(data_test, labeler, is_training=False, learningrate=0.0, config=config, name=\"test\"+str(i))\n","            i += 1\n","\n","if __name__ == \"__main__\":\n","    path1 = '/content/drive/My Drive/argmax_original_consensus_25.conf'\n","    run_experiment(path1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: DeprecationWarning: The SafeConfigParser class has been renamed to ConfigParser in Python 3.2. This alias will be removed in future versions. Use ConfigParser directly instead.\n"],"name":"stderr"},{"output_type":"stream","text":["dataset: fcepublic\n","alternating_training: True\n","path_train: /content/drive/My Drive/fce-public.train.original.tsv,/content/drive/My Drive/argmax_original_consensus_25.tsv\n","path_dev: /content/drive/My Drive/fce-public.dev.original.tsv\n","path_test: /content/drive/My Drive/fce-public.dev.original.tsv:/content/drive/My Drive/fce-public.test.original.tsv:/content/drive/My Drive/nucle.test0.original.tsv:/content/drive/My Drive/nucle.test1.original.tsv\n","conll_eval: False\n","main_label: i\n","model_selector: dev_f05:high\n","preload_vectors: /content/drive/My Drive/glove.6B.300d.txt\n","word_embedding_size: 300\n","crf_on_top: False\n","emb_initial_zero: False\n","train_embeddings: True\n","char_embedding_size: 100\n","word_recurrent_size: 300\n","char_recurrent_size: 100\n","hidden_layer_size: 50\n","char_hidden_layer_size: 50\n","lowercase: True\n","replace_digits: True\n","min_word_freq: -1.0\n","singletons_prob: 0.1\n","allowed_word_length: -1.0\n","max_train_sent_length: 150\n","vocab_include_devtest: True\n","vocab_only_embedded: False\n","initializer: glorot\n","opt_strategy: adadelta\n","learningrate: 1.0\n","clip: 0.0\n","batch_equal_size: False\n","max_batch_size: 64\n","epochs: 200\n","stop_if_no_improvement_for_epochs: 20\n","learningrate_delay: 6\n","learningrate_decay: 0.9\n","dropout_input: 0.5\n","dropout_word_lstm: 0.5\n","tf_per_process_gpu_memory_fraction: 1.0\n","tf_allow_growth: True\n","main_cost: 1.0\n","lmcost_max_vocab_size: 7500\n","lmcost_hidden_layer_size: 50\n","lmcost_lstm_gamma: 0.1\n","lmcost_joint_lstm_gamma: 0.0\n","lmcost_char_gamma: 0.0\n","lmcost_joint_char_gamma: 0.0\n","char_attention_cosine_cost: 1.0\n","char_integration_method: attention\n","save: None\n","load: None\n","garbage_collection: False\n","lstm_use_peepholes: False\n","random_seed: 100\n","minibatch sizes: 40600\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0805 06:49:20.254751 140237188847488 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","W0805 06:49:20.325737 140237188847488 deprecation.py:323] From <ipython-input-2-d0010f47c3cd>:140: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","W0805 06:49:20.328913 140237188847488 deprecation.py:323] From <ipython-input-2-d0010f47c3cd>:147: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n","W0805 06:49:20.331785 140237188847488 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"],"name":"stderr"},{"output_type":"stream","text":["n_words: 14399\n","n_chars: 97\n","n_labels: 2\n","n_singletons: 5277\n"],"name":"stdout"},{"output_type":"stream","text":["W0805 06:49:20.477244 140237188847488 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","W0805 06:49:21.441779 140237188847488 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","W0805 06:49:21.642228 140237188847488 deprecation.py:323] From <ipython-input-2-d0010f47c3cd>:160: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dense instead.\n","W0805 06:49:22.101043 140237188847488 deprecation.py:506] From <ipython-input-2-d0010f47c3cd>:186: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"],"name":"stderr"},{"output_type":"stream","text":["n_preloaded_embeddings: 10206\n","parameter_count: 7088752\n","parameter_count_without_word_embeddings: 2769052\n","EPOCH: 0\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.08104278422810544\n","train_time: 209.31455945968628\n","dev_f05: 0.12299771167048056\n","dev_time: 5.5237836837768555\n","best_epoch and best_batch: 0-0\n","no improvement for: 0\n","EPOCH: 1\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.2932781007414627\n","train_time: 206.886470079422\n","dev_f05: 0.2971246006389776\n","dev_time: 5.134061098098755\n","best_epoch and best_batch: 1-0\n","no improvement for: 0\n","EPOCH: 2\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.35055867426650433\n","train_time: 207.48093128204346\n","dev_f05: 0.36923525977816696\n","dev_time: 5.065644025802612\n","best_epoch and best_batch: 2-0\n","no improvement for: 0\n","EPOCH: 3\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.3748449438670739\n","train_time: 207.5800302028656\n","dev_f05: 0.38707386363636365\n","dev_time: 5.067681550979614\n","best_epoch and best_batch: 3-0\n","no improvement for: 0\n","EPOCH: 4\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.3885405515624204\n","train_time: 205.61034965515137\n","dev_f05: 0.401536312849162\n","dev_time: 5.1130406856536865\n","best_epoch and best_batch: 4-0\n","no improvement for: 0\n","EPOCH: 5\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.4084019785688089\n","train_time: 206.12854528427124\n","dev_f05: 0.4180000000000001\n","dev_time: 5.164378643035889\n","best_epoch and best_batch: 5-0\n","no improvement for: 0\n","EPOCH: 6\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.41899603163108645\n","train_time: 206.50041699409485\n","dev_f05: 0.42002147074610846\n","dev_time: 5.160841941833496\n","best_epoch and best_batch: 6-0\n","no improvement for: 0\n","EPOCH: 7\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.43211856491919887\n","train_time: 208.64507126808167\n","dev_f05: 0.41734088457389434\n","dev_time: 5.241452217102051\n","best_epoch and best_batch: 6-0\n","no improvement for: 1\n","EPOCH: 8\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.4443759034369735\n","train_time: 206.37121295928955\n","dev_f05: 0.4305411030176899\n","dev_time: 5.1923828125\n","best_epoch and best_batch: 8-0\n","no improvement for: 0\n","EPOCH: 9\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.45717887509052385\n","train_time: 208.86834239959717\n","dev_f05: 0.4279818956336528\n","dev_time: 5.194597005844116\n","best_epoch and best_batch: 8-0\n","no improvement for: 1\n","EPOCH: 10\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.4700424526237472\n","train_time: 207.16596364974976\n","dev_f05: 0.4485887096774193\n","dev_time: 5.1818037033081055\n","best_epoch and best_batch: 10-0\n","no improvement for: 0\n","EPOCH: 11\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.48327518106823864\n","train_time: 207.5847134590149\n","dev_f05: 0.46339202965709003\n","dev_time: 5.2103471755981445\n","best_epoch and best_batch: 11-0\n","no improvement for: 0\n","EPOCH: 12\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.4905470317429925\n","train_time: 208.17353868484497\n","dev_f05: 0.4622148288973384\n","dev_time: 5.239329099655151\n","best_epoch and best_batch: 11-0\n","no improvement for: 1\n","EPOCH: 13\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.502213917211683\n","train_time: 207.2954967021942\n","dev_f05: 0.4712100764732343\n","dev_time: 5.143016576766968\n","best_epoch and best_batch: 13-0\n","no improvement for: 0\n","EPOCH: 14\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.5128346848376221\n","train_time: 207.21873664855957\n","dev_f05: 0.4495777446597119\n","dev_time: 5.093609094619751\n","best_epoch and best_batch: 13-0\n","no improvement for: 1\n","EPOCH: 15\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.520226767486404\n","train_time: 207.24833965301514\n","dev_f05: 0.4773534635879218\n","dev_time: 5.185953855514526\n","best_epoch and best_batch: 15-0\n","no improvement for: 0\n","EPOCH: 16\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.53379798236627\n","train_time: 207.47470688819885\n","dev_f05: 0.4686749879980797\n","dev_time: 5.073389768600464\n","best_epoch and best_batch: 15-0\n","no improvement for: 1\n","EPOCH: 17\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.5410206836172619\n","train_time: 205.52188920974731\n","dev_f05: 0.481180496150556\n","dev_time: 4.992992877960205\n","best_epoch and best_batch: 17-0\n","no improvement for: 0\n","EPOCH: 18\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.5494954810174288\n","train_time: 207.70969796180725\n","dev_f05: 0.48411998235553594\n","dev_time: 5.150609970092773\n","best_epoch and best_batch: 18-0\n","no improvement for: 0\n","EPOCH: 19\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.55991423587891\n","train_time: 209.78792095184326\n","dev_f05: 0.4853249475890985\n","dev_time: 5.176265001296997\n","best_epoch and best_batch: 19-0\n","no improvement for: 0\n","EPOCH: 20\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.570214141849609\n","train_time: 209.72590923309326\n","dev_f05: 0.4865956665442527\n","dev_time: 5.170498371124268\n","best_epoch and best_batch: 20-0\n","no improvement for: 0\n","EPOCH: 21\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.5796964380059094\n","train_time: 208.7755434513092\n","dev_f05: 0.49079112599413977\n","dev_time: 5.151542663574219\n","best_epoch and best_batch: 21-0\n","no improvement for: 0\n","EPOCH: 22\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.5877019294232236\n","train_time: 209.88397645950317\n","dev_f05: 0.4800597329585382\n","dev_time: 5.201305866241455\n","best_epoch and best_batch: 21-0\n","no improvement for: 1\n","EPOCH: 23\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.595395790041999\n","train_time: 210.37360525131226\n","dev_f05: 0.48525903203817317\n","dev_time: 5.213655710220337\n","best_epoch and best_batch: 21-0\n","no improvement for: 2\n","EPOCH: 24\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.601256310691523\n","train_time: 208.677170753479\n","dev_f05: 0.4949600608596424\n","dev_time: 5.119649648666382\n","best_epoch and best_batch: 24-0\n","no improvement for: 0\n","EPOCH: 25\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.6107267790160696\n","train_time: 205.60372042655945\n","dev_f05: 0.4865803303303304\n","dev_time: 5.123342514038086\n","best_epoch and best_batch: 24-0\n","no improvement for: 1\n","EPOCH: 26\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.6129036068439673\n","train_time: 207.8318247795105\n","dev_f05: 0.4904426559356137\n","dev_time: 5.180285215377808\n","best_epoch and best_batch: 24-0\n","no improvement for: 2\n","EPOCH: 27\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.6223906611000769\n","train_time: 209.9865484237671\n","dev_f05: 0.4951136816912646\n","dev_time: 5.201803207397461\n","best_epoch and best_batch: 27-0\n","no improvement for: 0\n","EPOCH: 28\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.6289820250180775\n","train_time: 208.9492166042328\n","dev_f05: 0.49928545909253313\n","dev_time: 5.172013998031616\n","best_epoch and best_batch: 28-0\n","no improvement for: 0\n","EPOCH: 29\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.6371090632029607\n","train_time: 210.78045988082886\n","dev_f05: 0.4969314694851688\n","dev_time: 5.2028968334198\n","best_epoch and best_batch: 28-0\n","no improvement for: 1\n","EPOCH: 30\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.6410178499411632\n","train_time: 209.96285891532898\n","dev_f05: 0.4942918596955659\n","dev_time: 5.210148811340332\n","best_epoch and best_batch: 28-0\n","no improvement for: 2\n","EPOCH: 31\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.6500410386306685\n","train_time: 210.74473929405212\n","dev_f05: 0.4934782608695652\n","dev_time: 5.178167104721069\n","best_epoch and best_batch: 28-0\n","no improvement for: 3\n","EPOCH: 32\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.6534659307296612\n","train_time: 211.24248361587524\n","dev_f05: 0.4973378509196515\n","dev_time: 5.169922828674316\n","best_epoch and best_batch: 28-0\n","no improvement for: 4\n","EPOCH: 33\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.6621365776997273\n","train_time: 211.13108015060425\n","dev_f05: 0.5048500881834215\n","dev_time: 5.213441848754883\n","best_epoch and best_batch: 33-0\n","no improvement for: 0\n","EPOCH: 34\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.6646902752120739\n","train_time: 211.04546332359314\n","dev_f05: 0.5007394019060138\n","dev_time: 5.170363187789917\n","best_epoch and best_batch: 33-0\n","no improvement for: 1\n","EPOCH: 35\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.672330084643661\n","train_time: 208.10858631134033\n","dev_f05: 0.48707664884135476\n","dev_time: 5.084394693374634\n","best_epoch and best_batch: 33-0\n","no improvement for: 2\n","EPOCH: 36\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.6750607027778202\n","train_time: 204.5979437828064\n","dev_f05: 0.500249500998004\n","dev_time: 5.074869871139526\n","best_epoch and best_batch: 33-0\n","no improvement for: 3\n","EPOCH: 37\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.6779481862155401\n","train_time: 206.22594475746155\n","dev_f05: 0.5005854800936769\n","dev_time: 5.0965416431427\n","best_epoch and best_batch: 33-0\n","no improvement for: 4\n","EPOCH: 38\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.6867152804430068\n","train_time: 204.27439904212952\n","dev_f05: 0.5034670994393627\n","dev_time: 5.114405393600464\n","best_epoch and best_batch: 33-0\n","no improvement for: 5\n","EPOCH: 39\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.6877362919765747\n","train_time: 204.48947858810425\n","dev_f05: 0.4988789237668161\n","dev_time: 5.047496557235718\n","best_epoch and best_batch: 33-0\n","no improvement for: 6\n","EPOCH: 40\n","BATCH: 0\n","current_learningrate: 1.0\n","train_f05: 0.6900742410506456\n","train_time: 203.58985686302185\n","dev_f05: 0.497265625\n","dev_time: 5.084154367446899\n","best_epoch and best_batch: 33-0\n","no improvement for: 7\n","EPOCH: 41\n","BATCH: 0\n","current_learningrate: 0.9\n","train_f05: 0.6993426430682461\n","train_time: 205.76555347442627\n","dev_f05: 0.5030017152658662\n","dev_time: 5.09522557258606\n","best_epoch and best_batch: 33-0\n","no improvement for: 8\n","EPOCH: 42\n","BATCH: 0\n","current_learningrate: 0.81\n","train_f05: 0.704184586827665\n","train_time: 206.04855608940125\n","dev_f05: 0.5062838052815781\n","dev_time: 5.13655948638916\n","best_epoch and best_batch: 42-0\n","no improvement for: 0\n","EPOCH: 43\n","BATCH: 0\n","current_learningrate: 0.81\n","train_f05: 0.7079513537404296\n","train_time: 206.97704529762268\n","dev_f05: 0.4918733960650129\n","dev_time: 5.112880706787109\n","best_epoch and best_batch: 42-0\n","no improvement for: 1\n","EPOCH: 44\n","BATCH: 0\n","current_learningrate: 0.81\n","train_f05: 0.7086731016549132\n","train_time: 204.33538341522217\n","dev_f05: 0.49936688825577713\n","dev_time: 5.2397825717926025\n","best_epoch and best_batch: 42-0\n","no improvement for: 2\n","EPOCH: 45\n","BATCH: 0\n","current_learningrate: 0.81\n","train_f05: 0.7142555932997541\n","train_time: 206.47322797775269\n","dev_f05: 0.4991987179487179\n","dev_time: 5.15912652015686\n","best_epoch and best_batch: 42-0\n","no improvement for: 3\n","EPOCH: 46\n","BATCH: 0\n","current_learningrate: 0.81\n","train_f05: 0.7184558007329\n","train_time: 205.8293387889862\n","dev_f05: 0.4955032860601867\n","dev_time: 5.160956859588623\n","best_epoch and best_batch: 42-0\n","no improvement for: 4\n","EPOCH: 47\n","BATCH: 0\n","current_learningrate: 0.81\n","train_f05: 0.7188864502714567\n","train_time: 205.66781735420227\n","dev_f05: 0.50325682382134\n","dev_time: 5.169748783111572\n","best_epoch and best_batch: 42-0\n","no improvement for: 5\n","EPOCH: 48\n","BATCH: 0\n","current_learningrate: 0.81\n","train_f05: 0.7225795461346863\n","train_time: 206.86980366706848\n","dev_f05: 0.49868550772264214\n","dev_time: 5.1361072063446045\n","best_epoch and best_batch: 42-0\n","no improvement for: 6\n","EPOCH: 49\n","BATCH: 0\n","current_learningrate: 0.81\n","train_f05: 0.7234778797825184\n","train_time: 207.0573868751526\n","dev_f05: 0.5035321821036107\n","dev_time: 5.170780181884766\n","best_epoch and best_batch: 42-0\n","no improvement for: 7\n","EPOCH: 50\n","BATCH: 0\n","current_learningrate: 0.7290000000000001\n","train_f05: 0.7297979223967449\n","train_time: 206.69438934326172\n","dev_f05: 0.49781181619256015\n","dev_time: 5.152961015701294\n","best_epoch and best_batch: 42-0\n","no improvement for: 8\n","EPOCH: 51\n","BATCH: 0\n","current_learningrate: 0.6561000000000001\n","train_f05: 0.7327918607745072\n","train_time: 206.12516856193542\n","dev_f05: 0.4991736778846153\n","dev_time: 5.133108854293823\n","best_epoch and best_batch: 42-0\n","no improvement for: 9\n","EPOCH: 52\n","BATCH: 0\n","current_learningrate: 0.5904900000000002\n"],"name":"stdout"}]}]}