{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c2-ePca8wE2b"
   },
   "source": [
    "This notebook contains code from Weili Nie's 'RelGAN' repository (https://github.com/weilinie/RelGAN) and runs the RelGAN model. It was designed speficially to be run in the Colaboratory environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i-H8tjVrYrBl"
   },
   "source": [
    "## SET-UP## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "ln_RzmX6g2Dv",
    "outputId": "eac6f4cd-77b5-4be3-f75b-a28985edf986"
   },
   "outputs": [],
   "source": [
    "#I will attach my Drive so that I can easily load files.\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\", force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "w-gBlSROlzA4",
    "outputId": "dc775e68-4900-4ac5-93ef-6e9703df076c"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Mk1aCHlXnY7"
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import nltk\n",
    "\n",
    "\n",
    "# text tokens to code strings\n",
    "def text_to_code(tokens, dictionary, seq_len):\n",
    "    code_str = \"\"\n",
    "    eof_code = len(dictionary)  # used to filled in the blank to make up a sentence with seq_len\n",
    "    for sentence in tokens:\n",
    "        index = 0\n",
    "        for word in sentence:\n",
    "            code_str += (str(dictionary[word]) + ' ')\n",
    "            index += 1\n",
    "        while index < seq_len:\n",
    "            code_str += (str(eof_code) + ' ')\n",
    "            index += 1\n",
    "        code_str += '\\n'\n",
    "    return code_str\n",
    "\n",
    "\n",
    "# code tokens to text strings\n",
    "def code_to_text(codes, dictionary):\n",
    "    paras = \"\"\n",
    "    eof_code = len(dictionary)\n",
    "    for line in codes:\n",
    "        numbers = map(int, line)\n",
    "        for number in numbers:\n",
    "            if number == eof_code:\n",
    "                continue\n",
    "            paras += (dictionary[str(number)] + ' ')\n",
    "        paras += '\\n'\n",
    "    return paras\n",
    "\n",
    "\n",
    "# tokenlize the file\n",
    "def get_tokenlized(file):\n",
    "    tokenlized = list()\n",
    "    with open(file) as raw:\n",
    "        for text in raw:\n",
    "            text = nltk.word_tokenize(text.lower())\n",
    "            tokenlized.append(text)\n",
    "    return tokenlized\n",
    "\n",
    "\n",
    "# get word set\n",
    "def get_word_list(tokens):\n",
    "    word_set = list()\n",
    "    for sentence in tokens:\n",
    "        for word in sentence:\n",
    "            word_set.append(word)\n",
    "    return list(set(word_set))\n",
    "\n",
    "\n",
    "# get word_index_dict and index_word_dict\n",
    "def get_dict(word_set):\n",
    "    word_index_dict = dict()\n",
    "    index_word_dict = dict()\n",
    "    index = 0\n",
    "    for word in word_set:\n",
    "        word_index_dict[word] = str(index)\n",
    "        index_word_dict[str(index)] = word\n",
    "        index += 1\n",
    "    return word_index_dict, index_word_dict\n",
    "\n",
    "\n",
    "# get sequence length and dict size\n",
    "def text_precess(train_text_loc, test_text_loc=None):\n",
    "    train_tokens = get_tokenlized(train_text_loc)\n",
    "    if test_text_loc is None:\n",
    "        test_tokens = list()\n",
    "    else:\n",
    "        test_tokens = get_tokenlized(test_text_loc)\n",
    "    word_set = get_word_list(train_tokens + test_tokens)\n",
    "    [word_index_dict, index_word_dict] = get_dict(word_set)\n",
    "\n",
    "    if test_text_loc is None:\n",
    "        sequence_len = len(max(train_tokens, key=len))\n",
    "    else:\n",
    "        sequence_len = max(len(max(train_tokens, key=len)), len(max(test_tokens, key=len)))\n",
    "\n",
    "    # with open(oracle_file, 'w') as outfile:\n",
    "    #     outfile.write(text_to_code(tokens, word_index_dict, seq_len))\n",
    "\n",
    "    return sequence_len, len(word_index_dict) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5spA9xUlXYIL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "\n",
    "def generate_samples(sess, gen_x, batch_size, generated_num, output_file=None, get_code=True, iteration = 0):\n",
    "    # Generate Samples\n",
    "    sent_list = []\n",
    "    repeated_count = 0\n",
    "    generated_samples = []\n",
    "    for _ in range(int(generated_num / batch_size)):\n",
    "        generated_samples.extend(sess.run(gen_x))\n",
    "    codes = list()\n",
    "    if output_file is not None:\n",
    "        with open(output_file, 'w') as fout:\n",
    "          for sent in generated_samples:\n",
    "                buffer = ' '.join([str(x) for x in sent]) + '\\n'\n",
    "                if buffer not in sent_list:\n",
    "                  sent_list.append(buffer)\n",
    "                  codes += buffer\n",
    "                  fout.write(buffer)\n",
    "                  if get_code:\n",
    "                    codes.append(sent)\n",
    "                else:\n",
    "                  repeated_count += 1 \n",
    "        \n",
    "        print('Repeated samples: ', repeated_count)\n",
    "        if repeated_count < 40 and iteration > 1000:\n",
    "          sent_list = []\n",
    "          repeated_count = 0\n",
    "          normal_count = 0\n",
    "          generated_samples = []\n",
    "          for _ in range(int(100000 / batch_size)):\n",
    "              generated_samples.extend(sess.run(gen_x))\n",
    "          codes = list()\n",
    "          if output_file is not None:\n",
    "              with open(output_file, 'w') as fout:\n",
    "                  for sent in generated_samples:\n",
    "                      buffer = ' '.join([str(x) for x in sent]) + '\\n'\n",
    "                      if buffer not in sent_list:\n",
    "                        sent_list.append(buffer)\n",
    "                        codes += buffer\n",
    "                        fout.write(buffer)\n",
    "                        normal_count += 1 \n",
    "                        if get_code:\n",
    "                          codes.append(sent)\n",
    "                      else:\n",
    "                        repeated_count += 1 \n",
    "\n",
    "              print('Repeated samples: ', repeated_count)\n",
    "              print('Unique samples: ', normal_count)\n",
    "        return np.array(codes)\n",
    "    codes = \"\"\n",
    "    for sent in generated_samples:\n",
    "        buffer = ' '.join([str(x) for x in sent]) + '\\n'\n",
    "        codes += buffer\n",
    "    return codes\n",
    "\n",
    "def init_sess():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    return sess\n",
    "\n",
    "\n",
    "def pre_train_epoch(sess, g_pretrain_op, g_pretrain_loss, x_real, data_loader):\n",
    "    # Pre-train the generator using MLE for one epoch\n",
    "    supervised_g_losses = []\n",
    "    data_loader.reset_pointer()\n",
    "\n",
    "    for it in range(data_loader.num_batch):\n",
    "        batch = data_loader.next_batch()\n",
    "        _, g_loss = sess.run([g_pretrain_op, g_pretrain_loss], feed_dict={x_real: batch})\n",
    "        supervised_g_losses.append(g_loss)\n",
    "\n",
    "    return np.mean(supervised_g_losses)\n",
    "\n",
    "\n",
    "def plot_csv(csv_file, pre_epoch_num, metrics, method):\n",
    "    names = [str(i) for i in range(len(metrics) + 1)]\n",
    "    data = np.genfromtxt(csv_file, delimiter=',', skip_header=0, skip_footer=0, names=names)\n",
    "    for idx in range(len(metrics)):\n",
    "        metric_name = metrics[idx].get_name()\n",
    "        plt.figure()\n",
    "        plt.plot(data[names[0]], data[names[idx + 1]], color='r', label=method)\n",
    "        plt.axvline(x=pre_epoch_num, color='k', linestyle='--')\n",
    "        plt.xlabel('training epochs')\n",
    "        plt.ylabel(metric_name)\n",
    "        plt.legend()\n",
    "        plot_file = os.path.join(os.path.dirname(csv_file), '{}_{}.pdf'.format(method, metric_name))\n",
    "        print(plot_file)\n",
    "        plt.savefig(plot_file)\n",
    "\n",
    "\n",
    "def get_oracle_file(data_file, oracle_file, seq_len):\n",
    "    tokens = get_tokenlized(data_file)\n",
    "    word_set = get_word_list(tokens)\n",
    "    [word_index_dict, index_word_dict] = get_dict(word_set)\n",
    "    with open(oracle_file, 'w') as outfile:\n",
    "        outfile.write(text_to_code(tokens, word_index_dict, seq_len))\n",
    "\n",
    "    return index_word_dict\n",
    "\n",
    "\n",
    "def get_real_test_file(generator_file, gen_save_file, iw_dict):\n",
    "    codes = get_tokenlized(generator_file)\n",
    "    with open(gen_save_file, 'w') as outfile:\n",
    "        outfile.write(code_to_text(codes=codes, dictionary=iw_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0kPRKI3HbSYQ"
   },
   "source": [
    "## LSTM ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kv-wP2WYXzz8"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import tensor_array_ops, control_flow_ops\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class OracleLstm(object):\n",
    "    def __init__(self, num_vocabulary, batch_size, emb_dim, hidden_dim, sequence_length, start_token):\n",
    "        self.num_vocabulary = num_vocabulary\n",
    "        self.batch_size = batch_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.start_token = tf.constant([start_token] * self.batch_size, dtype=tf.int32)\n",
    "        self.g_params = []\n",
    "        self.temperature = 1.0\n",
    "\n",
    "        with tf.variable_scope('generator'):\n",
    "            tf.set_random_seed(1234)\n",
    "            self.g_embeddings = tf.Variable(\n",
    "                tf.random_normal([self.num_vocabulary, self.emb_dim], 0.0, 1.0, seed=123314154))\n",
    "            self.g_params.append(self.g_embeddings)\n",
    "            self.g_recurrent_unit = self.create_recurrent_unit(self.g_params)  # maps h_tm1 to h_t for generator\n",
    "            self.g_output_unit = self.create_output_unit(self.g_params)  # maps h_t to o_t (output token logits)\n",
    "\n",
    "        # placeholder definition\n",
    "        self.x = tf.placeholder(tf.int32, shape=[self.batch_size,\n",
    "                                                 self.sequence_length])  # sequence of tokens generated by generator\n",
    "\n",
    "        # processed for batch\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            tf.set_random_seed(1234)\n",
    "            self.processed_x = tf.transpose(tf.nn.embedding_lookup(self.g_embeddings, self.x),\n",
    "                                            perm=[1, 0, 2])  # seq_length x batch_size x emb_dim\n",
    "\n",
    "        # initial states\n",
    "        self.h0 = tf.zeros([self.batch_size, self.hidden_dim])\n",
    "        self.h0 = tf.stack([self.h0, self.h0])\n",
    "\n",
    "        # generator on initial randomness\n",
    "        gen_o = tensor_array_ops.TensorArray(dtype=tf.float32, size=self.sequence_length,\n",
    "                                             dynamic_size=False, infer_shape=True)\n",
    "        gen_x = tensor_array_ops.TensorArray(dtype=tf.int32, size=self.sequence_length,\n",
    "                                             dynamic_size=False, infer_shape=True)\n",
    "\n",
    "        def _g_recurrence(i, x_t, h_tm1, gen_o, gen_x):\n",
    "            h_t = self.g_recurrent_unit(x_t, h_tm1)  # hidden_memory_tuple\n",
    "            o_t = self.g_output_unit(h_t)  # batch x vocab , logits not prob\n",
    "            log_prob = tf.log(tf.nn.softmax(o_t))\n",
    "            next_token = tf.cast(tf.reshape(tf.multinomial(log_prob, 1), [self.batch_size]), tf.int32)\n",
    "            x_tp1 = tf.nn.embedding_lookup(self.g_embeddings, next_token)  # batch x emb_dim\n",
    "            gen_o = gen_o.write(i, tf.reduce_sum(\n",
    "                tf.multiply(tf.one_hot(next_token, self.num_vocabulary, 1.0, 0.0), tf.nn.softmax(o_t)),\n",
    "                1))  # [batch_size] , prob\n",
    "            gen_x = gen_x.write(i, next_token)  # indices, batch_size\n",
    "            return i + 1, x_tp1, h_t, gen_o, gen_x\n",
    "\n",
    "        _, _, _, self.gen_o, self.gen_x = control_flow_ops.while_loop(\n",
    "            cond=lambda i, _1, _2, _3, _4: i < self.sequence_length,\n",
    "            body=_g_recurrence,\n",
    "            loop_vars=(tf.constant(0, dtype=tf.int32),\n",
    "                       tf.nn.embedding_lookup(self.g_embeddings, self.start_token), self.h0, gen_o, gen_x)\n",
    "        )\n",
    "\n",
    "        self.gen_x = self.gen_x.stack()  # seq_length x batch_size\n",
    "        self.gen_x = tf.transpose(self.gen_x, perm=[1, 0])  # batch_size x seq_length\n",
    "\n",
    "        # supervised pretraining for generator\n",
    "        g_predictions = tensor_array_ops.TensorArray(\n",
    "            dtype=tf.float32, size=self.sequence_length,\n",
    "            dynamic_size=False, infer_shape=True)\n",
    "\n",
    "        ta_emb_x = tensor_array_ops.TensorArray(\n",
    "            dtype=tf.float32, size=self.sequence_length)\n",
    "        ta_emb_x = ta_emb_x.unstack(self.processed_x)\n",
    "\n",
    "        def _pretrain_recurrence(i, x_t, h_tm1, g_predictions):\n",
    "            h_t = self.g_recurrent_unit(x_t, h_tm1)\n",
    "            o_t = self.g_output_unit(h_t)\n",
    "            g_predictions = g_predictions.write(i, tf.nn.softmax(o_t))  # batch x vocab_size\n",
    "            x_tp1 = ta_emb_x.read(i)\n",
    "            return i + 1, x_tp1, h_t, g_predictions\n",
    "\n",
    "        _, _, _, self.g_predictions = control_flow_ops.while_loop(\n",
    "            cond=lambda i, _1, _2, _3: i < self.sequence_length,\n",
    "            body=_pretrain_recurrence,\n",
    "            loop_vars=(tf.constant(0, dtype=tf.int32),\n",
    "                       tf.nn.embedding_lookup(self.g_embeddings, self.start_token),\n",
    "                       self.h0, g_predictions))\n",
    "\n",
    "        self.g_predictions = tf.transpose(\n",
    "            self.g_predictions.stack(), perm=[1, 0, 2])  # batch_size x seq_length x vocab_size\n",
    "\n",
    "        # pretraining loss\n",
    "        self.pretrain_loss = -tf.reduce_sum(\n",
    "            tf.one_hot(tf.to_int32(tf.reshape(self.x, [-1])), self.num_vocabulary, 1.0, 0.0) * tf.log(\n",
    "                tf.reshape(self.g_predictions, [-1, self.num_vocabulary]))) / (self.sequence_length * self.batch_size)\n",
    "\n",
    "        self.out_loss = tf.reduce_sum(\n",
    "            tf.reshape(\n",
    "                -tf.reduce_sum(\n",
    "                    tf.one_hot(tf.to_int32(tf.reshape(self.x, [-1])), self.num_vocabulary, 1.0, 0.0) * tf.log(\n",
    "                        tf.reshape(self.g_predictions, [-1, self.num_vocabulary])), 1\n",
    "                ), [-1, self.sequence_length]\n",
    "            ), 1\n",
    "        )  # batch_size\n",
    "\n",
    "    def generate(self, session):\n",
    "        # h0 = np.random.normal(size=self.hidden_dim)\n",
    "        outputs = session.run(self.gen_x)\n",
    "        return outputs\n",
    "\n",
    "    def init_matrix(self, shape):\n",
    "        return tf.random_normal(shape, stddev=1.0, seed=10)\n",
    "\n",
    "    def create_recurrent_unit(self, params):\n",
    "        # Weights and Bias for input and hidden tensor\n",
    "        self.Wi = tf.Variable(tf.random_normal([self.emb_dim, self.hidden_dim], 0.0, 1.0, seed=111))\n",
    "        self.Ui = tf.Variable(tf.random_normal([self.hidden_dim, self.hidden_dim], 0.0, 1.0, seed=211))\n",
    "        self.bi = tf.Variable(tf.random_normal([self.hidden_dim, ], 0.0, 1.0, seed=311))\n",
    "\n",
    "        self.Wf = tf.Variable(tf.random_normal([self.emb_dim, self.hidden_dim], 0.0, 1.0, seed=114))\n",
    "        self.Uf = tf.Variable(tf.random_normal([self.hidden_dim, self.hidden_dim], 0.0, 1.0, seed=115))\n",
    "        self.bf = tf.Variable(tf.random_normal([self.hidden_dim, ], 0.0, 1.0, seed=116))\n",
    "\n",
    "        self.Wog = tf.Variable(tf.random_normal([self.emb_dim, self.hidden_dim], 0.0, 1.0, seed=997))\n",
    "        self.Uog = tf.Variable(tf.random_normal([self.hidden_dim, self.hidden_dim], 0.0, 1.0, seed=998))\n",
    "        self.bog = tf.Variable(tf.random_normal([self.hidden_dim, ], 0.0, 1.0, seed=999))\n",
    "\n",
    "        self.Wc = tf.Variable(tf.random_normal([self.emb_dim, self.hidden_dim], 0.0, 1.0, seed=110))\n",
    "        self.Uc = tf.Variable(tf.random_normal([self.hidden_dim, self.hidden_dim], 0.0, 1.0, seed=111))\n",
    "        self.bc = tf.Variable(tf.random_normal([self.hidden_dim, ], 0.0, 1.0, seed=112))\n",
    "        params.extend([\n",
    "            self.Wi, self.Ui, self.bi,\n",
    "            self.Wf, self.Uf, self.bf,\n",
    "            self.Wog, self.Uog, self.bog,\n",
    "            self.Wc, self.Uc, self.bc])\n",
    "\n",
    "        def unit(x, hidden_memory_tm1):\n",
    "            previous_hidden_state, c_prev = tf.unstack(hidden_memory_tm1)\n",
    "\n",
    "            # Input Gate\n",
    "            i = tf.sigmoid(\n",
    "                tf.matmul(x, self.Wi) +\n",
    "                tf.matmul(previous_hidden_state, self.Ui) + self.bi\n",
    "            )\n",
    "\n",
    "            # Forget Gate\n",
    "            f = tf.sigmoid(\n",
    "                tf.matmul(x, self.Wf) +\n",
    "                tf.matmul(previous_hidden_state, self.Uf) + self.bf\n",
    "            )\n",
    "\n",
    "            # Output Gate\n",
    "            o = tf.sigmoid(\n",
    "                tf.matmul(x, self.Wog) +\n",
    "                tf.matmul(previous_hidden_state, self.Uog) + self.bog\n",
    "            )\n",
    "\n",
    "            # New Memory Cell\n",
    "            c_ = tf.nn.tanh(\n",
    "                tf.matmul(x, self.Wc) +\n",
    "                tf.matmul(previous_hidden_state, self.Uc) + self.bc\n",
    "            )\n",
    "\n",
    "            # Final Memory cell\n",
    "            c = f * c_prev + i * c_\n",
    "\n",
    "            # Current Hidden state\n",
    "            current_hidden_state = o * tf.nn.tanh(c)\n",
    "\n",
    "            return tf.stack([current_hidden_state, c])\n",
    "\n",
    "        return unit\n",
    "\n",
    "    def create_output_unit(self, params):\n",
    "        self.Wo = tf.Variable(tf.random_normal([self.hidden_dim, self.num_vocabulary], 0.0, 1.0, seed=12341))\n",
    "        self.bo = tf.Variable(tf.random_normal([self.num_vocabulary], 0.0, 1.0, seed=56865246))\n",
    "        params.extend([self.Wo, self.bo])\n",
    "\n",
    "        def unit(hidden_memory_tuple):\n",
    "            hidden_state, c_prev = tf.unstack(hidden_memory_tuple)\n",
    "            logits = tf.matmul(hidden_state, self.Wo) + self.bo\n",
    "            return logits\n",
    "\n",
    "        return unit\n",
    "\n",
    "    # Compute the similarity between minibatch examples and all embeddings.\n",
    "    # We use the cosine distance:\n",
    "\n",
    "    def set_similarity(self, valid_examples=None, pca=True):\n",
    "        if valid_examples == None:\n",
    "            if pca:\n",
    "                valid_examples = np.array(range(20))\n",
    "            else:\n",
    "                valid_examples = np.array(range(self.num_vocabulary))\n",
    "        self.valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "        self.norm = tf.sqrt(tf.reduce_sum(tf.square(self.g_embeddings), 1, keep_dims=True))\n",
    "        self.normalized_embeddings = self.g_embeddings / self.norm\n",
    "        # PCA\n",
    "        if self.num_vocabulary >= 20 and pca == True:\n",
    "            emb = tf.matmul(self.normalized_embeddings, tf.transpose(self.normalized_embeddings))\n",
    "            s, u, v = tf.svd(emb)\n",
    "            u_r = tf.strided_slice(u, begin=[0, 0], end=[20, self.num_vocabulary], strides=[1, 1])\n",
    "            self.normalized_embeddings = tf.matmul(u_r, self.normalized_embeddings)\n",
    "        self.valid_embeddings = tf.nn.embedding_lookup(\n",
    "            self.normalized_embeddings, self.valid_dataset)\n",
    "        self.similarity = tf.matmul(self.valid_embeddings, tf.transpose(self.normalized_embeddings))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "51eu3udtYyDG"
   },
   "source": [
    "## METRICS ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wXhpIbDiYamV"
   },
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "\n",
    "class Metrics:\n",
    "    def __init__(self):\n",
    "        self.name = 'Metric'\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "\n",
    "    def set_name(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_score(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bj5sBHwVYYMG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Nll(Metrics):\n",
    "    def __init__(self, data_loader, pretrain_loss, x_real, sess, name='Nll'):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.data_loader = data_loader\n",
    "        self.sess = sess\n",
    "        self.pretrain_loss = pretrain_loss\n",
    "        self.x_real = x_real\n",
    "\n",
    "    def set_name(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "\n",
    "    def get_score(self):\n",
    "        return self.nll_loss()\n",
    "\n",
    "    def nll_loss(self):\n",
    "        nll = []\n",
    "        self.data_loader.reset_pointer()\n",
    "        for it in range(self.data_loader.num_batch):\n",
    "            batch = self.data_loader.next_batch()\n",
    "            g_loss = self.sess.run(self.pretrain_loss, {self.x_real: batch})\n",
    "            nll.append(g_loss)\n",
    "        return np.mean(nll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9duy2ZhWZPYB"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "class Bleu(Metrics):\n",
    "    def __init__(self, test_text='', real_text='', gram=3, name='Bleu', portion=1):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.test_data = test_text\n",
    "        self.real_data = real_text\n",
    "        self.gram = gram\n",
    "        self.sample_size = 200  # BLEU scores remain nearly unchanged for self.sample_size >= 200\n",
    "        self.reference = None\n",
    "        self.is_first = True\n",
    "        self.portion = portion  # how many portions to use in the evaluation, default to use the whole test dataset\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "\n",
    "    def get_score(self, is_fast=False, ignore=False):\n",
    "        if ignore:\n",
    "            return 0\n",
    "        if self.is_first:\n",
    "            self.get_reference()\n",
    "            self.is_first = False\n",
    "        return self.get_bleu()\n",
    "\n",
    "    def get_reference(self):\n",
    "        if self.reference is None:\n",
    "            reference = list()\n",
    "            with open(self.real_data) as real_data:\n",
    "                for text in real_data:\n",
    "                    text = nltk.word_tokenize(text)\n",
    "                    reference.append(text)\n",
    "\n",
    "            # randomly choose a portion of test data\n",
    "            # In-place shuffle\n",
    "            random.shuffle(reference)\n",
    "            len_ref = len(reference)\n",
    "            reference = reference[:int(self.portion*len_ref)]\n",
    "\n",
    "            self.reference = reference\n",
    "\n",
    "            return reference\n",
    "        else:\n",
    "            return self.reference\n",
    "\n",
    "    def get_bleu(self):\n",
    "        ngram = self.gram\n",
    "        bleu = list()\n",
    "        reference = self.get_reference()\n",
    "        weight = tuple((1. / ngram for _ in range(ngram)))\n",
    "        with open(self.test_data) as test_data:\n",
    "            i = 0\n",
    "            for hypothesis in test_data:\n",
    "                if i >= self.sample_size:\n",
    "                    break\n",
    "                hypothesis = nltk.word_tokenize(hypothesis)\n",
    "                bleu.append(self.calc_bleu(reference, hypothesis, weight))\n",
    "                i += 1\n",
    "        return sum(bleu) / len(bleu)\n",
    "\n",
    "    def calc_bleu(self, reference, hypothesis, weight):\n",
    "        return nltk.translate.bleu_score.sentence_bleu(reference, hypothesis, weight,\n",
    "                                                       smoothing_function=SmoothingFunction().method1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Po3ILk_MZW-e"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "class SelfBleu(Metrics):\n",
    "    def __init__(self, test_text='', gram=3, name='SelfBleu', portion=1):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.test_data = test_text\n",
    "        self.gram = gram\n",
    "        self.sample_size = 200  # SelfBLEU scores remain nearly unchanged for self.sample_size >= 200\n",
    "        self.portion = portion  # how many posrtions to use in the evaluation, default to use the whole test dataset\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "\n",
    "    def get_score(self, is_fast=False, ignore=False):\n",
    "        if ignore:\n",
    "            return 0\n",
    "\n",
    "        return self.get_bleu()\n",
    "\n",
    "    def get_reference(self):\n",
    "        reference = list()\n",
    "        with open(self.test_data) as real_data:\n",
    "            for text in real_data:\n",
    "                text = nltk.word_tokenize(text)\n",
    "                reference.append(text)\n",
    "        len_ref = len(reference)\n",
    "\n",
    "        return reference[:int(self.portion*len_ref)]\n",
    "\n",
    "    def get_bleu(self):\n",
    "        ngram = self.gram\n",
    "        bleu = list()\n",
    "        reference = self.get_reference()\n",
    "        weight = tuple((1. / ngram for _ in range(ngram)))\n",
    "        with open(self.test_data) as test_data:\n",
    "            i = 0\n",
    "            for hypothesis in test_data:\n",
    "                if i >= self.sample_size:\n",
    "                    break\n",
    "                hypothesis = nltk.word_tokenize(hypothesis)\n",
    "                bleu.append(self.calc_bleu(reference, hypothesis, weight))\n",
    "                i += 1\n",
    "\n",
    "        return sum(bleu) / len(bleu)\n",
    "\n",
    "    def calc_bleu(self, reference, hypothesis, weight):\n",
    "        return nltk.translate.bleu_score.sentence_bleu(reference, hypothesis, weight,\n",
    "                                                       smoothing_function=SmoothingFunction().method1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hHmWXPORYlC7"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "class DocEmbSim(Metrics):\n",
    "    def __init__(self, oracle_file=None, generator_file=None, num_vocabulary=None, name='DocEmbSim'):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.oracle_sim = None\n",
    "        self.gen_sim = None\n",
    "        self.is_first = True\n",
    "        self.oracle_file = oracle_file\n",
    "        self.generator_file = generator_file\n",
    "        self.num_vocabulary = num_vocabulary\n",
    "        self.batch_size = 64\n",
    "        self.embedding_size = 32\n",
    "        self.data_index = 0\n",
    "        self.valid_examples = None\n",
    "\n",
    "    def get_score(self):\n",
    "        if self.is_first:\n",
    "            self.get_oracle_sim()\n",
    "            self.is_first = False\n",
    "        self.get_gen_sim()\n",
    "        return self.get_dis_corr()\n",
    "\n",
    "    def get_frequent_word(self):\n",
    "        if self.valid_examples is not None:\n",
    "            return self.valid_examples\n",
    "\n",
    "        import collections\n",
    "        words = []\n",
    "        with open(self.oracle_file, 'r') as file:\n",
    "            for line in file:\n",
    "                text = nltk.word_tokenize(line)\n",
    "                text = list(map(int, text))\n",
    "                words += text\n",
    "        counts = collections.Counter(words)\n",
    "        new_list = sorted(words, key=lambda x: -counts[x])\n",
    "        word_set = list(set(new_list))\n",
    "        if len(word_set) < self.num_vocabulary // 10:\n",
    "            self.valid_examples = word_set\n",
    "            return word_set\n",
    "        else:\n",
    "            self.valid_examples = word_set[0: self.num_vocabulary//10]  # choose 1/10 words with the highest frequency\n",
    "            return word_set[0: self.num_vocabulary//10]\n",
    "\n",
    "    def read_data(self, file):\n",
    "        words = []\n",
    "        with open(file, 'r') as file:\n",
    "            for line in file:\n",
    "                text = nltk.word_tokenize(line)\n",
    "                words.append(text)\n",
    "        return words\n",
    "\n",
    "    def generate_batch(self, batch_size, num_skips, skip_window, data=None):\n",
    "        assert batch_size % num_skips == 0\n",
    "        assert num_skips <= 2 * skip_window\n",
    "        batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "        labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "        span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "        buffer = collections.deque(maxlen=span)  # deque to slide the window\n",
    "        for _ in range(span):\n",
    "            buffer.append(data[self.data_index])\n",
    "            self.data_index = (self.data_index + 1) % len(data)\n",
    "        for i in range(batch_size // num_skips):\n",
    "            target = skip_window  # target label at the center of the buffer\n",
    "            targets_to_avoid = [skip_window]\n",
    "            for j in range(num_skips):\n",
    "                while target in targets_to_avoid:\n",
    "                    target = random.randint(0, span - 1)\n",
    "                targets_to_avoid.append(target)\n",
    "                batch[i * num_skips + j] = buffer[skip_window]\n",
    "                labels[i * num_skips + j, 0] = buffer[target]\n",
    "            buffer.append(data[self.data_index])\n",
    "            self.data_index = (self.data_index + 1) % len(data)\n",
    "        return batch, labels\n",
    "\n",
    "    def get_wordvec(self, file):\n",
    "        graph = tf.Graph()\n",
    "        batch_size = self.batch_size\n",
    "        embedding_size = self.embedding_size\n",
    "        vocabulary_size = self.num_vocabulary\n",
    "        num_sampled = 64\n",
    "        if num_sampled > vocabulary_size:\n",
    "            num_sampled = vocabulary_size\n",
    "        num_steps = 2\n",
    "        skip_window = 1  # How many words to consider left and right.\n",
    "        num_skips = 2  # How many times to reuse an input to generate a label.\n",
    "        if self.valid_examples is None:\n",
    "            self.get_frequent_word()\n",
    "\n",
    "        with graph.as_default():\n",
    "\n",
    "            # Input data.\n",
    "            train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "            train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "            valid_dataset = tf.constant(self.valid_examples, dtype=tf.int32)\n",
    "\n",
    "            # initial Variables.\n",
    "            embeddings = tf.Variable(\n",
    "                tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0, seed=11))\n",
    "            softmax_weights = tf.Variable(\n",
    "                tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                    stddev=1.0 / math.sqrt(embedding_size), seed=12))\n",
    "            softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "            # Model.\n",
    "            # Look up embeddings for inputs.\n",
    "            embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "            # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
    "                                           labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
    "\n",
    "            optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "\n",
    "            # Compute the similarity between minibatch examples and all embeddings.\n",
    "            norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "            normalized_embeddings = embeddings / norm\n",
    "            valid_embeddings = tf.nn.embedding_lookup(\n",
    "                normalized_embeddings, valid_dataset)\n",
    "            similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))\n",
    "\n",
    "            data = self.read_data(file)\n",
    "\n",
    "        with tf.Session(graph=graph) as session:\n",
    "            tf.global_variables_initializer().run()\n",
    "            average_loss = 0\n",
    "            generate_num = len(data)\n",
    "            for step in range(num_steps):\n",
    "                for index in range(generate_num):\n",
    "                    cur_batch_data, cur_batch_labels = self.generate_batch(\n",
    "                        batch_size, num_skips, skip_window, data[index])\n",
    "                    feed_dict = {train_dataset: cur_batch_data, train_labels: cur_batch_labels}\n",
    "                    _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "                    average_loss += l\n",
    "            similarity_value = similarity.eval()\n",
    "            return similarity_value\n",
    "\n",
    "    def get_oracle_sim(self):\n",
    "        self.oracle_sim = self.get_wordvec(self.oracle_file)  # evaluate word embedding on the models file\n",
    "\n",
    "    def get_gen_sim(self):\n",
    "        self.gen_sim = self.get_wordvec(self.generator_file)  # evaluate word embedding on the generator file\n",
    "\n",
    "    def get_dis_corr(self):\n",
    "        if len(self.oracle_sim) != len(self.gen_sim):\n",
    "            raise ArithmeticError\n",
    "        corr = 0\n",
    "        for index in range(len(self.oracle_sim)):\n",
    "            corr += (1 - cosine(np.array(self.oracle_sim[index]), np.array(self.gen_sim[index])))\n",
    "        return np.log10(corr / len(self.oracle_sim))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0QAe458Y77h"
   },
   "source": [
    "## HELPER FUNCTIONS ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bu--vt45Yu2k"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def hw_flatten(x):\n",
    "    return tf.reshape(x, shape=[-1, x.shape[1] * x.shape[2], x.shape[-1]])\n",
    "\n",
    "\n",
    "def l2_norm(v, eps=1e-12):\n",
    "    return v / (tf.reduce_sum(v ** 2) ** 0.5 + eps)\n",
    "\n",
    "\n",
    "def lrelu(x, alpha=0.2):\n",
    "    return tf.nn.leaky_relu(x, alpha)\n",
    "\n",
    "\n",
    "def create_linear_initializer(input_size, dtype=tf.float32):\n",
    "    \"\"\"Returns a default initializer for weights of a linear module.\"\"\"\n",
    "    stddev = 1 / math.sqrt(input_size * 1.0)\n",
    "    return tf.truncated_normal_initializer(stddev=stddev, dtype=dtype)\n",
    "\n",
    "\n",
    "def create_bias_initializer(dtype=tf.float32):\n",
    "    \"\"\"Returns a default initializer for the biases of a linear/AddBias module.\"\"\"\n",
    "    return tf.zeros_initializer(dtype=dtype)\n",
    "\n",
    "\n",
    "def linear(input_, output_size, use_bias=False, sn=False, scope=None):\n",
    "    '''\n",
    "    Linear map: output[k] = sum_i(Matrix[k, i] * input_[i] ) + Bias[k]\n",
    "    Args:\n",
    "    input_: a tensor or a list of 2D, batch x n, Tensors.\n",
    "    output_size: int, second dimension of W[i].\n",
    "    scope: Variable Scope for the created subgraph; defaults to \"Linear\".\n",
    "  Returns:\n",
    "    A 2D Tensor with shape [batch x output_size] equal to\n",
    "    sum_i(input_[i] * W[i]), where W[i]s are newly created matrices.\n",
    "  Raises:\n",
    "    ValueError: if some of the arguments has unspecified or wrong shape.\n",
    "  '''\n",
    "\n",
    "    shape = input_.get_shape().as_list()\n",
    "    if len(shape) != 2:\n",
    "        raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shape))\n",
    "    if not shape[1]:\n",
    "        raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shape))\n",
    "    input_size = shape[1]\n",
    "\n",
    "    # Now the computation.\n",
    "    with tf.variable_scope(scope or \"Linear\"):\n",
    "        W = tf.get_variable(\"Matrix\", shape=[output_size, input_size],\n",
    "                            initializer=create_linear_initializer(input_size, input_.dtype),\n",
    "                            dtype=input_.dtype)\n",
    "        if sn:\n",
    "            W = spectral_norm(W)\n",
    "        output_ = tf.matmul(input_, tf.transpose(W))\n",
    "        if use_bias:\n",
    "            bias_term = tf.get_variable(\"Bias\", [output_size],\n",
    "                                        initializer=create_bias_initializer(input_.dtype),\n",
    "                                        dtype=input_.dtype)\n",
    "            output_ += bias_term\n",
    "\n",
    "    return output_\n",
    "\n",
    "\n",
    "def highway(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu, scope='Highway'):\n",
    "    \"\"\"Highway Network (cf. http://arxiv.org/abs/1505.00387).\n",
    "    t = sigmoid(Wy + b)\n",
    "    z = t * g(Wy + b) + (1 - t) * y\n",
    "    where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        for idx in range(num_layers):\n",
    "            g = f(linear(input_, size, scope='highway_lin_%d' % idx))\n",
    "\n",
    "            t = tf.sigmoid(linear(input_, size, scope='highway_gate_%d' % idx) + bias)\n",
    "\n",
    "            output = t * g + (1. - t) * input_\n",
    "            input_ = output\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def mlp(input_, output_sizes, act_func=tf.nn.relu, use_bias=True):\n",
    "    '''\n",
    "    Constructs a MLP module\n",
    "    :param input_:\n",
    "    :param output_sizes: An iterable of output dimensionalities\n",
    "    :param act_func: activation function\n",
    "    :param use_bias: whether use bias term for linear mapping\n",
    "    :return: the output of the MLP module\n",
    "    '''\n",
    "    net = input_\n",
    "    num_layers = len(output_sizes)\n",
    "    for layer_id in range(num_layers):\n",
    "        net = linear(net, output_sizes[layer_id], use_bias=use_bias, scope='linear_{}'.format(layer_id))\n",
    "        if layer_id != num_layers - 1:\n",
    "            net = act_func(net)\n",
    "    return net\n",
    "\n",
    "\n",
    "def conv2d(input_, out_nums, k_h=2, k_w=1, d_h=2, d_w=1, stddev=None, sn=False, padding='SAME', scope=None):\n",
    "    in_nums = input_.get_shape().as_list()[-1]\n",
    "    # Glorot initialization\n",
    "    if stddev is None:\n",
    "        stddev = math.sqrt(2. / (k_h * k_w * in_nums))\n",
    "    with tf.variable_scope(scope or \"Conv2d\"):\n",
    "        W = tf.get_variable(\"Matrix\", shape=[k_h, k_w, in_nums, out_nums],\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
    "        if sn:\n",
    "            W = spectral_norm(W)\n",
    "        b = tf.get_variable(\"Bias\", shape=[out_nums], initializer=tf.zeros_initializer)\n",
    "        conv = tf.nn.conv2d(input_, filter=W, strides=[1, d_h, d_w, 1], padding=padding)\n",
    "        conv = tf.nn.bias_add(conv, b)\n",
    "\n",
    "    return conv\n",
    "\n",
    "\n",
    "def self_attention(x, ch, sn=False):\n",
    "    \"\"\"self-attention for GAN\"\"\"\n",
    "    f = conv2d(x, ch // 8, k_h=1, d_h=1, sn=sn, scope='f_conv')  # [bs, h, w, c']\n",
    "    g = conv2d(x, ch // 8, k_h=1, d_h=1, sn=sn, scope='g_conv')  # [bs, h, w, c']\n",
    "    h = conv2d(x, ch, k_h=1, d_h=1, sn=sn, scope='h_conv')  # [bs, h, w, c]\n",
    "\n",
    "    # N = h * w\n",
    "    s = tf.matmul(hw_flatten(g), hw_flatten(f), transpose_b=True)  # # [bs, N, N]\n",
    "\n",
    "    beta = tf.nn.softmax(s, dim=-1)  # attention map\n",
    "\n",
    "    o = tf.matmul(beta, hw_flatten(h))  # [bs, N, C]\n",
    "    gamma = tf.get_variable(\"gamma\", [1], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    o = tf.reshape(o, [-1] + x.get_shape().as_list()[1:])  # [bs, h, w, C]\n",
    "    x = gamma * o + x\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def spectral_norm(w, iteration=1):\n",
    "    \"\"\"spectral normalization for GANs\"\"\"\n",
    "    w_shape = w.shape.as_list()\n",
    "    w = tf.reshape(w, [-1, w_shape[-1]])\n",
    "\n",
    "    u = tf.get_variable(\"u\", [1, w_shape[-1]], initializer=tf.truncated_normal_initializer(), trainable=False)\n",
    "\n",
    "    u_hat = u\n",
    "    v_hat = None\n",
    "    for i in range(iteration):\n",
    "        \"\"\"\n",
    "        power iteration\n",
    "        Usually iteration = 1 will be enough\n",
    "        \"\"\"\n",
    "        v_ = tf.matmul(u_hat, tf.transpose(w))\n",
    "        v_hat = l2_norm(v_)\n",
    "\n",
    "        u_ = tf.matmul(v_hat, w)\n",
    "        u_hat = l2_norm(u_)\n",
    "\n",
    "    sigma = tf.matmul(tf.matmul(v_hat, w), tf.transpose(u_hat))\n",
    "    w_norm = w / sigma\n",
    "\n",
    "    with tf.control_dependencies([u.assign(u_hat)]):\n",
    "        w_norm = tf.reshape(w_norm, w_shape)\n",
    "\n",
    "    return w_norm\n",
    "\n",
    "\n",
    "def create_output_unit(output_size, vocab_size):\n",
    "    # output_size = self.gen_mem.output_size.as_list()[0]\n",
    "    Wo = tf.get_variable('Wo', shape=[output_size, vocab_size], initializer=create_linear_initializer(output_size))\n",
    "    bo = tf.get_variable('bo', shape=[vocab_size], initializer=create_bias_initializer())\n",
    "\n",
    "    def unit(hidden_mem_o):\n",
    "        logits = tf.matmul(hidden_mem_o, Wo) + bo\n",
    "        return logits\n",
    "\n",
    "    return unit\n",
    "\n",
    "\n",
    "def add_gumbel(o_t, eps=1e-10):\n",
    "    \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
    "    u = tf.random_uniform(tf.shape(o_t), minval=0, maxval=1, dtype=tf.float32)\n",
    "    g_t = -tf.log(-tf.log(u + eps) + eps)\n",
    "    gumbel_t = tf.add(o_t, g_t)\n",
    "    return gumbel_t\n",
    "\n",
    "\n",
    "def add_gumbel_cond(o_t, next_token_onehot, eps=1e-10):\n",
    "    \"\"\"draw reparameterization z of categorical variable b from p(z|b).\"\"\"\n",
    "\n",
    "    def truncated_gumbel(gumbel, truncation):\n",
    "        return -tf.log(eps + tf.exp(-gumbel) + tf.exp(-truncation))\n",
    "\n",
    "    v = tf.random_uniform(tf.shape(o_t), minval=0, maxval=1, dtype=tf.float32)\n",
    "\n",
    "    print(\"shape of v: {}\".format(v.get_shape().as_list()))\n",
    "    print(\"shape of next_token_onehot: {}\".format(next_token_onehot.get_shape().as_list()))\n",
    "\n",
    "    gumbel = -tf.log(-tf.log(v + eps) + eps, name=\"gumbel\")\n",
    "    topgumbels = gumbel + tf.reduce_logsumexp(o_t, axis=-1, keep_dims=True)\n",
    "    topgumbel = tf.reduce_sum(next_token_onehot * topgumbels, axis=-1, keep_dims=True)\n",
    "\n",
    "    truncgumbel = truncated_gumbel(gumbel + o_t, topgumbel)\n",
    "    return (1. - next_token_onehot) * truncgumbel + next_token_onehot * topgumbels\n",
    "\n",
    "\n",
    "def gradient_penalty(discriminator, x_real_onehot, x_fake_onehot_appr, config):\n",
    "    \"\"\"compute the gradiet penalty for the WGAN-GP loss\"\"\"\n",
    "    alpha = tf.random_uniform(shape=[config['batch_size'], 1, 1], minval=0., maxval=1.)\n",
    "    interpolated = alpha * x_real_onehot + (1. - alpha) * x_fake_onehot_appr\n",
    "\n",
    "    logit = discriminator(x_onehot=interpolated)\n",
    "\n",
    "    grad = tf.gradients(logit, interpolated)[0]  # gradient of D(interpolated)\n",
    "    grad_norm = tf.norm(tf.layers.flatten(grad), axis=1)  # l2 norm\n",
    "\n",
    "    GP = config['reg_param'] * tf.reduce_mean(tf.square(grad_norm - 1.))\n",
    "\n",
    "    return GP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kRyJuoc3Y-uz"
   },
   "source": [
    "## TRAINING FUNCTIONS ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nzXEL8UyZehU"
   },
   "source": [
    "### ORACLE ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wlEKo7ccYO24"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "EPS = 1e-10\n",
    "\n",
    "\n",
    "def oracle_train(generator, discriminator, oracle_model, oracle_loader, gen_loader, config):\n",
    "    batch_size = config['batch_size']\n",
    "    vocab_size = config['vocab_size']\n",
    "    seq_len = config['seq_len']\n",
    "    num_sentences = config['num_sentences']\n",
    "    data_dir = config['data_dir']\n",
    "    log_dir = config['log_dir']\n",
    "    sample_dir = config['sample_dir']\n",
    "    npre_epochs = config['npre_epochs']\n",
    "    nadv_steps = config['nadv_steps']\n",
    "    seed = config['seed']\n",
    "    temper = config['temperature']\n",
    "    adapt = config['adapt']\n",
    "\n",
    "    # set random seed\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # filename\n",
    "    oracle_file = os.path.join(sample_dir, 'oracle.txt')\n",
    "    gen_file = os.path.join(sample_dir, 'generator.txt')\n",
    "    csv_file = os.path.join(log_dir, 'experiment-log-relgan.csv')\n",
    "\n",
    "    # create necessary directories\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    if not os.path.exists(sample_dir):\n",
    "        os.makedirs(sample_dir)\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    # placeholder definitions\n",
    "    x_real = tf.placeholder(tf.int32, [batch_size, seq_len], name=\"x_real\")  # tokens of oracle sequences\n",
    "\n",
    "    temperature = tf.Variable(1., trainable=False, name='temperature')\n",
    "\n",
    "    x_real_onehot = tf.one_hot(x_real, vocab_size)  # batch_size x seq_len x vocab_size\n",
    "    assert x_real_onehot.get_shape().as_list() == [batch_size, seq_len, vocab_size]\n",
    "\n",
    "    x_fake_onehot_appr, x_fake, g_pretrain_loss, gen_o = generator(x_real=x_real, temperature=temperature)\n",
    "\n",
    "    d_out_real = discriminator(x_onehot=x_real_onehot)\n",
    "    d_out_fake = discriminator(x_onehot=x_fake_onehot_appr)\n",
    "\n",
    "    # GAN / Divergence type\n",
    "    log_pg, g_loss, d_loss = get_losses(d_out_real, d_out_fake, x_real_onehot, x_fake_onehot_appr,\n",
    "                                                    gen_o, discriminator, config)\n",
    "\n",
    "    # Global step\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    global_step_op = global_step.assign_add(1)\n",
    "\n",
    "    # Train ops\n",
    "    g_pretrain_op, g_train_op, d_train_op, temp_train_op = get_train_ops(config, g_pretrain_loss, g_loss, d_loss,\n",
    "                                                                         log_pg, temperature, global_step)\n",
    "\n",
    "    # Record wall clock time\n",
    "    time_diff = tf.placeholder(tf.float32)\n",
    "    Wall_clock_time = tf.Variable(0., trainable=False)\n",
    "    update_Wall_op = Wall_clock_time.assign_add(time_diff)\n",
    "\n",
    "    # Temperature placeholder\n",
    "    temp_var = tf.placeholder(tf.float32)\n",
    "    update_temperature_op = temperature.assign(temp_var)\n",
    "\n",
    "    # Loss summaries\n",
    "    loss_summaries = [\n",
    "        tf.summary.scalar('loss/discriminator', d_loss),\n",
    "        tf.summary.scalar('loss/g_loss', g_loss),\n",
    "        tf.summary.scalar('loss/log_pg', log_pg),\n",
    "        tf.summary.scalar('loss/Wall_clock_time', Wall_clock_time),\n",
    "        tf.summary.scalar('loss/temperature', temperature),\n",
    "    ]\n",
    "    loss_summary_op = tf.summary.merge(loss_summaries)\n",
    "\n",
    "    # Metric Summaries\n",
    "    metrics_pl, metric_summary_op = get_metric_summary_op(config)\n",
    "\n",
    "    # ------------- initial the graph --------------\n",
    "    with init_sess() as sess:\n",
    "        log = open(csv_file, 'w')\n",
    "        sum_writer = tf.summary.FileWriter(os.path.join(log_dir, 'summary'), sess.graph)\n",
    "\n",
    "        # generate oracle data and create batches\n",
    "        generate_samples(sess, oracle_model.gen_x, batch_size, num_sentences, oracle_file)\n",
    "        oracle_loader.create_batches(oracle_file)\n",
    "\n",
    "        metrics = get_metrics(config, oracle_loader, gen_loader, oracle_file, gen_file,\n",
    "                              oracle_model, g_pretrain_loss, x_real, sess)\n",
    "\n",
    "        print('Start pre-training...')\n",
    "        for epoch in range(npre_epochs):\n",
    "            # run pre-training\n",
    "            g_pretrain_loss_np = pre_train_epoch(sess, g_pretrain_op, g_pretrain_loss, x_real, oracle_loader)\n",
    "\n",
    "            # Test\n",
    "            ntest_pre = 10\n",
    "            if np.mod(epoch, ntest_pre) == 0:\n",
    "                # generate fake data and create batches\n",
    "                gen_save_file = os.path.join(sample_dir, 'pre_samples_{:05d}.txt'.format(epoch))\n",
    "                generate_samples(sess, x_fake, batch_size, num_sentences, gen_file)\n",
    "                generate_samples(sess, x_fake, batch_size, 120, gen_save_file)\n",
    "                gen_loader.create_batches(gen_file)\n",
    "\n",
    "                scores = [metric.get_score() for metric in metrics]\n",
    "                metrics_summary_str = sess.run(metric_summary_op, feed_dict=dict(zip(metrics_pl, scores)))\n",
    "                sum_writer.add_summary(metrics_summary_str, epoch)\n",
    "\n",
    "                msg = 'pre_gen_epoch:' + str(epoch) + ', g_pre_loss: %.4f' % g_pretrain_loss_np\n",
    "                metric_names = [metric.get_name() for metric in metrics]\n",
    "                for (name, score) in zip(metric_names, scores):\n",
    "                    msg += ', ' + name + ': %.4f' % score\n",
    "                print(msg)\n",
    "                log.write(msg)\n",
    "                log.write('\\n')\n",
    "\n",
    "        print('Start adversarial training...')\n",
    "        progress = tqdm(range(nadv_steps))\n",
    "        for _ in progress:\n",
    "            niter = sess.run(global_step)\n",
    "\n",
    "            t0 = time.time()\n",
    "\n",
    "            # run adversarial training\n",
    "            for _ in range(config['gsteps']):\n",
    "                sess.run(g_train_op, feed_dict={x_real: oracle_loader.random_batch()})\n",
    "            for _ in range(config['dsteps']):\n",
    "                sess.run(d_train_op, feed_dict={x_real: oracle_loader.random_batch()})\n",
    "\n",
    "            t1 = time.time()\n",
    "            sess.run(update_Wall_op, feed_dict={time_diff: t1 - t0})\n",
    "\n",
    "            # temperature\n",
    "            temp_var_np = get_fixed_temperature(temper, niter, nadv_steps, adapt)\n",
    "            sess.run(update_temperature_op, feed_dict={temp_var: temp_var_np})\n",
    "\n",
    "            feed = {x_real: oracle_loader.random_batch()}\n",
    "            g_loss_np, d_loss_np, loss_summary_str = sess.run([g_loss, d_loss, loss_summary_op], feed_dict=feed)\n",
    "            sum_writer.add_summary(loss_summary_str, niter)\n",
    "\n",
    "            sess.run(global_step_op)\n",
    "\n",
    "            progress.set_description('g_loss: %4.4f, d_loss: %4.4f' % (g_loss_np, d_loss_np))\n",
    "\n",
    "            # Test\n",
    "            if np.mod(niter, config['ntest']) == 0:\n",
    "                # generate fake data and create batches\n",
    "                gen_save_file = os.path.join(sample_dir, 'adv_samples_{:05d}.txt'.format(niter))\n",
    "                generate_samples(sess, x_fake, batch_size, num_sentences, gen_file)\n",
    "                generate_samples(sess, x_fake, batch_size, 120, gen_save_file)\n",
    "                gen_loader.create_batches(gen_file)\n",
    "\n",
    "                # write summaries\n",
    "                scores = [metric.get_score() for metric in metrics]\n",
    "                metrics_summary_str = sess.run(metric_summary_op, feed_dict=dict(zip(metrics_pl, scores)))\n",
    "                sum_writer.add_summary(metrics_summary_str, niter + config['npre_epochs'])\n",
    "\n",
    "                msg = 'adv_step: ' + str(niter)\n",
    "                metric_names = [metric.get_name() for metric in metrics]\n",
    "                for (name, score) in zip(metric_names, scores):\n",
    "                    msg += ', ' + name + ': %.4f' % score\n",
    "                print(msg)\n",
    "                log.write(msg)\n",
    "                log.write('\\n')\n",
    "\n",
    "\n",
    "# A function to get different GAN losses\n",
    "def get_losses(d_out_real, d_out_fake, x_real_onehot, x_fake_onehot_appr, gen_o, discriminator, config):\n",
    "    batch_size = config['batch_size']\n",
    "    gan_type = config['gan_type']\n",
    "\n",
    "    if gan_type == 'standard':  # the non-satuating GAN loss\n",
    "        d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=d_out_real, labels=tf.ones_like(d_out_real)\n",
    "        ))\n",
    "        d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=d_out_fake, labels=tf.zeros_like(d_out_fake)\n",
    "        ))\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=d_out_fake, labels=tf.ones_like(d_out_fake)\n",
    "        ))\n",
    "\n",
    "    elif gan_type == 'JS':  # the vanilla GAN loss\n",
    "        d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=d_out_real, labels=tf.ones_like(d_out_real)\n",
    "        ))\n",
    "        d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=d_out_fake, labels=tf.zeros_like(d_out_fake)\n",
    "        ))\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        g_loss = -d_loss_fake\n",
    "\n",
    "    elif gan_type == 'KL':  # the GAN loss implicitly minimizing KL-divergence\n",
    "        d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=d_out_real, labels=tf.ones_like(d_out_real)\n",
    "        ))\n",
    "        d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=d_out_fake, labels=tf.zeros_like(d_out_fake)\n",
    "        ))\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        g_loss = tf.reduce_mean(-d_out_fake)\n",
    "\n",
    "    elif gan_type == 'hinge':  # the hinge loss\n",
    "        d_loss_real = tf.reduce_mean(tf.nn.relu(1.0 - d_out_real))\n",
    "        d_loss_fake = tf.reduce_mean(tf.nn.relu(1.0 + d_out_fake))\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        g_loss = -tf.reduce_mean(d_out_fake)\n",
    "\n",
    "    elif gan_type == 'tv':  # the total variation distance\n",
    "        d_loss = tf.reduce_mean(tf.tanh(d_out_fake) - tf.tanh(d_out_real))\n",
    "        g_loss = tf.reduce_mean(-tf.tanh(d_out_fake))\n",
    "\n",
    "    elif gan_type == 'wgan-gp': # WGAN-GP\n",
    "        d_loss = tf.reduce_mean(d_out_fake) - tf.reduce_mean(d_out_real)\n",
    "        GP = gradient_penalty(discriminator, x_real_onehot, x_fake_onehot_appr, config)\n",
    "        d_loss += GP\n",
    "\n",
    "        g_loss = -tf.reduce_mean(d_out_fake)\n",
    "\n",
    "    elif gan_type == 'LS': # LS-GAN\n",
    "        d_loss_real = tf.reduce_mean(tf.squared_difference(d_out_real, 1.0))\n",
    "        d_loss_fake = tf.reduce_mean(tf.square(d_out_fake))\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        g_loss = tf.reduce_mean(tf.squared_difference(d_out_fake, 1.0))\n",
    "\n",
    "    elif gan_type == 'RSGAN':  # relativistic standard GAN\n",
    "        d_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=d_out_real - d_out_fake, labels=tf.ones_like(d_out_real)\n",
    "        ))\n",
    "        g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=d_out_fake - d_out_real, labels=tf.ones_like(d_out_fake)\n",
    "        ))\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Divergence '%s' is not implemented\" % gan_type)\n",
    "\n",
    "    log_pg = tf.reduce_mean(tf.log(gen_o + EPS))  # [1], measures the log p_g(x)\n",
    "\n",
    "    return log_pg, g_loss, d_loss\n",
    "\n",
    "\n",
    "# A function to calculate the gradients and get training operations\n",
    "def get_train_ops(config, g_pretrain_loss, g_loss, d_loss, log_pg, temperature, global_step):\n",
    "    optimizer_name = config['optimizer']\n",
    "    nadv_steps = config['nadv_steps']\n",
    "    d_lr = config['d_lr']\n",
    "    gpre_lr = config['gpre_lr']\n",
    "    gadv_lr = config['gadv_lr']\n",
    "\n",
    "    g_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')\n",
    "    d_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='discriminator')\n",
    "\n",
    "    grad_clip = 5.0  # keep the same with the previous setting\n",
    "\n",
    "    # generator pre-training\n",
    "    pretrain_opt = tf.train.AdamOptimizer(gpre_lr, beta1=0.9, beta2=0.999)\n",
    "    pretrain_grad, _ = tf.clip_by_global_norm(tf.gradients(g_pretrain_loss, g_vars), grad_clip)  # gradient clipping\n",
    "    g_pretrain_op = pretrain_opt.apply_gradients(zip(pretrain_grad, g_vars))\n",
    "\n",
    "    if config['decay']:\n",
    "        d_lr = tf.train.exponential_decay(d_lr, global_step=global_step, decay_steps=nadv_steps, decay_rate=0.1)\n",
    "        gadv_lr = tf.train.exponential_decay(gadv_lr, global_step=global_step, decay_steps=nadv_steps, decay_rate=0.1)\n",
    "\n",
    "    if optimizer_name == 'adam':\n",
    "        d_optimizer = tf.train.AdamOptimizer(d_lr, beta1=0.9, beta2=0.999)\n",
    "        g_optimizer = tf.train.AdamOptimizer(gadv_lr, beta1=0.9, beta2=0.999)\n",
    "        temp_optimizer = tf.train.AdamOptimizer(1e-2, beta1=0.9, beta2=0.999)\n",
    "    elif optimizer_name == 'rmsprop':\n",
    "        d_optimizer = tf.train.RMSPropOptimizer(d_lr)\n",
    "        g_optimizer = tf.train.RMSPropOptimizer(gadv_lr)\n",
    "        temp_optimizer = tf.train.RMSPropOptimizer(1e-2)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    g_grads, _ = tf.clip_by_global_norm(tf.gradients(g_loss, g_vars), grad_clip)  # gradient clipping\n",
    "    g_train_op = g_optimizer.apply_gradients(zip(g_grads, g_vars))\n",
    "\n",
    "    print('len of g_grads without None: {}'.format(len([i for i in g_grads if i is not None])))\n",
    "    print('len of g_grads: {}'.format(len(g_grads)))\n",
    "\n",
    "    # d_train_op = d_optimizer.minimize(d_loss, var_list=d_vars)\n",
    "    d_grads, _ = tf.clip_by_global_norm(tf.gradients(d_loss, d_vars), grad_clip)  # gradient clipping\n",
    "    d_train_op = d_optimizer.apply_gradients(zip(d_grads, d_vars))\n",
    "\n",
    "    temp_grads = tf.gradients(-log_pg, [temperature])\n",
    "    temp_train_op = temp_optimizer.apply_gradients(zip(temp_grads, [temperature]))\n",
    "\n",
    "    return g_pretrain_op, g_train_op, d_train_op, temp_train_op\n",
    "\n",
    "\n",
    "# A function to get various evaluation metrics\n",
    "def get_metrics(config, oracle_loader, gen_loader, oracle_file, gen_file, oracle_model, g_pretrain_loss, x_real, sess):\n",
    "    # set up evaluation metric\n",
    "    metrics = []\n",
    "    if config['nll_oracle']:\n",
    "        nll_oracle = Nll(gen_loader, oracle_model.pretrain_loss, oracle_model.x, sess, name='nll_oracle')\n",
    "        metrics.append(nll_oracle)\n",
    "    if config['nll_gen']:\n",
    "        nll_gen = Nll(oracle_loader, g_pretrain_loss, x_real, sess, name='nll_gen')\n",
    "        metrics.append(nll_gen)\n",
    "    if config['doc_embsim']:\n",
    "        doc_embsim = DocEmbSim(oracle_file, gen_file, config['vocab_size'], name='doc_embsim')\n",
    "        metrics.append(doc_embsim)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# A function to get the summary for each metric\n",
    "def get_metric_summary_op(config):\n",
    "    metrics_pl = []\n",
    "    metrics_sum = []\n",
    "    if config['nll_oracle']:\n",
    "        nll_oracle = tf.placeholder(tf.float32)\n",
    "        metrics_pl.append(nll_oracle)\n",
    "        metrics_sum.append(tf.summary.scalar('metrics/nll_oracle', nll_oracle))\n",
    "\n",
    "    if config['nll_gen']:\n",
    "        nll_gen = tf.placeholder(tf.float32)\n",
    "        metrics_pl.append(nll_gen)\n",
    "        metrics_sum.append(tf.summary.scalar('metrics/nll_gen', nll_gen))\n",
    "\n",
    "    if config['doc_embsim']:\n",
    "        doc_embsim = tf.placeholder(tf.float32)\n",
    "        metrics_pl.append(doc_embsim)\n",
    "        metrics_sum.append(tf.summary.scalar('metrics/doc_embsim', doc_embsim))\n",
    "\n",
    "    metric_summary_op = tf.summary.merge(metrics_sum)\n",
    "    return metrics_pl, metric_summary_op\n",
    "\n",
    "\n",
    "def get_fixed_temperature(temper, i, nadv_steps, adapt):\n",
    "    # using a fixed number of maximum adversarial steps\n",
    "    N = 5000\n",
    "    assert nadv_steps <= N\n",
    "    if adapt == 'no':\n",
    "        temper_var_np = temper  # no increase\n",
    "    elif adapt == 'lin':\n",
    "        temper_var_np = 1 + i / (N - 1) * (temper - 1)  # linear increase\n",
    "    elif adapt == 'exp':\n",
    "        temper_var_np = temper ** (i / N)  # exponential increase\n",
    "    elif adapt == 'log':\n",
    "        temper_var_np = 1 + (temper - 1) / np.log(N) * np.log(i + 1)  # logarithm increase\n",
    "    elif adapt == 'sigmoid':\n",
    "        temper_var_np = (temper - 1) * 1 / (1 + np.exp((N / 2 - i) * 20 / N)) + 1  # sigmoid increase\n",
    "    elif adapt == 'quad':\n",
    "        temper_var_np = (temper - 1) / (N - 1)**2 * i ** 2 + 1\n",
    "    elif adapt == 'sqrt':\n",
    "        temper_var_np = (temper - 1) / np.sqrt(N - 1) * np.sqrt(i) + 1\n",
    "    else:\n",
    "        raise Exception(\"Unknown adapt type!\")\n",
    "\n",
    "    return temper_var_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wj-rFeaIY12T"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class OracleDataLoader():\n",
    "    def __init__(self, batch_size, seq_length, end_token=0):\n",
    "        self.batch_size = batch_size\n",
    "        self.token_stream = []\n",
    "        self.seq_length = seq_length\n",
    "        self.end_token = end_token\n",
    "\n",
    "    def create_batches(self, data_file):\n",
    "        self.token_stream = []\n",
    "\n",
    "        with open(data_file, 'r') as raw:\n",
    "            for line in raw:\n",
    "                line = line.strip().split()\n",
    "                parse_line = [int(x) for x in line]\n",
    "                if len(parse_line) > self.seq_length:\n",
    "                    self.token_stream.append(parse_line[:self.seq_length])\n",
    "                else:\n",
    "                    while len(parse_line) < self.seq_length:\n",
    "                        parse_line.append(self.end_token)\n",
    "                    if len(parse_line) == self.seq_length:\n",
    "                        self.token_stream.append(parse_line)\n",
    "\n",
    "        self.num_batch = int(len(self.token_stream) / self.batch_size)\n",
    "        self.token_stream = self.token_stream[:self.num_batch * self.batch_size]\n",
    "        self.sequence_batches = np.split(np.array(self.token_stream), self.num_batch, 0)\n",
    "        self.pointer = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        ret = self.sequence_batches[self.pointer]\n",
    "        self.pointer = (self.pointer + 1) % self.num_batch\n",
    "        return ret\n",
    "\n",
    "    def random_batch(self):\n",
    "        rn_pointer = random.randint(0, self.num_batch - 1)\n",
    "        ret = self.sequence_batches[rn_pointer]\n",
    "        return ret\n",
    "\n",
    "    def reset_pointer(self):\n",
    "        self.pointer = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hChWfhrjZiP2"
   },
   "source": [
    "### REAL ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k02L8u_aZCEf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class RealDataLoader():\n",
    "    def __init__(self, batch_size, seq_length, end_token=0):\n",
    "        self.batch_size = batch_size\n",
    "        self.token_stream = []\n",
    "        self.seq_length = seq_length\n",
    "        self.end_token = end_token\n",
    "\n",
    "    def create_batches(self, data_file):\n",
    "        self.token_stream = []\n",
    "\n",
    "        with open(data_file, 'r') as raw:\n",
    "            for line in raw:\n",
    "                line = line.strip().split()\n",
    "                parse_line = [int(x) for x in line]\n",
    "                if len(parse_line) > self.seq_length:\n",
    "                    self.token_stream.append(parse_line[:self.seq_length])\n",
    "                else:\n",
    "                    while len(parse_line) < self.seq_length:\n",
    "                        parse_line.append(self.end_token)\n",
    "                    if len(parse_line) == self.seq_length:\n",
    "                        self.token_stream.append(parse_line)\n",
    "\n",
    "        self.num_batch = int(len(self.token_stream) / self.batch_size)\n",
    "        self.token_stream = self.token_stream[:self.num_batch * self.batch_size]\n",
    "        self.sequence_batches = np.split(np.array(self.token_stream), self.num_batch, 0)\n",
    "        self.pointer = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        ret = self.sequence_batches[self.pointer]\n",
    "        self.pointer = (self.pointer + 1) % self.num_batch\n",
    "        return ret\n",
    "\n",
    "    def random_batch(self):\n",
    "        rn_pointer = random.randint(0, self.num_batch - 1)\n",
    "        ret = self.sequence_batches[rn_pointer]\n",
    "        return ret\n",
    "\n",
    "    def reset_pointer(self):\n",
    "        self.pointer = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AUhXRrWJZHNm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "EPS = 1e-10\n",
    "\n",
    "\n",
    "# A function to initiate the graph and train the networks\n",
    "def real_train(generator, discriminator, oracle_loader, config):\n",
    "    batch_size = config['batch_size']\n",
    "    num_sentences = config['num_sentences']\n",
    "    vocab_size = config['vocab_size']\n",
    "    seq_len = config['seq_len']\n",
    "    data_dir = config['data_dir']\n",
    "    dataset = config['dataset']\n",
    "    log_dir = config['log_dir']\n",
    "    sample_dir = config['sample_dir']\n",
    "    npre_epochs = config['npre_epochs']\n",
    "    nadv_steps = config['nadv_steps']\n",
    "    temper = config['temperature']\n",
    "    adapt = config['adapt']\n",
    "\n",
    "    # filename\n",
    "    oracle_file = os.path.join(sample_dir, 'oracle_{}.txt'.format(dataset))\n",
    "    gen_file = os.path.join(sample_dir, 'generator.txt')\n",
    "    gen_text_file = os.path.join(sample_dir, 'generator_text.txt')\n",
    "    csv_file = os.path.join(log_dir, 'experiment-log-rmcgan.csv')\n",
    "    data_file = os.path.join(data_dir, '{}.txt'.format(dataset))\n",
    "    if dataset == 'image_coco':\n",
    "        test_file = os.path.join(data_dir, 'testdata/test_coco.txt')\n",
    "    elif dataset == 'emnlp_news':\n",
    "        test_file = os.path.join(data_dir, 'testdata/test_emnlp.txt')\n",
    "    elif dataset == 'fce_train_new':\n",
    "        test_file = os.path.join(data_dir, 'testdata/fce_test_new.txt')\n",
    "    else:\n",
    "        raise NotImplementedError('Unknown dataset!')\n",
    "\n",
    "    # create necessary directories\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    if not os.path.exists(sample_dir):\n",
    "        os.makedirs(sample_dir)\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    # placeholder definitions\n",
    "    x_real = tf.placeholder(tf.int32, [batch_size, seq_len], name=\"x_real\")  # tokens of oracle sequences\n",
    "\n",
    "    temperature = tf.Variable(1., trainable=False, name='temperature')\n",
    "\n",
    "    x_real_onehot = tf.one_hot(x_real, vocab_size)  # batch_size x seq_len x vocab_size\n",
    "    assert x_real_onehot.get_shape().as_list() == [batch_size, seq_len, vocab_size]\n",
    "\n",
    "    # generator and discriminator outputs\n",
    "    x_fake_onehot_appr, x_fake, g_pretrain_loss, gen_o = generator(x_real=x_real, temperature=temperature)\n",
    "    d_out_real = discriminator(x_onehot=x_real_onehot)\n",
    "    d_out_fake = discriminator(x_onehot=x_fake_onehot_appr)\n",
    "\n",
    "    # GAN / Divergence type\n",
    "    log_pg, g_loss, d_loss = get_losses(d_out_real, d_out_fake, x_real_onehot, x_fake_onehot_appr,\n",
    "                                                    gen_o, discriminator, config)\n",
    "\n",
    "    # Global step\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    global_step_op = global_step.assign_add(1)\n",
    "\n",
    "    # Train ops\n",
    "    g_pretrain_op, g_train_op, d_train_op = get_train_ops(config, g_pretrain_loss, g_loss, d_loss,\n",
    "                                                          log_pg, temperature, global_step)\n",
    "\n",
    "    # Record wall clock time\n",
    "    time_diff = tf.placeholder(tf.float32)\n",
    "    Wall_clock_time = tf.Variable(0., trainable=False)\n",
    "    update_Wall_op = Wall_clock_time.assign_add(time_diff)\n",
    "\n",
    "    # Temperature placeholder\n",
    "    temp_var = tf.placeholder(tf.float32)\n",
    "    update_temperature_op = temperature.assign(temp_var)\n",
    "\n",
    "    # Loss summaries\n",
    "    loss_summaries = [\n",
    "        tf.summary.scalar('loss/discriminator', d_loss),\n",
    "        tf.summary.scalar('loss/g_loss', g_loss),\n",
    "        tf.summary.scalar('loss/log_pg', log_pg),\n",
    "        tf.summary.scalar('loss/Wall_clock_time', Wall_clock_time),\n",
    "        tf.summary.scalar('loss/temperature', temperature),\n",
    "    ]\n",
    "    loss_summary_op = tf.summary.merge(loss_summaries)\n",
    "\n",
    "    # Metric Summaries\n",
    "    metrics_pl, metric_summary_op = get_metric_summary_op(config)\n",
    "\n",
    "    # ------------- initial the graph --------------\n",
    "    with init_sess() as sess:\n",
    "        if config['checkpoint_restore']:\n",
    "          saver = tf.train.Saver()\n",
    "          saver.restore(sess, config['checkpoint'])\n",
    "          print('[INFO]: Restored checkpoint')\n",
    "          \n",
    "        log = open(csv_file, 'w')\n",
    "        sum_writer = tf.summary.FileWriter(os.path.join(log_dir, 'summary'), sess.graph)\n",
    "\n",
    "        # generate oracle data and create batches\n",
    "        index_word_dict = get_oracle_file(data_file, oracle_file, seq_len)\n",
    "        oracle_loader.create_batches(oracle_file)\n",
    "\n",
    "        metrics = get_metrics(config, oracle_loader, test_file, gen_text_file, g_pretrain_loss, x_real, sess)\n",
    "\n",
    "        if not config['checkpoint_restore']:\n",
    "          print('Start pre-training...')\n",
    "          for epoch in range(npre_epochs):\n",
    "              # pre-training\n",
    "              g_pretrain_loss_np = pre_train_epoch(sess, g_pretrain_op, g_pretrain_loss, x_real, oracle_loader)\n",
    "\n",
    "              # Test\n",
    "              ntest_pre = 10\n",
    "              if np.mod(epoch, ntest_pre) == 0:\n",
    "                  # generate fake data and create batches\n",
    "                  gen_save_file = os.path.join(sample_dir, 'pre_samples_{:05d}.txt'.format(epoch))\n",
    "                  generate_samples(sess, x_fake, batch_size, num_sentences, gen_file)\n",
    "                  get_real_test_file(gen_file, gen_save_file, index_word_dict)\n",
    "                  get_real_test_file(gen_file, gen_text_file, index_word_dict)\n",
    "\n",
    "                  # write summaries\n",
    "                  scores = [metric.get_score() for metric in metrics]\n",
    "                  metrics_summary_str = sess.run(metric_summary_op, feed_dict=dict(zip(metrics_pl, scores)))\n",
    "                  sum_writer.add_summary(metrics_summary_str, epoch)\n",
    "\n",
    "                  msg = 'pre_gen_epoch:' + str(epoch) + ', g_pre_loss: %.4f' % g_pretrain_loss_np\n",
    "                  metric_names = [metric.get_name() for metric in metrics]\n",
    "                  for (name, score) in zip(metric_names, scores):\n",
    "                      msg += ', ' + name + ': %.4f' % score\n",
    "                  print(msg)\n",
    "                  log.write(msg)\n",
    "                  log.write('\\n')\n",
    "\n",
    "        print('Start adversarial training...')\n",
    "        progress = tqdm(range(nadv_steps))\n",
    "        for _ in progress:\n",
    "            niter = sess.run(global_step)\n",
    "\n",
    "            t0 = time.time()\n",
    "\n",
    "            # adversarial training\n",
    "            for _ in range(config['gsteps']):\n",
    "                sess.run(g_train_op, feed_dict={x_real: oracle_loader.random_batch()})\n",
    "            for _ in range(config['dsteps']):\n",
    "                sess.run(d_train_op, feed_dict={x_real: oracle_loader.random_batch()})\n",
    "\n",
    "            t1 = time.time()\n",
    "            sess.run(update_Wall_op, feed_dict={time_diff: t1 - t0})\n",
    "\n",
    "            # temperature\n",
    "            temp_var_np = get_fixed_temperature(temper, niter, nadv_steps, adapt)\n",
    "            sess.run(update_temperature_op, feed_dict={temp_var: temp_var_np})\n",
    "\n",
    "            feed = {x_real: oracle_loader.random_batch()}\n",
    "            g_loss_np, d_loss_np, loss_summary_str = sess.run([g_loss, d_loss, loss_summary_op], feed_dict=feed)\n",
    "            sum_writer.add_summary(loss_summary_str, niter)\n",
    "\n",
    "            sess.run(global_step_op)\n",
    "\n",
    "            progress.set_description('g_loss: %4.4f, d_loss: %4.4f' % (g_loss_np, d_loss_np))\n",
    "\n",
    "            # Test\n",
    "            if np.mod(niter, config['ntest']) == 0:\n",
    "              \n",
    "                #Save the model\n",
    "                #saver= tf.train.Saver()\n",
    "                #saver.save(sess, '/content/drive/My Drive/model_saver/' + str(niter) + '.ckpt')\n",
    "                \n",
    "                # generate fake data and create batches\n",
    "                gen_save_file = os.path.join(sample_dir, 'adv_samples_{:05d}.txt'.format(niter))\n",
    "                generate_samples(sess, x_fake, batch_size, num_sentences, gen_file, iteration = niter)\n",
    "                get_real_test_file(gen_file, gen_save_file, index_word_dict)\n",
    "                get_real_test_file(gen_file, gen_text_file, index_word_dict)\n",
    "\n",
    "                # write summaries\n",
    "                scores = [metric.get_score() for metric in metrics]\n",
    "                metrics_summary_str = sess.run(metric_summary_op, feed_dict=dict(zip(metrics_pl, scores)))\n",
    "                sum_writer.add_summary(metrics_summary_str, niter + config['npre_epochs'])\n",
    "\n",
    "                msg = 'adv_step: ' + str(niter)\n",
    "                metric_names = [metric.get_name() for metric in metrics]\n",
    "                for (name, score) in zip(metric_names, scores):\n",
    "                    msg += ', ' + name + ': %.4f' % score\n",
    "                print(msg)\n",
    "                log.write(msg)\n",
    "                log.write('\\n')\n",
    "\n",
    "\n",
    "# A function to get different GAN losses\n",
    "def get_losses(d_out_real, d_out_fake, x_real_onehot, x_fake_onehot_appr, gen_o, discriminator, config):\n",
    "    batch_size = config['batch_size']\n",
    "    gan_type = config['gan_type']\n",
    "\n",
    "    if gan_type == 'standard':  # the non-satuating GAN loss\n",
    "        d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=d_out_real, labels=tf.ones_like(d_out_real)\n",
    "        ))\n",
    "        d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=d_out_fake, labels=tf.zeros_like(d_out_fake)\n",
    "        ))\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=d_out_fake, labels=tf.ones_like(d_out_fake)\n",
    "        ))\n",
    "\n",
    "    elif gan_type == 'JS':  # the vanilla GAN loss\n",
    "        d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=d_out_real, labels=tf.ones_like(d_out_real)\n",
    "        ))\n",
    "        d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=d_out_fake, labels=tf.zeros_like(d_out_fake)\n",
    "        ))\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        g_loss = -d_loss_fake\n",
    "\n",
    "    elif gan_type == 'KL':  # the GAN loss implicitly minimizing KL-divergence\n",
    "        d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=d_out_real, labels=tf.ones_like(d_out_real)\n",
    "        ))\n",
    "        d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=d_out_fake, labels=tf.zeros_like(d_out_fake)\n",
    "        ))\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        g_loss = tf.reduce_mean(-d_out_fake)\n",
    "\n",
    "    elif gan_type == 'hinge':  # the hinge loss\n",
    "        d_loss_real = tf.reduce_mean(tf.nn.relu(1.0 - d_out_real))\n",
    "        d_loss_fake = tf.reduce_mean(tf.nn.relu(1.0 + d_out_fake))\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        g_loss = -tf.reduce_mean(d_out_fake)\n",
    "\n",
    "    elif gan_type == 'tv':  # the total variation distance\n",
    "        d_loss = tf.reduce_mean(tf.tanh(d_out_fake) - tf.tanh(d_out_real))\n",
    "        g_loss = tf.reduce_mean(-tf.tanh(d_out_fake))\n",
    "\n",
    "    elif gan_type == 'wgan-gp':  # WGAN-GP\n",
    "        d_loss = tf.reduce_mean(d_out_fake) - tf.reduce_mean(d_out_real)\n",
    "        GP = gradient_penalty(discriminator, x_real_onehot, x_fake_onehot_appr, config)\n",
    "        d_loss += GP\n",
    "\n",
    "        g_loss = -tf.reduce_mean(d_out_fake)\n",
    "\n",
    "    elif gan_type == 'LS':  # LS-GAN\n",
    "        d_loss_real = tf.reduce_mean(tf.squared_difference(d_out_real, 1.0))\n",
    "        d_loss_fake = tf.reduce_mean(tf.square(d_out_fake))\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        g_loss = tf.reduce_mean(tf.squared_difference(d_out_fake, 1.0))\n",
    "\n",
    "    elif gan_type == 'RSGAN':  # relativistic standard GAN\n",
    "        d_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=d_out_real - d_out_fake, labels=tf.ones_like(d_out_real)\n",
    "        ))\n",
    "        g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=d_out_fake - d_out_real, labels=tf.ones_like(d_out_fake)\n",
    "        ))\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Divergence '%s' is not implemented\" % gan_type)\n",
    "\n",
    "    log_pg = tf.reduce_mean(tf.log(gen_o + EPS))  # [1], measures the log p_g(x)\n",
    "\n",
    "    return log_pg, g_loss, d_loss\n",
    "\n",
    "\n",
    "# A function to calculate the gradients and get training operations\n",
    "def get_train_ops(config, g_pretrain_loss, g_loss, d_loss, log_pg, temperature, global_step):\n",
    "    optimizer_name = config['optimizer']\n",
    "    nadv_steps = config['nadv_steps']\n",
    "    d_lr = config['d_lr']\n",
    "    gpre_lr = config['gpre_lr']\n",
    "    gadv_lr = config['gadv_lr']\n",
    "\n",
    "    g_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')\n",
    "    d_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='discriminator')\n",
    "\n",
    "    grad_clip = 5.0  # keep the same with the previous setting\n",
    "\n",
    "    # generator pre-training\n",
    "    pretrain_opt = tf.train.AdamOptimizer(gpre_lr, beta1=0.9, beta2=0.999)\n",
    "    pretrain_grad, _ = tf.clip_by_global_norm(tf.gradients(g_pretrain_loss, g_vars), grad_clip)  # gradient clipping\n",
    "    g_pretrain_op = pretrain_opt.apply_gradients(zip(pretrain_grad, g_vars))\n",
    "\n",
    "    # decide if using the weight decaying\n",
    "    if config['decay']:\n",
    "        d_lr = tf.train.exponential_decay(d_lr, global_step=global_step, decay_steps=nadv_steps, decay_rate=0.1)\n",
    "        gadv_lr = tf.train.exponential_decay(gadv_lr, global_step=global_step, decay_steps=nadv_steps, decay_rate=0.1)\n",
    "\n",
    "    # Adam optimizer\n",
    "    if optimizer_name == 'adam':\n",
    "        d_optimizer = tf.train.AdamOptimizer(d_lr, beta1=0.9, beta2=0.999)\n",
    "        g_optimizer = tf.train.AdamOptimizer(gadv_lr, beta1=0.9, beta2=0.999)\n",
    "        temp_optimizer = tf.train.AdamOptimizer(1e-2, beta1=0.9, beta2=0.999)\n",
    "\n",
    "    # RMSProp optimizer\n",
    "    elif optimizer_name == 'rmsprop':\n",
    "        d_optimizer = tf.train.RMSPropOptimizer(d_lr)\n",
    "        g_optimizer = tf.train.RMSPropOptimizer(gadv_lr)\n",
    "        temp_optimizer = tf.train.RMSPropOptimizer(1e-2)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # gradient clipping\n",
    "    g_grads, _ = tf.clip_by_global_norm(tf.gradients(g_loss, g_vars), grad_clip)\n",
    "    g_train_op = g_optimizer.apply_gradients(zip(g_grads, g_vars))\n",
    "\n",
    "    print('len of g_grads without None: {}'.format(len([i for i in g_grads if i is not None])))\n",
    "    print('len of g_grads: {}'.format(len(g_grads)))\n",
    "\n",
    "    # gradient clipping\n",
    "    d_grads, _ = tf.clip_by_global_norm(tf.gradients(d_loss, d_vars), grad_clip)\n",
    "    d_train_op = d_optimizer.apply_gradients(zip(d_grads, d_vars))\n",
    "\n",
    "    return g_pretrain_op, g_train_op, d_train_op\n",
    "\n",
    "\n",
    "# A function to get various evaluation metrics\n",
    "def get_metrics(config, oracle_loader, test_file, gen_file, g_pretrain_loss, x_real, sess):\n",
    "    # set up evaluation metric\n",
    "    metrics = []\n",
    "    if config['nll_gen']:\n",
    "        nll_gen = Nll(oracle_loader, g_pretrain_loss, x_real, sess, name='nll_gen')\n",
    "        metrics.append(nll_gen)\n",
    "    if config['doc_embsim']:\n",
    "        doc_embsim = DocEmbSim(test_file, gen_file, config['vocab_size'], name='doc_embsim')\n",
    "        metrics.append(doc_embsim)\n",
    "    if config['bleu']:\n",
    "        for i in range(2, 6):\n",
    "            bleu = Bleu(test_text=gen_file, real_text=test_file, gram=i, name='bleu' + str(i))\n",
    "            metrics.append(bleu)\n",
    "    if config['selfbleu']:\n",
    "        for i in range(2, 6):\n",
    "            selfbleu = SelfBleu(test_text=gen_file, gram=i, name='selfbleu' + str(i))\n",
    "            metrics.append(selfbleu)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# A function to get the summary for each metric\n",
    "def get_metric_summary_op(config):\n",
    "    metrics_pl = []\n",
    "    metrics_sum = []\n",
    "\n",
    "    if config['nll_gen']:\n",
    "        nll_gen = tf.placeholder(tf.float32)\n",
    "        metrics_pl.append(nll_gen)\n",
    "        metrics_sum.append(tf.summary.scalar('metrics/nll_gen', nll_gen))\n",
    "\n",
    "    if config['doc_embsim']:\n",
    "        doc_embsim = tf.placeholder(tf.float32)\n",
    "        metrics_pl.append(doc_embsim)\n",
    "        metrics_sum.append(tf.summary.scalar('metrics/doc_embsim', doc_embsim))\n",
    "\n",
    "    if config['bleu']:\n",
    "        for i in range(2, 6):\n",
    "            temp_pl = tf.placeholder(tf.float32, name='bleu{}'.format(i))\n",
    "            metrics_pl.append(temp_pl)\n",
    "            metrics_sum.append(tf.summary.scalar('metrics/bleu{}'.format(i), temp_pl))\n",
    "\n",
    "    if config['selfbleu']:\n",
    "        for i in range(2, 6):\n",
    "            temp_pl = tf.placeholder(tf.float32, name='selfbleu{}'.format(i))\n",
    "            metrics_pl.append(temp_pl)\n",
    "            metrics_sum.append(tf.summary.scalar('metrics/selfbleu{}'.format(i), temp_pl))\n",
    "\n",
    "    metric_summary_op = tf.summary.merge(metrics_sum)\n",
    "    return metrics_pl, metric_summary_op\n",
    "\n",
    "\n",
    "# A function to set up different temperature control policies\n",
    "def get_fixed_temperature(temper, i, nadv_steps, adapt):\n",
    "    # using a fixed number of maximum adversarial steps\n",
    "    N = 5000\n",
    "    assert nadv_steps <= N\n",
    "    if adapt == 'no':\n",
    "        temper_var_np = temper  # no increase\n",
    "    elif adapt == 'lin':\n",
    "        temper_var_np = 1 + i / (N - 1) * (temper - 1)  # linear increase\n",
    "    elif adapt == 'exp':\n",
    "        temper_var_np = temper ** (i / N)  # exponential increase\n",
    "    elif adapt == 'log':\n",
    "        temper_var_np = 1 + (temper - 1) / np.log(N) * np.log(i + 1)  # logarithm increase\n",
    "    elif adapt == 'sigmoid':\n",
    "        temper_var_np = (temper - 1) * 1 / (1 + np.exp((N / 2 - i) * 20 / N)) + 1  # sigmoid increase\n",
    "    elif adapt == 'quad':\n",
    "        temper_var_np = (temper - 1) / (N - 1)**2 * i ** 2 + 1\n",
    "    elif adapt == 'sqrt':\n",
    "        temper_var_np = (temper - 1) / np.sqrt(N - 1) * np.sqrt(i) + 1\n",
    "    else:\n",
    "        raise Exception(\"Unknown adapt type!\")\n",
    "\n",
    "    return temper_var_np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4pr_JO9MZVo3"
   },
   "source": [
    "## MEMORY ARCHITECTURE ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IOENRBKknIR3"
   },
   "outputs": [],
   "source": [
    "\"\"\"Relational Memory architecture.\n",
    "\n",
    "An implementation of the architecture described in \"Relational Recurrent\n",
    "Neural Networks\", Santoro et al., 2018.\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class RelationalMemory(object):\n",
    "    \"\"\"Relational Memory Core.\"\"\"\n",
    "\n",
    "    def __init__(self, mem_slots, head_size, num_heads=1, num_blocks=1,\n",
    "                 forget_bias=1.0, input_bias=0.0, gate_style='unit',\n",
    "                 attention_mlp_layers=2, key_size=None, name='relational_memory'):\n",
    "        \"\"\"Constructs a `RelationalMemory` object.\n",
    "\n",
    "        Args:\n",
    "          mem_slots: The total number of memory slots to use.\n",
    "          head_size: The size of an attention head.\n",
    "          num_heads: The number of attention heads to use. Defaults to 1.\n",
    "          num_blocks: Number of times to compute attention per time step. Defaults\n",
    "            to 1.\n",
    "          forget_bias: Bias to use for the forget gate, assuming we are using\n",
    "            some form of gating. Defaults to 1.\n",
    "          input_bias: Bias to use for the input gate, assuming we are using\n",
    "            some form of gating. Defaults to 0.\n",
    "          gate_style: Whether to use per-element gating ('unit'),\n",
    "            per-memory slot gating ('memory'), or no gating at all (None).\n",
    "            Defaults to `unit`.\n",
    "          attention_mlp_layers: Number of layers to use in the post-attention\n",
    "            MLP. Defaults to 2.\n",
    "          key_size: Size of vector to use for key & query vectors in the attention\n",
    "            computation. Defaults to None, in which case we use `head_size`.\n",
    "          name: Name of the module.\n",
    "\n",
    "        Raises:\n",
    "          ValueError: gate_style not one of [None, 'memory', 'unit'].\n",
    "          ValueError: num_blocks is < 1.\n",
    "          ValueError: attention_mlp_layers is < 1.\n",
    "        \"\"\"\n",
    "\n",
    "        self._mem_slots = mem_slots\n",
    "        self._head_size = head_size\n",
    "        self._num_heads = num_heads\n",
    "        self._mem_size = self._head_size * self._num_heads\n",
    "        self._name = name\n",
    "\n",
    "        if num_blocks < 1:\n",
    "            raise ValueError('num_blocks must be >= 1. Got: {}.'.format(num_blocks))\n",
    "        self._num_blocks = num_blocks\n",
    "\n",
    "        self._forget_bias = forget_bias\n",
    "        self._input_bias = input_bias\n",
    "\n",
    "        if gate_style not in ['unit', 'memory', None]:\n",
    "            raise ValueError(\n",
    "                'gate_style must be one of [\\'unit\\', \\'memory\\', None]. Got: '\n",
    "                '{}.'.format(gate_style))\n",
    "        self._gate_style = gate_style\n",
    "\n",
    "        if attention_mlp_layers < 1:\n",
    "            raise ValueError('attention_mlp_layers must be >= 1. Got: {}.'.format(\n",
    "                attention_mlp_layers))\n",
    "        self._attention_mlp_layers = attention_mlp_layers\n",
    "\n",
    "        self._key_size = key_size if key_size else self._head_size\n",
    "\n",
    "        self._template = tf.make_template(self._name, self._build)  # wrapper for variable sharing\n",
    "\n",
    "    def initial_state(self, batch_size):\n",
    "        \"\"\"Creates the initial memory.\n",
    "\n",
    "        We should ensure each row of the memory is initialized to be unique,\n",
    "        so initialize the matrix to be the identity. We then pad or truncate\n",
    "        as necessary so that init_state is of size\n",
    "        (batch_size, self._mem_slots, self._mem_size).\n",
    "\n",
    "        Args:\n",
    "          batch_size: The size of the batch.\n",
    "\n",
    "        Returns:\n",
    "          init_state: A truncated or padded matrix of size\n",
    "            (batch_size, self._mem_slots, self._mem_size).\n",
    "        \"\"\"\n",
    "        init_state = tf.eye(self._mem_slots, batch_shape=[batch_size])\n",
    "\n",
    "        # Pad the matrix with zeros.\n",
    "        if self._mem_size > self._mem_slots:\n",
    "            difference = self._mem_size - self._mem_slots\n",
    "            pad = tf.zeros((batch_size, self._mem_slots, difference))\n",
    "            init_state = tf.concat([init_state, pad], -1)\n",
    "        # Truncation. Take the first `self._mem_size` components.\n",
    "        elif self._mem_size < self._mem_slots:\n",
    "            init_state = init_state[:, :, :self._mem_size]\n",
    "        return init_state\n",
    "\n",
    "    def _multihead_attention(self, memory):\n",
    "        \"\"\"Perform multi-head attention from 'Attention is All You Need'.\n",
    "\n",
    "        Implementation of the attention mechanism from\n",
    "        https://arxiv.org/abs/1706.03762.\n",
    "\n",
    "        Args:\n",
    "          memory: Memory tensor to perform attention on, with size [B, N, H*V].\n",
    "\n",
    "        Returns:\n",
    "          new_memory: New memory tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        qkv_size = 2 * self._key_size + self._head_size\n",
    "        total_size = qkv_size * self._num_heads  # Denote as F.\n",
    "        batch_size = memory.get_shape().as_list()[0]  # Denote as B\n",
    "        memory_flattened = tf.reshape(memory, [-1, self._mem_size])  # [B * N, H * V]\n",
    "        qkv = linear(memory_flattened, total_size, use_bias=False, scope='lin_qkv')  # [B*N, F]\n",
    "        qkv = tf.reshape(qkv, [batch_size, -1, total_size])  # [B, N, F]\n",
    "        qkv = tf.contrib.layers.layer_norm(qkv, trainable=True)  # [B, N, F]\n",
    "\n",
    "        # [B, N, F] -> [B, N, H, F/H]\n",
    "        qkv_reshape = tf.reshape(qkv, [batch_size, -1, self._num_heads, qkv_size])\n",
    "\n",
    "        # [B, N, H, F/H] -> [B, H, N, F/H]\n",
    "        qkv_transpose = tf.transpose(qkv_reshape, [0, 2, 1, 3])\n",
    "        q, k, v = tf.split(qkv_transpose, [self._key_size, self._key_size, self._head_size], -1)\n",
    "\n",
    "        q *= qkv_size ** -0.5\n",
    "        dot_product = tf.matmul(q, k, transpose_b=True)  # [B, H, N, N]\n",
    "        weights = tf.nn.softmax(dot_product)\n",
    "\n",
    "        output = tf.matmul(weights, v)  # [B, H, N, V]\n",
    "\n",
    "        # [B, H, N, V] -> [B, N, H, V]\n",
    "        output_transpose = tf.transpose(output, [0, 2, 1, 3])\n",
    "\n",
    "        # [B, N, H, V] -> [B, N, H * V]\n",
    "        new_memory = tf.reshape(output_transpose, [batch_size, -1, self._mem_size])\n",
    "        return new_memory\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return tf.TensorShape([self._mem_slots, self._mem_size])\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return tf.TensorShape(self._mem_slots * self._mem_size)\n",
    "\n",
    "    def _calculate_gate_size(self):\n",
    "        \"\"\"Calculate the gate size from the gate_style.\n",
    "\n",
    "        Returns:\n",
    "          The per sample, per head parameter size of each gate.\n",
    "        \"\"\"\n",
    "        if self._gate_style == 'unit':\n",
    "            return self._mem_size\n",
    "        elif self._gate_style == 'memory':\n",
    "            return 1\n",
    "        else:  # self._gate_style == None\n",
    "            return 0\n",
    "\n",
    "    def _create_gates(self, inputs, memory):\n",
    "        \"\"\"Create input and forget gates for this step using `inputs` and `memory`.\n",
    "\n",
    "        Args:\n",
    "          inputs: Tensor input.\n",
    "          memory: The current state of memory.\n",
    "\n",
    "        Returns:\n",
    "          input_gate: A LSTM-like insert gate.\n",
    "          forget_gate: A LSTM-like forget gate.\n",
    "        \"\"\"\n",
    "        # We'll create the input and forget gates at once. Hence, calculate double\n",
    "        # the gate size.\n",
    "        num_gates = 2 * self._calculate_gate_size()\n",
    "        batch_size = memory.get_shape().as_list()[0]\n",
    "\n",
    "        memory = tf.tanh(memory)  # B x N x H * V\n",
    "\n",
    "        inputs = tf.reshape(inputs, [batch_size, -1])  # B x In_size\n",
    "        gate_inputs = linear(inputs, num_gates, use_bias=False, scope='gate_in')  # B x num_gates\n",
    "        gate_inputs = tf.expand_dims(gate_inputs, axis=1)  # B x 1 x num_gates\n",
    "\n",
    "        memory_flattened = tf.reshape(memory, [-1, self._mem_size])  # [B * N, H * V]\n",
    "        gate_memory = linear(memory_flattened, num_gates, use_bias=False, scope='gate_mem')  # [B * N, num_gates]\n",
    "        gate_memory = tf.reshape(gate_memory, [batch_size, self._mem_slots, num_gates])  # [B, N, num_gates]\n",
    "\n",
    "        gates = tf.split(gate_memory + gate_inputs, num_or_size_splits=2, axis=2)\n",
    "        input_gate, forget_gate = gates  # B x N x num_gates/2, B x N x num_gates/2\n",
    "\n",
    "        input_gate = tf.sigmoid(input_gate + self._input_bias)\n",
    "        forget_gate = tf.sigmoid(forget_gate + self._forget_bias)\n",
    "\n",
    "        return input_gate, forget_gate\n",
    "\n",
    "    def _attend_over_memory(self, memory):\n",
    "        \"\"\"Perform multiheaded attention over `memory`.\n",
    "\n",
    "        Args:\n",
    "          memory: Current relational memory.\n",
    "\n",
    "        Returns:\n",
    "          The attended-over memory.\n",
    "        \"\"\"\n",
    "\n",
    "        for _ in range(self._num_blocks):\n",
    "            attended_memory = self._multihead_attention(memory)  # [B, N, H * V]\n",
    "\n",
    "            # Add a skip connection to the multiheaded attention's input.\n",
    "            memory = tf.contrib.layers.layer_norm(memory + attended_memory, trainable=True)  # [B, N, H * V]\n",
    "\n",
    "            # Add a mlp map\n",
    "            batch_size = memory.get_shape().as_list()[0]\n",
    "\n",
    "            memory_mlp = tf.reshape(memory, [-1, self._mem_size])  # [B * N, H * V]\n",
    "            memory_mlp = mlp(memory_mlp, [self._mem_size] * self._attention_mlp_layers)  # [B * N, H * V]\n",
    "            memory_mlp = tf.reshape(memory_mlp, [batch_size, -1, self._mem_size])\n",
    "\n",
    "            # Add a skip connection to the memory_mlp's input.\n",
    "            memory = tf.contrib.layers.layer_norm(memory + memory_mlp, trainable=True)  # [B, N, H * V]\n",
    "\n",
    "        return memory\n",
    "\n",
    "    def _build(self, inputs, memory):\n",
    "        \"\"\"Adds relational memory to the TensorFlow graph.\n",
    "\n",
    "        Args:\n",
    "          inputs: Tensor input.\n",
    "          memory: Memory output from the previous time step.\n",
    "\n",
    "        Returns:\n",
    "          output: This time step's output.\n",
    "          next_memory: The next version of memory to use.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = memory.get_shape().as_list()[0]\n",
    "        inputs = tf.reshape(inputs, [batch_size, -1])  # [B, In_size]\n",
    "        inputs = linear(inputs, self._mem_size, use_bias=True, scope='input_for_cancat')  # [B, V * H]\n",
    "        inputs_reshape = tf.expand_dims(inputs, 1)  # [B, 1, V * H]\n",
    "\n",
    "        memory_plus_input = tf.concat([memory, inputs_reshape], axis=1)  # [B, N + 1, V * H]\n",
    "        next_memory = self._attend_over_memory(memory_plus_input)  # [B, N + 1, V * H]\n",
    "\n",
    "        n = inputs_reshape.get_shape().as_list()[1]\n",
    "        next_memory = next_memory[:, :-n, :]  # [B, N, V * H]\n",
    "\n",
    "        if self._gate_style == 'unit' or self._gate_style == 'memory':\n",
    "            self._input_gate, self._forget_gate = self._create_gates(inputs_reshape, memory)\n",
    "            next_memory = self._input_gate * tf.tanh(next_memory)\n",
    "            next_memory += self._forget_gate * memory\n",
    "\n",
    "        output = tf.reshape(next_memory, [batch_size, -1])\n",
    "        return output, next_memory\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"Operator overload for calling.\n",
    "\n",
    "        This is the entry point when users connect a Module into the Graph. The\n",
    "        underlying _build method will have been wrapped in a Template by the\n",
    "        constructor, and we call this template with the provided inputs here.\n",
    "\n",
    "        Args:\n",
    "          *args: Arguments for underlying _build method.\n",
    "          **kwargs: Keyword arguments for underlying _build method.\n",
    "\n",
    "        Returns:\n",
    "          The result of the underlying _build method.\n",
    "        \"\"\"\n",
    "        outputs = self._template(*args, **kwargs)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    @property\n",
    "    def input_gate(self):\n",
    "        \"\"\"Returns the input gate Tensor.\"\"\"\n",
    "        return self._input_gate\n",
    "\n",
    "    @property\n",
    "    def forget_gate(self):\n",
    "        \"\"\"Returns the forget gate Tensor.\"\"\"\n",
    "        return self._forget_gate\n",
    "\n",
    "    @property\n",
    "    def rmc_params(self):\n",
    "        \"\"\"Returns the parameters in the RMC module\"\"\"\n",
    "        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self._name)\n",
    "\n",
    "    def set_rmc_params(self, ref_rmc_params):\n",
    "        \"\"\"Set parameters of the RMC module to be the same with those of the reference module\"\"\"\n",
    "        rmc_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self._name)\n",
    "        if len(rmc_params) != len(ref_rmc_params):\n",
    "            raise ValueError(\"the number of parameters in the two RMC modules does not match\")\n",
    "        for i in range(len(ref_rmc_params)):\n",
    "            rmc_params[i] = tf.identity(ref_rmc_params[i])\n",
    "\n",
    "    def update_rmc_params(self, ref_rmc_params, update_ratio):\n",
    "        \"\"\"Update parameters of the RMC module based on a reference module\"\"\"\n",
    "        rmc_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self._name)\n",
    "        if len(rmc_params) != len(ref_rmc_params):\n",
    "            raise ValueError(\"the number of parameters in the two RMC modules does not match\")\n",
    "        for i in range(len(ref_rmc_params)):\n",
    "            rmc_params[i] = update_ratio * rmc_params[i] + (1 - update_ratio) * tf.identity(ref_rmc_params[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AUeJ9X5PZaVn"
   },
   "source": [
    "## GENERATOR AND DISCRIMINATOR ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TwvlmG_YnCfj"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import tensor_array_ops, control_flow_ops\n",
    "\n",
    "# The generator network based on the Relational Memory\n",
    "def generator(x_real, temperature, vocab_size, batch_size, seq_len, gen_emb_dim, mem_slots, head_size, num_heads,\n",
    "              hidden_dim, start_token):\n",
    "    start_tokens = tf.constant([start_token] * batch_size, dtype=tf.int32)\n",
    "    output_size = mem_slots * head_size * num_heads\n",
    "\n",
    "    # build relation memory module\n",
    "    g_embeddings = tf.get_variable('g_emb', shape=[vocab_size, gen_emb_dim],\n",
    "                                   initializer=create_linear_initializer(vocab_size))\n",
    "    gen_mem = RelationalMemory(mem_slots=mem_slots, head_size=head_size, num_heads=num_heads)\n",
    "    g_output_unit = create_output_unit(output_size, vocab_size)\n",
    "\n",
    "    # initial states\n",
    "    init_states = gen_mem.initial_state(batch_size)\n",
    "\n",
    "    # ---------- generate tokens and approximated one-hot results (Adversarial) ---------\n",
    "    gen_o = tensor_array_ops.TensorArray(dtype=tf.float32, size=seq_len, dynamic_size=False, infer_shape=True)\n",
    "    gen_x = tensor_array_ops.TensorArray(dtype=tf.int32, size=seq_len, dynamic_size=False, infer_shape=True)\n",
    "    gen_x_onehot_adv = tensor_array_ops.TensorArray(dtype=tf.float32, size=seq_len, dynamic_size=False,\n",
    "                                                    infer_shape=True)  # generator output (relaxed of gen_x)\n",
    "\n",
    "    # the generator recurrent module used for adversarial training\n",
    "    def _gen_recurrence(i, x_t, h_tm1, gen_o, gen_x, gen_x_onehot_adv):\n",
    "        mem_o_t, h_t = gen_mem(x_t, h_tm1)  # hidden_memory_tuple\n",
    "        o_t = g_output_unit(mem_o_t)  # batch x vocab, logits not probs\n",
    "        gumbel_t = add_gumbel(o_t)\n",
    "        next_token = tf.stop_gradient(tf.argmax(gumbel_t, axis=1, output_type=tf.int32))\n",
    "        next_token_onehot = tf.one_hot(next_token, vocab_size, 1.0, 0.0)\n",
    "\n",
    "        x_onehot_appr = tf.nn.softmax(tf.multiply(gumbel_t, temperature))  # one-hot-like, [batch_size x vocab_size]\n",
    "\n",
    "        # x_tp1 = tf.matmul(x_onehot_appr, g_embeddings)  # approximated embeddings, [batch_size x emb_dim]\n",
    "        x_tp1 = tf.nn.embedding_lookup(g_embeddings, next_token)  # embeddings, [batch_size x emb_dim]\n",
    "\n",
    "        gen_o = gen_o.write(i, tf.reduce_sum(tf.multiply(next_token_onehot, x_onehot_appr), 1))  # [batch_size], prob\n",
    "        gen_x = gen_x.write(i, next_token)  # indices, [batch_size]\n",
    "\n",
    "        gen_x_onehot_adv = gen_x_onehot_adv.write(i, x_onehot_appr)\n",
    "\n",
    "        return i + 1, x_tp1, h_t, gen_o, gen_x, gen_x_onehot_adv\n",
    "\n",
    "    # build a graph for outputting sequential tokens\n",
    "    _, _, _, gen_o, gen_x, gen_x_onehot_adv = control_flow_ops.while_loop(\n",
    "        cond=lambda i, _1, _2, _3, _4, _5: i < seq_len,\n",
    "        body=_gen_recurrence,\n",
    "        loop_vars=(tf.constant(0, dtype=tf.int32), tf.nn.embedding_lookup(g_embeddings, start_tokens),\n",
    "                   init_states, gen_o, gen_x, gen_x_onehot_adv))\n",
    "\n",
    "    gen_o = tf.transpose(gen_o.stack(), perm=[1, 0])  # batch_size x seq_len\n",
    "    gen_x = tf.transpose(gen_x.stack(), perm=[1, 0])  # batch_size x seq_len\n",
    "\n",
    "    gen_x_onehot_adv = tf.transpose(gen_x_onehot_adv.stack(), perm=[1, 0, 2])  # batch_size x seq_len x vocab_size\n",
    "\n",
    "    # ----------- pre-training for generator -----------------\n",
    "    x_emb = tf.transpose(tf.nn.embedding_lookup(g_embeddings, x_real), perm=[1, 0, 2])  # seq_len x batch_size x emb_dim\n",
    "    g_predictions = tensor_array_ops.TensorArray(dtype=tf.float32, size=seq_len, dynamic_size=False, infer_shape=True)\n",
    "\n",
    "    ta_emb_x = tensor_array_ops.TensorArray(dtype=tf.float32, size=seq_len)\n",
    "    ta_emb_x = ta_emb_x.unstack(x_emb)\n",
    "\n",
    "    # the generator recurrent moddule used for pre-training\n",
    "    def _pretrain_recurrence(i, x_t, h_tm1, g_predictions):\n",
    "        mem_o_t, h_t = gen_mem(x_t, h_tm1)\n",
    "        o_t = g_output_unit(mem_o_t)\n",
    "        g_predictions = g_predictions.write(i, tf.nn.softmax(o_t))  # batch_size x vocab_size\n",
    "        x_tp1 = ta_emb_x.read(i)\n",
    "        return i + 1, x_tp1, h_t, g_predictions\n",
    "\n",
    "    # build a graph for outputting sequential tokens\n",
    "    _, _, _, g_predictions = control_flow_ops.while_loop(\n",
    "        cond=lambda i, _1, _2, _3: i < seq_len,\n",
    "        body=_pretrain_recurrence,\n",
    "        loop_vars=(tf.constant(0, dtype=tf.int32), tf.nn.embedding_lookup(g_embeddings, start_tokens),\n",
    "                   init_states, g_predictions))\n",
    "\n",
    "    g_predictions = tf.transpose(g_predictions.stack(),\n",
    "                                 perm=[1, 0, 2])  # batch_size x seq_length x vocab_size\n",
    "\n",
    "    # pre-training loss\n",
    "    pretrain_loss = -tf.reduce_sum(\n",
    "        tf.one_hot(tf.to_int32(tf.reshape(x_real, [-1])), vocab_size, 1.0, 0.0) * tf.log(\n",
    "            tf.clip_by_value(tf.reshape(g_predictions, [-1, vocab_size]), 1e-20, 1.0)\n",
    "        )\n",
    "    ) / (seq_len * batch_size)\n",
    "\n",
    "    return gen_x_onehot_adv, gen_x, pretrain_loss, gen_o\n",
    "\n",
    "\n",
    "# The discriminator network based on the CNN classifier\n",
    "def discriminator(x_onehot, batch_size, seq_len, vocab_size, dis_emb_dim, num_rep, sn):\n",
    "    # get the embedding dimension for each presentation\n",
    "    emb_dim_single = int(dis_emb_dim / num_rep)\n",
    "    assert isinstance(emb_dim_single, int) and emb_dim_single > 0\n",
    "\n",
    "    filter_sizes = [2, 3, 4, 5]\n",
    "    num_filters = [300, 300, 300, 300]\n",
    "    dropout_keep_prob = 0.75\n",
    "\n",
    "    d_embeddings = tf.get_variable('d_emb', shape=[vocab_size, dis_emb_dim],\n",
    "                                   initializer=create_linear_initializer(vocab_size))\n",
    "    input_x_re = tf.reshape(x_onehot, [-1, vocab_size])\n",
    "    emb_x_re = tf.matmul(input_x_re, d_embeddings)\n",
    "    emb_x = tf.reshape(emb_x_re, [batch_size, seq_len, dis_emb_dim])  # batch_size x seq_len x dis_emb_dim\n",
    "\n",
    "    emb_x_expanded = tf.expand_dims(emb_x, -1)  # batch_size x seq_len x dis_emb_dim x 1\n",
    "    print('shape of emb_x_expanded: {}'.format(emb_x_expanded.get_shape().as_list()))\n",
    "\n",
    "    # Create a convolution + maxpool layer for each filter size\n",
    "    pooled_outputs = []\n",
    "    for filter_size, num_filter in zip(filter_sizes, num_filters):\n",
    "        conv = conv2d(emb_x_expanded, num_filter, k_h=filter_size, k_w=emb_dim_single,\n",
    "                      d_h=1, d_w=emb_dim_single, sn=sn, stddev=None, padding='VALID',\n",
    "                      scope=\"conv-%s\" % filter_size)  # batch_size x (seq_len-k_h+1) x num_rep x num_filter\n",
    "        out = tf.nn.relu(conv, name=\"relu\")\n",
    "        pooled = tf.nn.max_pool(out, ksize=[1, seq_len - filter_size + 1, 1, 1],\n",
    "                                strides=[1, 1, 1, 1], padding='VALID',\n",
    "                                name=\"pool\")  # batch_size x 1 x num_rep x num_filter\n",
    "        pooled_outputs.append(pooled)\n",
    "\n",
    "    # Combine all the pooled features\n",
    "    num_filters_total = sum(num_filters)\n",
    "    h_pool = tf.concat(pooled_outputs, 3)  # batch_size x 1 x num_rep x num_filters_total\n",
    "    print('shape of h_pool: {}'.format(h_pool.get_shape().as_list()))\n",
    "    h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    "\n",
    "    # Add highway\n",
    "    h_highway = highway(h_pool_flat, h_pool_flat.get_shape()[1], 1, 0)  # (batch_size*num_rep) x num_filters_total\n",
    "\n",
    "    # Add dropout\n",
    "    h_drop = tf.nn.dropout(h_highway, dropout_keep_prob, name='dropout')\n",
    "\n",
    "    # fc\n",
    "    fc_out = linear(h_drop, output_size=100, use_bias=True, sn=sn, scope='fc')\n",
    "    logits = linear(fc_out, output_size=1, use_bias=True, sn=sn, scope='logits')\n",
    "    logits = tf.squeeze(logits, -1)  # batch_size*num_rep\n",
    "\n",
    "    return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4A3TvyVDnUgc"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "generator_dict = {\n",
    "    'rmc_vanilla': generator,\n",
    "}\n",
    "\n",
    "discriminator_dict = {\n",
    "    'rmc_vanilla': discriminator,\n",
    "}\n",
    "\n",
    "\n",
    "def get_generator(model_name, scope='generator', **kwargs):\n",
    "    model_func = generator_dict[model_name]\n",
    "    return tf.make_template(scope, model_func, **kwargs)\n",
    "\n",
    "\n",
    "def get_discriminator(model_name, scope='discriminator', **kwargs):\n",
    "    model_func = discriminator_dict[model_name]\n",
    "    return tf.make_template(scope, model_func, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G0qIZU-CZqQ8"
   },
   "source": [
    "## RUN ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "pX9zZYBNXNo7",
    "outputId": "bd156b92-696d-40c6-fa0c-3e00b89bccca"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Train and run a RmcGAN')\n",
    "# Architecture\n",
    "parser.add_argument('--gf-dim', default=64, type=int, help='Number of filters to use for generator')\n",
    "parser.add_argument('--df-dim', default=64, type=int, help='Number of filters to use for discriminator')\n",
    "parser.add_argument('--g-architecture', default='rmc_att', type=str, help='Architecture for generator')\n",
    "parser.add_argument('--d-architecture', default='rmc_att', type=str, help='Architecture for discriminator')\n",
    "parser.add_argument('--gan-type', default='standard', type=str, help='Which type of GAN to use')\n",
    "parser.add_argument('--hidden-dim', default=32, type=int, help='only used for OrcaleLstm and lstm_vanilla (generator)')\n",
    "parser.add_argument('--sn', default=False, action='store_true', help='if using spectral norm')\n",
    "\n",
    "# Training\n",
    "parser.add_argument('--gsteps', default='1', type=int, help='How many training steps to use for generator')\n",
    "parser.add_argument('--dsteps', default='5', type=int, help='How many training steps to use for discriminator')\n",
    "parser.add_argument('--npre-epochs', default=150, type=int, help='Number of steps to run pre-training')\n",
    "parser.add_argument('--nadv-steps', default=5000, type=int, help='Number of steps to run adversarial training')\n",
    "parser.add_argument('--ntest', default=50, type=int, help='How often to run tests')\n",
    "parser.add_argument('--d-lr', default=1e-4, type=float, help='Learning rate for the discriminator')\n",
    "parser.add_argument('--gpre-lr', default=1e-2, type=float, help='Learning rate for the generator in pre-training')\n",
    "parser.add_argument('--gadv-lr', default=1e-4, type=float, help='Learning rate for the generator in adv-training')\n",
    "parser.add_argument('--batch-size', default=64, type=int, help='Batch size for training')\n",
    "parser.add_argument('--log-dir', default='./oracle/logs', type=str, help='Where to store log and checkpoint files')\n",
    "parser.add_argument('--sample-dir', default='./oracle/samples', type=str, help='Where to put samples during training')\n",
    "parser.add_argument('--optimizer', default='adam', type=str, help='training method')\n",
    "parser.add_argument('--decay', default=False, action='store_true', help='if decaying learning rate')\n",
    "parser.add_argument('--adapt', default='exp', type=str, help='temperature control policy: [no, lin, exp, log, sigmoid, quad, sqrt]')\n",
    "parser.add_argument('--seed', default=123, type=int, help='for reproducing the results')\n",
    "parser.add_argument('--temperature', default=1000, type=float, help='the largest temperature')\n",
    "\n",
    "# evaluation\n",
    "parser.add_argument('--nll-oracle', default=False, action='store_true', help='if using nll-oracle metric')\n",
    "parser.add_argument('--nll-gen', default=False, action='store_true', help='if using nll-gen metric')\n",
    "parser.add_argument('--bleu', default=False, action='store_true', help='if using bleu metric, [2,3,4,5]')\n",
    "parser.add_argument('--selfbleu', default=False, action='store_true', help='if using selfbleu metric, [2,3,4,5]')\n",
    "parser.add_argument('--doc-embsim', default=False, action='store_true', help='if using DocEmbSim metric')\n",
    "\n",
    "# relational memory\n",
    "parser.add_argument('--mem-slots', default=1, type=int, help=\"memory size\")\n",
    "parser.add_argument('--head-size', default=512, type=int, help=\"head size or memory size\")\n",
    "parser.add_argument('--num-heads', default=2, type=int, help=\"number of heads\")\n",
    "\n",
    "# Data\n",
    "parser.add_argument('--dataset', default='oracle', type=str, help='[oracle, image_coco, emnlp_news]')\n",
    "parser.add_argument('--vocab-size', default=5000, type=int, help=\"vocabulary size\")\n",
    "parser.add_argument('--start-token', default=0, type=int, help=\"start token for a sentence\")\n",
    "parser.add_argument('--seq-len', default=20, type=int, help=\"sequence length: [20, 40]\")\n",
    "parser.add_argument('--num-sentences', default=10000, type=int, help=\"number of total sentences\")\n",
    "parser.add_argument('--gen-emb-dim', default=32, type=int, help=\"generator embedding dimension\")\n",
    "parser.add_argument('--dis-emb-dim', default=64, type=int, help=\"TOTAL discriminator embedding dimension\")\n",
    "parser.add_argument('--num-rep', default=64, type=int, help=\"number of discriminator embedded representations\")\n",
    "parser.add_argument('--data-dir', default='./data', type=str, help='Where data data is stored')\n",
    "\n",
    "\n",
    "def main():\n",
    "    config = dict()\n",
    "    #args = parser.parse_args()\n",
    "    #pp.pprint(vars(args))\n",
    "    \n",
    "    config['gf_dim'] = 64 \n",
    "    config['df_dim'] = 64\n",
    "    config['g_architecture'] = 'rmc_vanilla'\n",
    "    config['d_architecture'] = 'rmc_vanilla'\n",
    "    config['gan_type'] = 'RSGAN'\n",
    "    config['hidden_dim'] = 32\n",
    "    config['sn'] = False\n",
    "    \n",
    "    config['gsteps'] = 1\n",
    "    config['dsteps'] = 1\n",
    "    config['npre_epochs'] = 30\n",
    "    config['nadv_steps'] = 5000\n",
    "    config['ntest'] = 25\n",
    "    config['d_lr'] = 1e-4\n",
    "    config['gpre_lr'] = 1e-2\n",
    "    config['gadv_lr'] = 1e-4\n",
    "    config['batch_size'] = 32\n",
    "    config['log_dir'] = '/content/drive/My Drive/oracle/logs'\n",
    "    config['sample_dir'] = '/content/drive/My Drive/oracle/samples'\n",
    "    config['optimizer'] = 'adam'\n",
    "    config['decay'] = False\n",
    "    config['adapt'] = 'exp'\n",
    "    config['seed'] = 171\n",
    "    config['temperature'] = 100\n",
    "    config['nll_oracle']= False\n",
    "    config['nll_gen'] = True\n",
    "    config['bleu'] = True\n",
    "    config['selfbleu'] = False\n",
    "    config['doc_embsim'] = False\n",
    "    \n",
    "    config['mem_slots'] = 1\n",
    "    config['head_size'] = 256\n",
    "    config['num_heads'] = 2 \n",
    "    \n",
    "    config['dataset'] = 'fce_train_new'\n",
    "    config['vocab_size'] = 5000\n",
    "    config['start_token'] = 0\n",
    "    config['seq_len'] = 20\n",
    "    config['num_sentences'] = 300\n",
    "    config['num_rep'] = 64\n",
    "    config['data_dir'] = '/content/drive/My Drive/data'\n",
    "    \n",
    "    config['gen_emb_dim'] = 32\n",
    "    config['dis_emb_dim'] = 64\n",
    "   \n",
    "    config['checkpoint_restore'] = False \n",
    "    config['checkpoint'] = '/content/drive/My Drive/model_saver/1525.ckpt'\n",
    "    config['check_meta'] = '/content/drive/My Drive/model_saver/1525.ckpt'\n",
    "\n",
    "    \n",
    "    # train with different datasets\n",
    "    if config['dataset'] == 'oracle':\n",
    "        oracle_model = OracleLstm(num_vocabulary= config['vocab_size'], batch_size=config['batch_size'], emb_dim=config['gen_emb_dim'],\n",
    "                                  hidden_dim=config['hidden_dim'], sequence_length=config['seq_len'],\n",
    "                                  start_token=config['start_token'])\n",
    "        oracle_loader = OracleDataLoader(config['batch_size'],config['seq_len'])\n",
    "        gen_loader = OracleDataLoader(config['batch_size'],config['seq_len'])\n",
    "\n",
    "        generator = get_generator(config['g_architecture'], vocab_size=config['vocab_size'], batch_size=config['batch_size'],\n",
    "                                         seq_len=config['seq_len'], gen_emb_dim=config['gen_emb_dim'], mem_slots=config['mem_slots'],\n",
    "                                         head_size=config['head_size'], num_heads=config['num_heads'], hidden_dim=config['hidden_dim'],\n",
    "                                         start_token=config['start_token'])\n",
    "        discriminator = get_discriminator(config['d_architecture'], batch_size= config['batch_size'], seq_len= config['seq_len'],\n",
    "                                                 vocab_size= config['vocab_size'], dis_emb_dim=config['dis_emb_dim'],\n",
    "                                                 num_rep= config['num_rep'], sn= config['sn'])\n",
    "        oracle_train(generator, discriminator, oracle_model, oracle_loader, gen_loader, config)\n",
    "\n",
    "    elif config['dataset'] in ['image_coco', 'emnlp_news', 'fce_train_new']:\n",
    "        data_file = os.path.join(config['data_dir'], '{}.txt'.format(config['dataset']))\n",
    "        seq_len, vocab_size = text_precess(data_file)\n",
    "        config['seq_len'] = seq_len\n",
    "        config['vocab_size'] = vocab_size\n",
    "        print('seq_len: %d, vocab_size: %d' % (seq_len, vocab_size))\n",
    "\n",
    "        oracle_loader = RealDataLoader(config['batch_size'], config['seq_len'])\n",
    "\n",
    "        generator = get_generator(config['g_architecture'], vocab_size=vocab_size, batch_size=config['batch_size'],\n",
    "                                         seq_len=seq_len, gen_emb_dim=config['gen_emb_dim'], mem_slots=config['mem_slots'],\n",
    "                                         head_size=config['head_size'], num_heads=config['num_heads'], hidden_dim=config['hidden_dim'],\n",
    "                                         start_token=config['start_token'])\n",
    "        discriminator = get_discriminator(config['d_architecture'], batch_size=config['batch_size'], seq_len=seq_len,\n",
    "                                                 vocab_size=vocab_size, dis_emb_dim=config['dis_emb_dim'],\n",
    "                                                 num_rep=config['num_rep'], sn=config['sn'])\n",
    "        real_train(generator, discriminator, oracle_loader, config)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError('{}: unknown dataset!'.format(config['dataset']))\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "i-H8tjVrYrBl",
    "0kPRKI3HbSYQ",
    "51eu3udtYyDG",
    "W0QAe458Y77h",
    "nzXEL8UyZehU",
    "4pr_JO9MZVo3"
   ],
   "name": "Copy of RelGan.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
