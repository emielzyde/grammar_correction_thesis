{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VQG02iODtURA"
   },
   "source": [
    "This notebook contains code from Hareesh Bahuleyan's 'probabilistic_nlg' repository (https://github.com/HareeshBahuleyan/probabilistic_nlg) and runs the standard deterministic WAE. It was designed specifically to be run in the Colaboratory environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uo8G6iRvtmyP"
   },
   "source": [
    "### SET-UP ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "ZQpXboO9dMvJ",
    "outputId": "646603d6-670b-4861-ae9f-a04b851b1d27"
   },
   "outputs": [],
   "source": [
    "#I will attach my Drive so that I can easily load files.\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\", force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "colab_type": "code",
    "id": "YyqvMwXpdmo6",
    "outputId": "244ee1b7-47f1-490e-cbcf-f5b827521256"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tv-2HFerdcXx"
   },
   "source": [
    "### UTILS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "VjhVWGioddoU",
    "outputId": "96e1daf6-6ad5-4dcd-b242-30bc10ca622a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def calculate_bleu_scores(references, hypotheses):\n",
    "    \"\"\"\n",
    "    Calculates BLEU 1-4 scores based on NLTK functionality\n",
    "    Args:\n",
    "        references: List of reference sentences\n",
    "        hypotheses: List of generated sentences\n",
    "    Returns:\n",
    "        bleu_1, bleu_2, bleu_3, bleu_4: BLEU scores\n",
    "    \"\"\"\n",
    "    bleu_1 = np.round(100 * corpus_bleu(references, hypotheses, weights=(1.0, 0., 0., 0.)), decimals=2)\n",
    "    bleu_2 = np.round(100 * corpus_bleu(references, hypotheses, weights=(0.50, 0.50, 0., 0.)), decimals=2)\n",
    "    bleu_3 = np.round(100 * corpus_bleu(references, hypotheses, weights=(0.34, 0.33, 0.33, 0.)), decimals=2)\n",
    "    bleu_4 = np.round(100 * corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25)), decimals=2)\n",
    "    return bleu_1, bleu_2, bleu_3, bleu_4\n",
    "\n",
    "\n",
    "def calculate_ngram_diversity(corpus):\n",
    "    \"\"\"\n",
    "    Calculates unigram and bigram diversity\n",
    "    Args:\n",
    "        corpus: tokenized list of sentences sampled\n",
    "    Returns:\n",
    "        uni_diversity: distinct-1 score\n",
    "        bi_diversity: distinct-2 score\n",
    "    \"\"\"\n",
    "    bigram_finder = BigramCollocationFinder.from_words(corpus)\n",
    "    bi_diversity = len(bigram_finder.ngram_fd) / bigram_finder.N\n",
    "\n",
    "    dist = FreqDist(corpus)\n",
    "    uni_diversity = len(dist) / len(corpus)\n",
    "\n",
    "    return uni_diversity, bi_diversity\n",
    "\n",
    "\n",
    "def calculate_entropy(corpus):\n",
    "    \"\"\"\n",
    "    Calculates diversity in terms of entropy (using unigram probability)\n",
    "    Args:\n",
    "        corpus: tokenized list of sentences sampled\n",
    "    Returns:\n",
    "        ent: entropy on the sample sentence list\n",
    "    \"\"\"\n",
    "    fdist = FreqDist(corpus)\n",
    "    total_len = len(corpus)\n",
    "    ent = 0\n",
    "    for k, v in fdist.items():\n",
    "        p = v / total_len\n",
    "\n",
    "        ent += -p * np.log(p)\n",
    "\n",
    "    return ent\n",
    "\n",
    "\n",
    "def tokenize_sequence(sentences, filters, max_num_words, max_vocab_size):\n",
    "    \"\"\"\n",
    "    Tokenizes a given input sequence of words.\n",
    "    Args:\n",
    "        sentences: List of sentences\n",
    "        filters: List of filters/punctuations to omit (for Keras tokenizer)\n",
    "        max_num_words: Number of words to be considered in the fixed length sequence\n",
    "        max_vocab_size: Number of most frequently occurring words to be kept in the vocabulary\n",
    "    Returns:\n",
    "        x : List of padded/truncated indices created from list of sentences\n",
    "        word_index: dictionary storing the word-to-index correspondence\n",
    "    \"\"\"\n",
    "\n",
    "    sentences = [' '.join(word_tokenize(s)[:max_num_words]) for s in sentences]\n",
    "\n",
    "    tokenizer = Tokenizer(filters=filters)\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "    word_index = dict()\n",
    "    word_index['PAD'] = 0\n",
    "    word_index['UNK'] = 1\n",
    "    word_index['GO'] = 2\n",
    "    word_index['EOS'] = 3\n",
    "\n",
    "    for i, word in enumerate(dict(tokenizer.word_index).keys()):\n",
    "        word_index[word] = i + 4\n",
    "\n",
    "    tokenizer.word_index = word_index\n",
    "    x = tokenizer.texts_to_sequences(list(sentences))\n",
    "\n",
    "    for i, seq in enumerate(x):\n",
    "        if any(t >= max_vocab_size for t in seq):\n",
    "            seq = [t if t < max_vocab_size else word_index['UNK'] for t in seq]\n",
    "        seq.append(word_index['EOS'])\n",
    "        x[i] = seq\n",
    "\n",
    "    x = pad_sequences(x, padding='post', truncating='post', maxlen=max_num_words, value=word_index['PAD'])\n",
    "\n",
    "    word_index = {k: v for k, v in word_index.items() if v < max_vocab_size}\n",
    "\n",
    "    return x, word_index\n",
    "\n",
    "\n",
    "def create_embedding_matrix(word_index, embedding_dim, w2v_path):\n",
    "    \"\"\"\n",
    "    Create the initial embedding matrix for TF Graph.\n",
    "    Args:\n",
    "        word_index: dictionary storing the word-to-index correspondence\n",
    "        embedding_dim: word2vec dimension\n",
    "        w2v_path: file path to the w2v pickle file\n",
    "    Returns:\n",
    "        embeddings_matrix : numpy 2d-array with word vectors\n",
    "    \"\"\"\n",
    "    w2v_model = gensim.models.Word2Vec.load(w2v_path)\n",
    "    embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(word_index), embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embeddings_vector = w2v_model[word]\n",
    "            embeddings_matrix[i] = embeddings_vector\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    return embeddings_matrix\n",
    "\n",
    "\n",
    "def get_sentences(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def clean_sentence(sent):\n",
    "    sent = re.sub(r'[^\\w\\s\\?\\.\\,]', '', sent.strip().lower())  # Lower case, remove punctuations (except , ? .)\n",
    "    sent = re.sub(r'(([a-z]*)\\d+.?\\d*\\%?)', ' NUM ', sent.strip())  # Replace Numbers with <NUM> token\n",
    "    return sent\n",
    "\n",
    "\n",
    "def get_batches(x, batch_size):\n",
    "    \"\"\"\n",
    "    Generate inputs and targets in a batch-wise fashion for feed-dict\n",
    "    Args:\n",
    "        x: entire source sequence array\n",
    "        batch_size: batch size\n",
    "    Returns:\n",
    "        x_batch, y_batch, sentence_length\n",
    "    \"\"\"\n",
    "\n",
    "    for batch_i in range(0, len(x) // batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        x_batch = x[start_i:start_i + batch_size]\n",
    "        y_batch = x[start_i:start_i + batch_size]\n",
    "\n",
    "        sentence_length = [np.count_nonzero(seq) for seq in x_batch]\n",
    "\n",
    "        yield x_batch, y_batch, sentence_length\n",
    "\n",
    "\n",
    "def get_batches_xy(x, y, batch_size):\n",
    "    \"\"\"\n",
    "    Generate inputs and targets in a batch-wise fashion for feed-dict\n",
    "    Args:\n",
    "        x: entire source sequence array\n",
    "        y: entire output sequence array\n",
    "        batch_size: batch size\n",
    "    Returns:\n",
    "        x_batch, y_batch, source_sentence_length, target_sentence_length\n",
    "    \"\"\"\n",
    "\n",
    "    for batch_i in range(0, len(x) // batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        x_batch = x[start_i:start_i + batch_size]\n",
    "        y_batch = y[start_i:start_i + batch_size]\n",
    "\n",
    "        source_sentence_length = [np.count_nonzero(seq) for seq in x_batch]\n",
    "        target_sentence_length = [np.count_nonzero(seq) for seq in y_batch]\n",
    "\n",
    "        yield x_batch, y_batch, source_sentence_length, target_sentence_length\n",
    "\n",
    "\n",
    "def create_data_split(x, y, dataset_sizes):\n",
    "    \"\"\"\n",
    "    Create test-train split according to previously defined CSV files\n",
    "    Depending on the experiment - qgen or dialogue\n",
    "    Args:\n",
    "        x: input sequence of indices\n",
    "        y: output sequence of indices\n",
    "    Returns:\n",
    "        x_train, y_train, x_val, y_val, x_test, y_test: train val test split arrays\n",
    "    \"\"\"\n",
    "\n",
    "    train_size, val_size, test_size = dataset_sizes[0], dataset_sizes[1], dataset_sizes[2],\n",
    "\n",
    "    train_indices = range(train_size)\n",
    "    val_indices = range(train_size, train_size + val_size)\n",
    "    test_indices = range(train_size + val_size, train_size + val_size + test_size)\n",
    "\n",
    "    x_train = x[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    x_val = x[val_indices]\n",
    "    y_val = y[val_indices]\n",
    "    x_test = x[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "\n",
    "def plot_2d(zvectors, labels, method):\n",
    "    if method == 'tsne':\n",
    "        cluster = TSNE(n_components=2, random_state=17)\n",
    "    else:  # PCA\n",
    "        cluster = PCA(n_components=2, random_state=17)\n",
    "\n",
    "    cluster_result = cluster.fit_transform(X=zvectors)\n",
    "    labels = labels[:cluster_result.shape[0]]\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    class_dict = {0: 'automobile', 1: 'home and kitchen'}\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.figure.set_size_inches(w=10, h=10)\n",
    "    ax.scatter(cluster_result[np.where(labels == 0), 0], cluster_result[np.where(labels == 0), 1], s=6,\n",
    "               label=class_dict[0])\n",
    "    ax.scatter(cluster_result[np.where(labels == 1), 0], cluster_result[np.where(labels == 1), 1], s=6,\n",
    "               label=class_dict[1])\n",
    "    plt.grid()\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N4Kj5hGkdpko"
   },
   "source": [
    "### EMBEDDINGS ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eeakIke-dsNH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def main():\n",
    "\n",
    "    snli_data = get_sentences(file_path='/content/drive/My Drive/sources.txt')\n",
    "\n",
    "    print('[INFO] Number of sentences = {}'.format(len(snli_data)))\n",
    "\n",
    "    sentences = [s.strip() for s in snli_data]\n",
    "\n",
    "    np.random.shuffle(sentences)\n",
    "    sentences = [word_tokenize(s) for s in sentences]\n",
    "    w2v_model = gensim.models.Word2Vec(sentences,\n",
    "                                       size=300,\n",
    "                                       min_count=1,\n",
    "                                       iter=50)\n",
    "    if not os.path.exists('w2v_models'):\n",
    "        os.mkdir('w2v_models')\n",
    "\n",
    "    w2v_model.save('/content/drive/My Drive/w2v_models/fce_incorrect.pkl')\n",
    "    print('[INFO] Word embeddings pre-trained successfully')\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_wow7PGtd6ew"
   },
   "source": [
    "### DECODER ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2k-K90C9d8x8"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.seq2seq.python.ops import decoder\n",
    "from tensorflow.contrib.seq2seq.python.ops import helper as helper_py\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.layers import base as layers_base\n",
    "from tensorflow.python.ops import rnn_cell_impl\n",
    "from tensorflow.python.util import nest\n",
    "\n",
    "__all__ = [\n",
    "    \"BasicDecoderOutput\",\n",
    "    \"BasicDecoder\",\n",
    "]\n",
    "\n",
    "\n",
    "class BasicDecoderOutput(collections.namedtuple(\"BasicDecoderOutput\", (\"rnn_output\", \"sample_id\"))):\n",
    "    pass\n",
    "\n",
    "\n",
    "class BasicDecoder(decoder.Decoder):\n",
    "    \"\"\"Basic sampling decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, cell, helper, initial_state, latent_vector, output_layer=None):\n",
    "        \"\"\"Initialize BasicDecoder.\n",
    "        Args:\n",
    "          cell: An `RNNCell` instance.\n",
    "          helper: A `Helper` instance.\n",
    "          initial_state: A (possibly nested tuple of...) tensors and TensorArrays.\n",
    "            The initial state of the RNNCell.\n",
    "          output_layer: (Optional) An instance of `tf.layers.Layer`, i.e.,\n",
    "            `tf.layers.Dense`.  Optional layer to apply to the RNN output prior\n",
    "            to storing the result or sampling.\n",
    "        Raises:\n",
    "          TypeError: if `cell`, `helper` or `output_layer` have an incorrect type.\n",
    "        \"\"\"\n",
    "        #if not rnn_cell_impl._like_rnncell(cell):  # pylint: disable=protected-access\n",
    "        #    raise TypeError(\"cell must be an RNNCell, received: %s\" % type(cell))\n",
    "        if not isinstance(helper, helper_py.Helper):\n",
    "            raise TypeError(\"helper must be a Helper, received: %s\" % type(helper))\n",
    "        if (output_layer is not None and not isinstance(output_layer, layers_base.Layer)):\n",
    "            raise TypeError(\"output_layer must be a Layer, received: %s\" % type(output_layer))\n",
    "        self._cell = cell\n",
    "        self._helper = helper\n",
    "        self._initial_state = initial_state\n",
    "        self._output_layer = output_layer\n",
    "        self._latent_vector = latent_vector\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self._helper.batch_size\n",
    "\n",
    "    def _rnn_output_size(self):\n",
    "        size = self._cell.output_size\n",
    "        if self._output_layer is None:\n",
    "            return size\n",
    "        else:\n",
    "      # To use layer's compute_output_shape, we need to convert the\n",
    "      # RNNCell's output_size entries into shapes with an unknown\n",
    "      # batch size.  We then pass this through the layer's\n",
    "      # compute_output_shape and read off all but the first (batch)\n",
    "      # dimensions to get the output size of the rnn with the layer\n",
    "      # applied to the top.\n",
    "            output_shape_with_unknown_batch = nest.map_structure(\n",
    "            lambda s: tensor_shape.TensorShape([None]).concatenate(s),\n",
    "            size)\n",
    "            layer_output_shape = self._output_layer.compute_output_shape(  # pylint: disable=protected-access\n",
    "                    output_shape_with_unknown_batch)\n",
    "        return nest.map_structure(lambda s: s[1:], layer_output_shape)\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "    # Return the cell output and the id\n",
    "        return BasicDecoderOutput(\n",
    "            rnn_output=self._rnn_output_size(),\n",
    "            sample_id=tensor_shape.TensorShape([]))\n",
    "\n",
    "    @property\n",
    "    def output_dtype(self):\n",
    "        # Assume the dtype of the cell is the output_size structure\n",
    "        # containing the input_state's first component's dtype.\n",
    "        # Return that structure and int32 (the id)\n",
    "        dtype = nest.flatten(self._initial_state)[0].dtype\n",
    "        return BasicDecoderOutput(\n",
    "            nest.map_structure(lambda _: dtype, self._rnn_output_size()),\n",
    "            dtypes.int32)\n",
    "\n",
    "    def initialize(self, name=None):\n",
    "        \"\"\"Initialize the decoder.\n",
    "        Args:\n",
    "          name: Name scope for any created operations.\n",
    "        Returns:\n",
    "          `(finished, first_inputs, initial_state)`.\n",
    "        \"\"\"\n",
    "        # Concatenate the latent vector to the 1st input to the decoder LSTM, i.e, the <GO> embedding + latent vector\n",
    "        return (self._helper.initialize()[0], tf.concat([self._helper.initialize()[1], self._latent_vector], axis=-1)) + (self._initial_state,)\n",
    "\n",
    "    def step(self, time, inputs, state, name=None):\n",
    "        \"\"\"Perform a decoding step.\n",
    "        Args:\n",
    "          time: scalar `int32` tensor.\n",
    "          inputs: A (structure of) input tensors.\n",
    "          state: A (structure of) state tensors and TensorArrays.\n",
    "          name: Name scope for any created operations.\n",
    "        Returns:\n",
    "          `(outputs, next_state, next_inputs, finished)`.\n",
    "        \"\"\"\n",
    "        with ops.name_scope(name, \"BasicDecoderStep\", (time, inputs, state)):\n",
    "            cell_outputs, cell_state = self._cell(inputs, state)\n",
    "\n",
    "            if self._output_layer is not None:\n",
    "                cell_outputs = self._output_layer(cell_outputs)\n",
    "            sample_ids = self._helper.sample(\n",
    "                            time=time, outputs=cell_outputs, state=cell_state)\n",
    "            (finished, next_inputs, next_state) = self._helper.next_inputs(\n",
    "                                                  time=time,\n",
    "                                                  outputs=cell_outputs,\n",
    "                                                  state=cell_state,\n",
    "                                                  sample_ids=sample_ids)\n",
    "\n",
    "            # Concatenate the latent vector to the predicted word's embedding\n",
    "            next_inputs = tf.concat([next_inputs, self._latent_vector], axis=-1)\n",
    "\n",
    "        outputs = BasicDecoderOutput(cell_outputs, sample_ids)\n",
    "\n",
    "        return (outputs, next_state, next_inputs, finished)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oN6pYjDqeLGc"
   },
   "source": [
    "### MODEL ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h6HqQcxWeO9K"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "class DetWAEModel(object):\n",
    "\n",
    "    def __init__(self, config, embeddings_matrix, word_index):\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.lstm_hidden_units = config['lstm_hidden_units']\n",
    "        self.embedding_size = config['embedding_size']\n",
    "        self.latent_dim = config['latent_dim']\n",
    "        self.num_layers = config['num_layers']\n",
    "        \n",
    "        self.lambda_val = config['lambda_val']\n",
    "\n",
    "        self.vocab_size = config['vocab_size']\n",
    "        self.num_tokens = config['num_tokens']\n",
    "\n",
    "        self.dropout_keep_prob = config['dropout_keep_prob']\n",
    "\n",
    "        self.initial_learning_rate = config['initial_learning_rate']\n",
    "        self.learning_rate_decay = config['learning_rate_decay']\n",
    "        self.min_learning_rate = config['min_learning_rate']\n",
    "\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.epochs = config['n_epochs']\n",
    "\n",
    "        self.embeddings_matrix = embeddings_matrix\n",
    "        self.word_index = word_index\n",
    "        self.idx_word = dict((i, word) for word, i in word_index.items())\n",
    "\n",
    "        self.logs_dir = config['logs_dir']\n",
    "        self.model_checkpoint_dir = config['model_checkpoint_dir']\n",
    "        self.bleu_path = config['bleu_path']\n",
    "\n",
    "        self.pad = self.word_index['PAD']\n",
    "        self.eos = self.word_index['EOS']\n",
    "        self.unk = self.word_index['UNK']\n",
    "        \n",
    "        self.epoch_bleu_score_val = {'1': [], '2': [], '3': [], '4': []}\n",
    "        self.log_str = []\n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        print(\"[INFO] Building Model ...\")\n",
    "\n",
    "        self.init_placeholders()\n",
    "        self.embedding_layer()\n",
    "        self.build_encoder()\n",
    "        self.build_latent_space()\n",
    "        self.sample_gaussian()\n",
    "        self.build_decoder()\n",
    "        self.loss()\n",
    "        self.optimize()\n",
    "        self.summary()\n",
    "\n",
    "    def init_placeholders(self):\n",
    "        with tf.name_scope(\"model_inputs\"):\n",
    "            # Create palceholders for inputs to the model\n",
    "            self.input_data = tf.placeholder(tf.int32, [self.batch_size, self.num_tokens], name='input') # batch x maxlen\n",
    "            self.target_data = tf.placeholder(tf.int32, [self.batch_size, self.num_tokens], name='targets') # batch x maxlen\n",
    "            self.lr = tf.placeholder(tf.float32, name='learning_rate', shape=())\n",
    "            self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')  # Dropout Keep Probability\n",
    "            self.source_sentence_length = tf.placeholder(tf.int32, shape=(self.batch_size,),\n",
    "                                                         name='source_sentence_length') # batch\n",
    "            self.target_sentence_length = tf.placeholder(tf.int32, shape=(self.batch_size,),\n",
    "                                                         name='target_sentence_length') # batch\n",
    "            self.lambda_coeff = tf.placeholder(tf.float32, name='lambda_coeff', shape=())\n",
    "\n",
    "    def embedding_layer(self):\n",
    "        with tf.name_scope(\"word_embeddings\"):\n",
    "            self.embeddings = tf.Variable(\n",
    "                initial_value=np.array(self.embeddings_matrix, dtype=np.float32),\n",
    "                dtype=tf.float32, trainable=False)\n",
    "            self.enc_embed_input = tf.nn.embedding_lookup(self.embeddings, self.input_data) # batch x maxlen x embed_dim\n",
    "            self.enc_embed_input = self.enc_embed_input[:, :tf.reduce_max(self.source_sentence_length), :]\n",
    "\n",
    "            with tf.name_scope(\"decoder_inputs\"):\n",
    "                # shifted = tf.strided_slice(self.target_data, [0, 0], [self.batch_size, -1], [1, 1],\n",
    "                #                          name='slice_input')  # Minus 1 implies everything till the last dim\n",
    "                shifted = self.target_data[:,:-1] # batch x (maxlen - 1)\n",
    "                self.dec_input = tf.concat([tf.fill([self.batch_size, 1], self.word_index['GO']), shifted], 1,\n",
    "                                           name='dec_input') # batch x maxlen\n",
    "                self.dec_embed_input = tf.nn.embedding_lookup(self.embeddings, self.dec_input)\n",
    "                self.max_tar_len = tf.reduce_max(self.target_sentence_length)\n",
    "                self.dec_embed_input = self.dec_embed_input[:, :self.max_tar_len, :] # batch x maxlen x embed_dim\n",
    "                # self.dec_embed_input = tf.nn.dropout(self.dec_embed_input, keep_prob=self.keep_prob)\n",
    "\n",
    "    def build_encoder(self):\n",
    "        with tf.name_scope(\"encode\"):\n",
    "            for layer in range(self.num_layers):\n",
    "                with tf.variable_scope('encoder_{}'.format(layer + 1)):\n",
    "                    cell_fw = tf.contrib.rnn.LayerNormBasicLSTMCell(self.lstm_hidden_units)\n",
    "                    cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, input_keep_prob=self.keep_prob)\n",
    "\n",
    "                    cell_bw = tf.contrib.rnn.LayerNormBasicLSTMCell(self.lstm_hidden_units)\n",
    "                    cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, input_keep_prob=self.keep_prob)\n",
    "\n",
    "                    self.enc_output, self.enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw,\n",
    "                                                                                      cell_bw,\n",
    "                                                                                      self.enc_embed_input,\n",
    "                                                                                      self.source_sentence_length,\n",
    "                                                                                      dtype=tf.float32)\n",
    "\n",
    "            # Join outputs since we are using a bidirectional RNN\n",
    "            self.h_N = tf.concat([self.enc_state[0][1], self.enc_state[1][1]], axis=-1,\n",
    "                                 name='h_N')  # Concatenated h from the fw and bw LSTMs\n",
    "            self.enc_outputs = tf.concat([self.enc_output[0], self.enc_output[1]], axis=-1, name='encoder_outputs')\n",
    "\n",
    "    def build_latent_space(self):\n",
    "        with tf.name_scope(\"latent_space\"):\n",
    "            self.z_tilda = Dense(self.latent_dim, name='z_tilda')(self.h_N) # [batch_size x latent_dim]\n",
    "\n",
    "    def sample_gaussian(self):\n",
    "        with tf.name_scope('sample_gaussian'):\n",
    "            # Random sample from Gaussian prior\n",
    "            self.z_sampled = tf.random_normal([self.batch_size, self.latent_dim], name='z_sampled') # Dimension [batch_size x latent_dim]\n",
    "\n",
    "    def build_decoder(self):\n",
    "        with tf.variable_scope(\"decode\"):\n",
    "            for layer in range(self.num_layers):\n",
    "                with tf.variable_scope('decoder_{}'.format(layer + 1)):\n",
    "                    dec_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(2 * self.lstm_hidden_units)\n",
    "                    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, input_keep_prob=self.keep_prob)\n",
    "\n",
    "            self.output_layer = Dense(self.vocab_size)\n",
    "\n",
    "            self.init_state = dec_cell.zero_state(self.batch_size, tf.float32)\n",
    "\n",
    "            with tf.name_scope(\"training_decoder\"):\n",
    "                training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=self.dec_embed_input,\n",
    "                                                                    sequence_length=self.target_sentence_length,\n",
    "                                                                    time_major=False)\n",
    "\n",
    "                training_decoder = BasicDecoder(dec_cell,\n",
    "                                                              training_helper,\n",
    "                                                              initial_state=self.init_state,\n",
    "                                                              latent_vector=self.z_tilda,\n",
    "                                                              output_layer=self.output_layer)\n",
    "\n",
    "                self.training_logits, _state, _len = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                                                       output_time_major=False,\n",
    "                                                                                       impute_finished=True,\n",
    "                                                                                       maximum_iterations=self.num_tokens)\n",
    "\n",
    "                self.training_logits = tf.identity(self.training_logits.rnn_output, 'logits')\n",
    "\n",
    "            with tf.name_scope(\"validate_decoder\"):\n",
    "                start_token = self.word_index['GO']\n",
    "                end_token = self.word_index['EOS']\n",
    "\n",
    "                start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [self.batch_size],\n",
    "                                       name='start_tokens')\n",
    "\n",
    "                inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(self.embeddings,\n",
    "                                                                            start_tokens,\n",
    "                                                                            end_token)\n",
    "\n",
    "                inference_decoder = BasicDecoder(dec_cell,\n",
    "                                                               inference_helper,\n",
    "                                                               initial_state=self.init_state,\n",
    "                                                               latent_vector=self.z_tilda,\n",
    "                                                               output_layer=self.output_layer)\n",
    "\n",
    "                self.validate_logits, _state, _len = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                                                        output_time_major=False,\n",
    "                                                                                        impute_finished=True,\n",
    "                                                                                        maximum_iterations=self.num_tokens)\n",
    "\n",
    "\n",
    "                self.validate_sent = tf.identity(self.validate_logits.sample_id, name='predictions')\n",
    "\n",
    "            with tf.name_scope(\"inference_decoder\"):\n",
    "                start_token = self.word_index['GO']\n",
    "                end_token = self.word_index['EOS']\n",
    "\n",
    "                start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [self.batch_size],\n",
    "                                       name='start_tokens')\n",
    "\n",
    "                inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(self.embeddings,\n",
    "                                                                            start_tokens,\n",
    "                                                                            end_token)\n",
    "\n",
    "                inference_decoder = BasicDecoder(dec_cell,\n",
    "                                                               inference_helper,\n",
    "                                                               initial_state=self.init_state,\n",
    "                                                               latent_vector=self.z_sampled,\n",
    "                                                               output_layer=self.output_layer)\n",
    "\n",
    "                self.inference_logits, _state, _len = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                                                        output_time_major=False,\n",
    "                                                                                        impute_finished=True,\n",
    "                                                                                        maximum_iterations=self.num_tokens)\n",
    "\n",
    "                self.inference_logits = tf.identity(self.inference_logits.sample_id, name='predictions')\n",
    "\n",
    "    def mmd_penalty(self, sample_qz, sample_pz):\n",
    "        n = self.batch_size\n",
    "        n = tf.cast(n, tf.int32)\n",
    "        nf = tf.cast(n, tf.float32)\n",
    "        half_size = (n * n - n) / 2\n",
    "\n",
    "        norms_pz = tf.reduce_sum(tf.square(sample_pz), axis=1, keep_dims=True)\n",
    "        dotprods_pz = tf.matmul(sample_pz, sample_pz, transpose_b=True)\n",
    "        distances_pz = norms_pz + tf.transpose(norms_pz) - 2. * dotprods_pz\n",
    "\n",
    "        norms_qz = tf.reduce_sum(tf.square(sample_qz), axis=1, keep_dims=True)\n",
    "        dotprods_qz = tf.matmul(sample_qz, sample_qz, transpose_b=True)\n",
    "        distances_qz = norms_qz + tf.transpose(norms_qz) - 2. * dotprods_qz\n",
    "\n",
    "        dotprods = tf.matmul(sample_qz, sample_pz, transpose_b=True)\n",
    "        distances = norms_qz + tf.transpose(norms_pz) - 2. * dotprods\n",
    "\n",
    "        if self.config['kernel'] == 'RBF':\n",
    "            # Median heuristic for the sigma^2 of Gaussian kernel\n",
    "            sigma2_k = tf.nn.top_k(\n",
    "                tf.reshape(distances, [-1]), half_size).values[half_size - 1]\n",
    "            sigma2_k += tf.nn.top_k(\n",
    "                tf.reshape(distances_qz, [-1]), half_size).values[half_size - 1]\n",
    "            # if opts['verbose']:\n",
    "            #     sigma2_k = tf.Print(sigma2_k, [sigma2_k], 'Kernel width:')\n",
    "            res1 = tf.exp(- distances_qz / 2. / sigma2_k)\n",
    "            res1 += tf.exp(- distances_pz / 2. / sigma2_k)\n",
    "            res1 = tf.multiply(res1, 1. - tf.eye(n))\n",
    "            res1 = tf.reduce_sum(res1) / (nf * nf - nf)\n",
    "            res2 = tf.exp(- distances / 2. / sigma2_k)\n",
    "            res2 = tf.reduce_sum(res2) * 2. / (nf * nf)\n",
    "            stat = res1 - res2\n",
    "        elif self.config['kernel'] == 'IMQ':\n",
    "            # k(x, y) = C / (C + ||x - y||^2)\n",
    "            # C = tf.nn.top_k(tf.reshape(distances, [-1]), half_size).values[half_size - 1]\n",
    "            # C += tf.nn.top_k(tf.reshape(distances_qz, [-1]), half_size).values[half_size - 1]\n",
    "            #if opts['pz'] == 'normal':\n",
    "            #    Cbase = 2. * opts['zdim'] * sigma2_p\n",
    "            #elif opts['pz'] == 'sphere':\n",
    "            #    Cbase = 2.\n",
    "            #elif opts['pz'] == 'uniform':\n",
    "                # E ||x - y||^2 = E[sum (xi - yi)^2]\n",
    "                #               = zdim E[(xi - yi)^2]\n",
    "                #               = const * zdim\n",
    "            #    Cbase = opts['zdim']\n",
    "\n",
    "            Cbase = 2. * self.config['latent_dim'] * 2. * 1. # sigma2_p # for normal sigma2_p = 1\n",
    "            stat = 0.\n",
    "            for scale in [.1, .2, .5, 1., 2., 5., 10.]:\n",
    "                C = Cbase * scale\n",
    "                res1 = C / (C + distances_qz)\n",
    "                res1 += C / (C + distances_pz)\n",
    "                res1 = tf.multiply(res1, 1. - tf.eye(n))\n",
    "                res1 = tf.reduce_sum(res1) / (nf * nf - nf)\n",
    "                res2 = C / (C + distances)\n",
    "                res2 = tf.reduce_sum(res2) * 2. / (nf * nf)\n",
    "                stat += res1 - res2\n",
    "        return stat\n",
    "\n",
    "    def loss(self):\n",
    "        with tf.name_scope('losses'):\n",
    "            self.wasserstein_loss = self.mmd_penalty(self.z_sampled, self.z_tilda)\n",
    "\n",
    "            # Create the weights for sequence_loss\n",
    "            masks = tf.sequence_mask(self.target_sentence_length, self.num_tokens, dtype=tf.float32, name='masks')\n",
    "\n",
    "            self.xent_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "                self.training_logits,\n",
    "                self.target_data[:, :self.max_tar_len],\n",
    "                weights=masks[:, :self.max_tar_len],\n",
    "                average_across_batch=True)\n",
    "\n",
    "            # L2-Regularization\n",
    "            self.var_list = tf.trainable_variables()\n",
    "            self.lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in self.var_list if 'bias' not in v.name]) * 0.001\n",
    "\n",
    "            self.cost = self.xent_loss + self.config['lambda_val'] * self.wasserstein_loss # + self.lossL2\n",
    "\n",
    "    def optimize(self):\n",
    "        # Optimizer\n",
    "        with tf.name_scope('optimization'):\n",
    "            optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "            # optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "\n",
    "            # Gradient Clipping\n",
    "            gradients = optimizer.compute_gradients(self.cost, var_list=self.var_list)\n",
    "            capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "            self.train_op = optimizer.apply_gradients(capped_gradients)\n",
    "\n",
    "    def summary(self):\n",
    "        with tf.name_scope('summaries'):\n",
    "            tf.summary.scalar('xent_loss', tf.reduce_sum(self.xent_loss))\n",
    "            tf.summary.scalar('l2_loss', tf.reduce_sum(self.lossL2))\n",
    "            tf.summary.scalar(\"wasserstein_loss\", tf.reduce_sum(self.wasserstein_loss))\n",
    "            tf.summary.scalar('total_loss', tf.reduce_sum(self.cost))\n",
    "            tf.summary.scalar('lambda', self.lambda_coeff)\n",
    "\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "\n",
    "    def monitor(self, x_val, sess, epoch_i, time_consumption):\n",
    "        self.validate(sess, x_val)\n",
    "        val_bleu_str = str(self.epoch_bleu_score_val['1'][-1]) + ' | ' \\\n",
    "                       + str(self.epoch_bleu_score_val['2'][-1]) + ' | ' \\\n",
    "                       + str(self.epoch_bleu_score_val['3'][-1]) + ' | ' \\\n",
    "                       + str(self.epoch_bleu_score_val['4'][-1])\n",
    "\n",
    "        val_str = '\\t\\t Generated \\t|\\t Actual \\n'\n",
    "        for pred, ref in zip(self.val_pred[:20], self.val_ref[:20]):\n",
    "            val_str += '\\t\\t' + pred + '\\t|\\t' + ref + '\\n'\n",
    "\n",
    "        print(val_str)\n",
    "        #log_writer.write(val_str)\n",
    "\n",
    "        generated = self.random_sample_in_session(sess)\n",
    "\n",
    "        print(generated)\n",
    "        #log_writer.write(generated)\n",
    "\n",
    "        log_thisepoch = 'Epoch {:>3}/{} - Time {:>6.1f}, Train loss: {:>3.2f}, Val BLEU: {}\\n\\n'.format(epoch_i,\n",
    "                                                                                                        self.epochs,\n",
    "                                                                                                        time_consumption,\n",
    "                                                                                                        self.train_xent,\n",
    "                                                                                                        val_bleu_str)\n",
    "\n",
    "        print(log_thisepoch)\n",
    "        #log_writer.write(log_thisepoch)\n",
    "        #log_writer.flush()\n",
    "    \n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, self.model_checkpoint_dir + str(epoch_i) + \".ckpt\")\n",
    "\n",
    "        # Save the validation BLEU scores so far\n",
    "        #with open(self.bleu_path + config_fingerprint + '.pkl', 'wb') as f:\n",
    "        #    pickle.dump(self.epoch_bleu_score_val, f)\n",
    "\n",
    "        self.log_str.append(log_thisepoch)\n",
    "\n",
    "        #with open('bleu_logs.txt', 'w') as f:\n",
    "        #    f.write('\\n'.join(self.log_str))\n",
    "\n",
    "    def train(self, x_train, x_val, checkpoint_return = True, checkpoint = None):\n",
    "\n",
    "        print('[INFO] Training process started')\n",
    "\n",
    "        learning_rate = self.initial_learning_rate\n",
    "        iter_i = 0\n",
    "\n",
    "        with tf.Session() as sess: \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            if checkpoint_return:\n",
    "                saver = tf.train.Saver()\n",
    "                saver.restore(sess, checkpoint)\n",
    "\n",
    "            writer = tf.summary.FileWriter(self.logs_dir, sess.graph)\n",
    "\n",
    "            for epoch_i in range(1, self.epochs + 1):\n",
    "\n",
    "                start_time = time.time()\n",
    "                for batch_i, (input_batch, output_batch, sent_lengths) in enumerate(\n",
    "                        get_batches(x_train, self.batch_size)):\n",
    "\n",
    "                    try:\n",
    "                        iter_i += 1\n",
    "\n",
    "                        _, _summary, self.train_xent = sess.run(\n",
    "                            [self.train_op, self.summary_op, self.xent_loss],\n",
    "                            feed_dict={self.input_data: input_batch, # <batch x maxlen>\n",
    "                                       self.target_data: output_batch, # <batch x maxlen>\n",
    "                                       self.lr: learning_rate,\n",
    "                                       self.source_sentence_length: sent_lengths,\n",
    "                                       self.target_sentence_length: sent_lengths,\n",
    "                                       self.keep_prob: self.dropout_keep_prob,\n",
    "                                       self.lambda_coeff: self.lambda_val,\n",
    "                                       })\n",
    "\n",
    "                        writer.add_summary(_summary, iter_i)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(iter_i, e)\n",
    "                        pass\n",
    "\n",
    "                # Reduce learning rate, but not below its minimum value\n",
    "                learning_rate = np.max([self.min_learning_rate, learning_rate * self.learning_rate_decay])\n",
    "                time_consumption = time.time() - start_time\n",
    "                self.monitor(x_val, sess, epoch_i, time_consumption)\n",
    "\n",
    "    def validate(self, sess, x_val):\n",
    "        # Calculate BLEU on validation data\n",
    "        hypotheses_val = []\n",
    "        references_val = []\n",
    "\n",
    "        for batch_i, (input_batch, output_batch, sent_lengths) in enumerate(\n",
    "               get_batches(x_val, self.batch_size)):\n",
    "            pred_sentences, self._validate_logits = sess.run(\n",
    "                                     [self.validate_sent, self.validate_logits],\n",
    "                                     feed_dict={self.input_data: input_batch,\n",
    "                                                self.source_sentence_length: sent_lengths,\n",
    "                                                self.keep_prob: 1.0,\n",
    "                                                })\n",
    "\n",
    "\n",
    "            for pred, actual in zip(pred_sentences, output_batch):\n",
    "                hypotheses_val.append(\n",
    "                    word_tokenize(\n",
    "                        \" \".join([self.idx_word[i] for i in pred if i not in [self.pad, -1, self.eos]])))\n",
    "                references_val.append(\n",
    "                    [word_tokenize(\" \".join([self.idx_word[i] for i in actual if i not in [self.pad, -1, self.eos]]))])\n",
    "            self.val_pred = ([\" \".join(sent)    for sent in hypotheses_val])\n",
    "            self.val_ref  = ([\" \".join(sent[0]) for sent in references_val])\n",
    "\n",
    "        bleu_scores = calculate_bleu_scores(references_val, hypotheses_val)\n",
    "\n",
    "        self.epoch_bleu_score_val['1'].append(bleu_scores[0])\n",
    "        self.epoch_bleu_score_val['2'].append(bleu_scores[1])\n",
    "        self.epoch_bleu_score_val['3'].append(bleu_scores[2])\n",
    "        self.epoch_bleu_score_val['4'].append(bleu_scores[3])\n",
    "\n",
    "    def predict(self, checkpoint, x_test):\n",
    "        pred_logits = []\n",
    "        hypotheses_test = []\n",
    "        references_test = []\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, checkpoint)\n",
    "\n",
    "            for batch_i, (input_batch, output_batch, sent_lengths) in enumerate(\n",
    "                    get_batches(x_test, self.batch_size)):\n",
    "                result = sess.run(self.validate_sent, feed_dict={self.input_data: input_batch,\n",
    "                                                                    self.source_sentence_length: sent_lengths,\n",
    "                                                                    self.keep_prob: 1.0,\n",
    "                                                                    })\n",
    "\n",
    "                pred_logits.extend(result)\n",
    "\n",
    "                for pred, actual in zip(result, output_batch):\n",
    "                    hypotheses_test.append(\n",
    "                        word_tokenize(\" \".join(\n",
    "                            [self.idx_word[i] for i in pred if i not in [self.pad, -1, self.eos]])))\n",
    "                    references_test.append([word_tokenize(\n",
    "                        \" \".join([self.idx_word[i] for i in actual if i not in [self.pad, -1, self.eos]]))])\n",
    "\n",
    "            bleu_scores = calculate_bleu_scores(references_test, hypotheses_test)\n",
    "\n",
    "        print('BLEU 1 to 4 : {}'.format(' | '.join(map(str, bleu_scores))))\n",
    "\n",
    "        return pred_logits\n",
    "\n",
    "    def show_output_sentences(self, preds, x_test):\n",
    "        for pred, actual in zip(preds, x_test):\n",
    "            # Actual and generated\n",
    "            print('A: {}'.format(\n",
    "                \" \".join([self.idx_word[i] for i in actual if i not in [self.pad, self.eos]])))\n",
    "            print('G: {}\\n'.format(\n",
    "                \" \".join([self.idx_word[i] for i in pred if i not in [self.pad, self.eos]])))\n",
    "\n",
    "    def random_sample(self, checkpoint):\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, checkpoint)\n",
    "\n",
    "            \n",
    "            z_sampled = np.random.normal(size=(self.batch_size, self.latent_dim))\n",
    "            result = sess.run(self.inference_logits,\n",
    "                                feed_dict={self.z_sampled: z_sampled,\n",
    "                                            self.keep_prob: 1.0,\n",
    "                                            })\n",
    "\n",
    "            for pred in result:\n",
    "                sent = \" \".join([self.idx_word[i] for i in pred if i not in [self.pad, self.eos]])\n",
    "                print('G: {}'.format(sent))\n",
    "\n",
    "    def random_sample_save(self, checkpoint, num_batches=1):\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, checkpoint)\n",
    "            gen_samples = []\n",
    "            repeated_counter = 0\n",
    "            num_batches = int(100000/self.batch_size)\n",
    "            print('[INFO] NUMBER OF SAMPLES: ', num_batches * self.batch_size)\n",
    "\n",
    "            for i in range(num_batches):\n",
    "                z_sampled = np.random.normal(size=(self.batch_size, self.latent_dim))\n",
    "                result = sess.run(self.inference_logits,\n",
    "                                  feed_dict={self.z_sampled: z_sampled,\n",
    "                                            self.keep_prob: 1.0,\n",
    "                                            })\n",
    "\n",
    "                for pred in result:\n",
    "                    sent = \" \".join([self.idx_word[i] for i in pred if i not in [self.pad, self.eos]])\n",
    "                    if sent not in gen_samples:\n",
    "                      gen_samples.append(sent)\n",
    "                    else:\n",
    "                      repeated_counter += 1\n",
    "\n",
    "        # Create directories for saving sentences generated by random sampling\n",
    "        out_file = '/content/drive/My Drive/wae_det_samples.txt'\n",
    "        \n",
    "        with open(out_file, 'w') as f:\n",
    "            f.write('\\n'.join(gen_samples))\n",
    "\n",
    "    def random_sample_in_session(self, sess):\n",
    "        z_sampled = np.random.normal(size=(self.batch_size, self.latent_dim))\n",
    "        result = sess.run(self.inference_logits,feed_dict={self.z_sampled: z_sampled,self.keep_prob: 1.0,})\n",
    "\n",
    "        generated = ''\n",
    "\n",
    "        for pred in result[:10]:\n",
    "            generated += '\\t\\t' + ' '.join([self.idx_word[i] for i in pred if i not in [self.pad, self.eos]]) + '\\n'\n",
    "        return generated\n",
    "                \n",
    "    def linear_interpolate(self, checkpoint, num_samples):\n",
    "        sampled = []\n",
    "        for i in range(self.batch_size // num_samples):\n",
    "            z = np.random.normal(0, 1, (2, self.latent_dim))\n",
    "            s1_z = z[0]\n",
    "            s2_z = z[1]\n",
    "            s1_z = np.repeat(s1_z[None, :], num_samples, axis=0)\n",
    "            s2_z = np.repeat(s2_z[None, :], num_samples, axis=0)\n",
    "            steps = np.linspace(0, 1, num_samples)[:, None]\n",
    "            sampled.append(s1_z * (1 - steps) + s2_z * steps)\n",
    "\n",
    "        sampled = np.reshape(np.array(sampled), newshape=(self.batch_size, self.latent_dim))\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, checkpoint)\n",
    "\n",
    "            result = sess.run(self.inference_logits,\n",
    "                              feed_dict={self.z_sampled: sampled,\n",
    "                                         self.keep_prob: 1.0,\n",
    "                                         })\n",
    "\n",
    "            for i, pred in enumerate(result):\n",
    "                if i % num_samples == 0:\n",
    "                    print()\n",
    "                print('G: {}'.format(\n",
    "                    \" \".join([self.idx_word[i] for i in pred if i not in [self.pad, self.eos]])))\n",
    "                \n",
    "    def linear_interpolate_between_inputs(self, checkpoint, start_sent, end_sent, num_samples=8):\n",
    "\n",
    "        # Convert seq of words to seq of indices\n",
    "        # if the word is not present, use default in get(): UNK\n",
    "        start_sent = word_tokenize(start_sent)\n",
    "        end_sent = word_tokenize(end_sent)\n",
    "        start_idx_seq = [self.word_index.get(word, self.unk) for word in start_sent] + [self.eos] \n",
    "        end_idx_seq = [self.word_index.get(word, self.unk) for word in end_sent] + [self.eos]  # Append EOS token\n",
    "        start_idx_seq = np.concatenate([start_idx_seq, np.zeros(max(0, self.num_tokens - len(start_idx_seq)))])[\n",
    "                      :self.num_tokens]\n",
    "        end_idx_seq = np.concatenate([end_idx_seq, np.zeros(max(0, self.num_tokens - len(end_idx_seq)))])[\n",
    "                        :self.num_tokens]\n",
    "\n",
    "        # Reshape/tile so that the input has first dimension as batch size\n",
    "        inp_idx_seq = np.tile(np.vstack([start_idx_seq, end_idx_seq]), [self.batch_size//2, 1])\n",
    "        # source_sent_lengths = [np.count_nonzero(seq) for seq in inp_idx_seq]\n",
    "\n",
    "        # Get z_vector of first and last sentence\n",
    "        z_vecs = self.get_zvector(checkpoint, inp_idx_seq)\n",
    "\n",
    "        sampled = []\n",
    "        s1_z = z_vecs[0]\n",
    "        s2_z = z_vecs[1]\n",
    "        s1_z = np.repeat(s1_z[None, :], num_samples, axis=0)\n",
    "        s2_z = np.repeat(s2_z[None, :], num_samples, axis=0)\n",
    "        steps = np.linspace(0, 1, num_samples)[:, None]\n",
    "        sampled.append(s1_z * (1 - steps) + s2_z * steps)\n",
    "\n",
    "        sampled = np.tile(sampled[0], [self.batch_size//num_samples, 1])\n",
    "        # sampled = np.reshape(np.array(sampled), newshape=(self.batch_size, self.latent_dim))\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, checkpoint)\n",
    "\n",
    "            result = sess.run(self.inference_logits,\n",
    "                              feed_dict={self.z_sampled: sampled,\n",
    "                                         self.keep_prob: 1.0,\n",
    "                                         })\n",
    "\n",
    "            for i, pred in enumerate(result[:num_samples]):\n",
    "                print('G: {}'.format(\n",
    "                    \" \".join([self.idx_word[i] for i in pred if i not in [self.pad, self.eos]])))\n",
    "\n",
    "    def get_zvector(self, checkpoint, x_test):\n",
    "        z_vecs = []\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, checkpoint)\n",
    "\n",
    "            for batch_i, (input_batch, output_batch, sent_lengths) in enumerate(\n",
    "                    get_batches(x_test, self.batch_size)):\n",
    "                result = sess.run(self.z_tilda, feed_dict={self.input_data: input_batch,\n",
    "                                                           self.source_sentence_length: sent_lengths,\n",
    "                                                           self.keep_prob: 1.0,\n",
    "                                                           })\n",
    "                z_vecs.extend(result)\n",
    "\n",
    "        return np.array(z_vecs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sFWIhd76ermN"
   },
   "source": [
    "### RUN MODEL ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c58K03y6et9y"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "def model_argparse():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # parser.add_argument(\"--isDebug\", type=bool, default=isDebug, help='is debug')\n",
    "    parser.add_argument(\"--device\", type=str, default=\"0\", help='tf device') # GPU 0 or 1\n",
    "    parser.add_argument(\"--lstm_hidden_units\", type=int, default=100, help='number of hidden units for the LSTM')\n",
    "    parser.add_argument(\"--embedding_size\", type=int, default=300, help='word embedding dimension')\n",
    "    parser.add_argument(\"--num_layers\", type=int, default=1, help='number of LSTM layers')\n",
    "    parser.add_argument(\"--vocab_size\", type=int, default=30000, help='vocabulary size')\n",
    "    parser.add_argument(\"--num_tokens\", type=int, default=20, help='max number of words/tokens in the input/generated sequence')\n",
    "    \n",
    "    parser.add_argument(\"--latent_dim\", type=int, default=100, help='dimension of z-latent space')\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128, help='batch size')\n",
    "    parser.add_argument(\"--n_epochs\", type=int, default=20, help='number of epochs')\n",
    "\n",
    "    parser.add_argument(\"--dropout_keep_prob\", type=float, default=0.8, help='dropout keep probability')\n",
    "    parser.add_argument(\"--initial_learning_rate\", type=float, default=0.001, help='initial learning rate')\n",
    "    parser.add_argument(\"--learning_rate_decay\", type=float, default=1.0, help='learning rate decay')\n",
    "    parser.add_argument(\"--min_learning_rate\", type=float, default=0.00001, help='minimum learning rate')\n",
    "\n",
    "    parser.add_argument(\"--lambda_val\", type=float, default=0., help='initial value of lambda, i.e., MMD co-efficient')\n",
    "    parser.add_argument(\"--kernel\", type=str, default='IMQ', help='MMD loss based on kernel type from: IMQ or RBF ')\n",
    "\n",
    "    parser.add_argument(\"--data\", type=str, default='../data/snli_sentences_all.txt')\n",
    "    parser.add_argument(\"--w2v_file\", type=str, default='../w2v_models/w2v_300d_snli_all_sentences.pkl')\n",
    "    parser.add_argument(\"--bleu_path\", type=str, default='bleu/', help='path to save bleu scores')\n",
    "    parser.add_argument(\"--model_checkpoint_dir\", type=str, default='', help='path to save model checkpoints')\n",
    "    parser.add_argument(\"--logs_dir\", type=str, default='', help='path to save log files')\n",
    "\n",
    "    parser.add_argument(\"--ckpt\", type=str, default=None, help='checkpoint')\n",
    "  \n",
    "    \n",
    "    config = dict()\n",
    "    config['device'] = '0'\n",
    "    config['lstm_hidden_units'] = 100\n",
    "    config['embedding_size'] = 300\n",
    "    config['vocab_size'] = 30000\n",
    "    config['num_layers'] = 1\n",
    "    config['num_tokens'] = 50\n",
    "    \n",
    "    config['latent_dim'] = 100\n",
    "    config['batch_size'] = 128\n",
    "    config['n_epochs'] = 500\n",
    "    \n",
    "    config['dropout_keep_prob'] = 0.8\n",
    "    config['initial_learning_rate'] = 0.001\n",
    "    config['learning_rate_decay'] = 1.0\n",
    "    config['min_learning_rate'] = 0.00001\n",
    "    \n",
    "    config['lambda_val'] = 3.0\n",
    "    config['kernel'] = 'IMQ'\n",
    "    \n",
    "    config['data'] ='/content/drive/My Drive/sources.txt'\n",
    "    config['w2v_file'] = '/content/drive/My Drive/w2v_models/fce_incorrect.pkl'\n",
    "    config['bleu_path'] = 'bleu/'\n",
    "    config['model_checkpoint_dir'] = ''\n",
    "    config['logs_dir'] = ''\n",
    "\n",
    "    # Output log file\n",
    "    config_fingerprint = 'full_snli_' + \\\n",
    "            'lambdaWAE' + str(config['lambda_val']) + \\\n",
    "            '_batch' + str(config['batch_size']) + \\\n",
    "            '_kernel_' + str(config['kernel']) + \\\n",
    "            '_num_tokens_' + str(config['num_tokens'])\n",
    "    \n",
    "    if not isTrain:\n",
    "        return config\n",
    "\n",
    "    # Create directories for saving model runs and stats\n",
    "    pwd = os.path.dirname('/content/drive/My Drive/')\n",
    "\n",
    "    if not os.path.exists(pwd + '/bleu'):\n",
    "        os.mkdir(pwd + '/bleu')\n",
    "    \n",
    "    if not os.path.exists(pwd + '/runs'):\n",
    "        os.mkdir(pwd + '/runs')\n",
    "\n",
    "    log_writer = open(pwd + '/runs/log_' + config_fingerprint, 'a')\n",
    "    log_writer.write(str(config) + '\\n')\n",
    "    log_writer.flush()\n",
    "\n",
    "    # Model checkpoint\n",
    "    if not os.path.exists(pwd + '/models'):\n",
    "        os.mkdir(pwd + '/models')\n",
    "    model_path = pwd + '/models/' + config_fingerprint\n",
    "    if not os.path.exists(model_path):\n",
    "        os.mkdir(model_path)\n",
    "    config['model_checkpoint_dir'] = model_path + '/'\n",
    "\n",
    "    # Model summary directory\n",
    "    if not os.path.exists(pwd + '/summary_logs'):\n",
    "        os.mkdir(pwd + '/summary_logs')\n",
    "    summary_path = pwd + '/summary_logs/' + config_fingerprint\n",
    "    if not os.path.exists(summary_path):\n",
    "        os.mkdir(summary_path)\n",
    "\n",
    "    config['logs_dir'] = summary_path\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 152
    },
    "colab_type": "code",
    "id": "5Fu25E8SX0lk",
    "outputId": "89ea302b-21fe-41d0-d917-02cbb01e8b2e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "isTrain = True\n",
    "\n",
    "config = model_argparse()\n",
    "\n",
    "import tensorflow as tf\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=tf_config)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('FILE PATH: ', config['data'])\n",
    "snli_data = get_sentences(file_path = config['data'])\n",
    "\n",
    "print('[INFO] Number of sentences = {}'.format(len(snli_data)))\n",
    "\n",
    "sentences = [s.strip() for s in snli_data]\n",
    "\n",
    "np.random.shuffle(sentences)\n",
    "\n",
    "print('[INFO] Tokenizing input and output sequences')\n",
    "filters = '!\"#$%&()*+/:;<=>@[\\\\]^`{|}~\\t\\n'\n",
    "x, word_index = tokenize_sequence(sentences,\n",
    "                                             filters,\n",
    "                                             config['num_tokens'],\n",
    "                                             config['vocab_size'])\n",
    "\n",
    "print('[INFO] Split data into train-validation-test sets')\n",
    "x_train, _x_val_test = train_test_split(x, test_size = 0.1, random_state = 10)\n",
    "x_val, x_test = train_test_split(_x_val_test, test_size = 0.5, random_state = 10)\n",
    "\n",
    "w2v = config['w2v_file']\n",
    "embeddings_matrix = create_embedding_matrix(word_index,\n",
    "                                                  config['embedding_size'],\n",
    "                                                  w2v)\n",
    "\n",
    "# Re-calculate the vocab size based on the word_idx dictionary\n",
    "config['vocab_size'] = len(word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "s4-WAYohfl5_",
    "outputId": "25f33037-ee98-4185-a97a-b794ca117860"
   },
   "outputs": [],
   "source": [
    "\n",
    "#----------------------------------------------------------------#\n",
    "model = DetWAEModel(config,\n",
    "                    embeddings_matrix,\n",
    "                    word_index)\n",
    "\n",
    "checkpoint = \n",
    "model.train(x_train, x_val)\n",
    "\n",
    "log_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "colab_type": "code",
    "id": "eiFWTNTCXE0D",
    "outputId": "d93358a4-b77d-4d72-bfc9-e876896ca60d"
   },
   "outputs": [],
   "source": [
    "model = DetWAEModel(config,\n",
    "                    embeddings_matrix,\n",
    "                    word_index)\n",
    "\n",
    "checkpoint = '/content/drive/My Drive/models/full_snli_lambdaWAE3.0_batch128_kernel_IMQ_num_tokens_50/146.ckpt'\n",
    "model.random_sample_save(checkpoint = checkpoint)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Tv-2HFerdcXx",
    "_wow7PGtd6ew"
   ],
   "name": "WAE_deterministic (1).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
